---
title: "Gradient boosting"
author: 
  - name: "Vladimir Buskin"
date: 11-12-2024
abstract: > 
  Gradient boosting constitutes a powerful extension of tree-based methods and is generally appreciated for its high predictive performance. Nevertheless, this family of methods, which includes implementations such as AdaBoost, XGBoost, and CatBoost, among many others, is not yet established in corpus-linguistic statistics. A practical scenario is presented to introduce the core ideas of gradient boosting, demonstrate its application to linguistic data as well as point out its advantages and drawbacks.
keywords: "Machine learning, gradient descent, loss function, regularization"
format:
  html:
    self-contained: true
    theme: default
    toc: true
    number-sections: true
    slide-number: true
    incremental: false
    slide-level: 3
    scrollable: true
editor: visual
bibliography: R.bib
---

## Recommended reading

> @james_introduction_2021: Chapter 8.2
>
> @hastie2017: Chapter 10

## Preparation

## Boosting

The core idea of **boosting** is as simple as it is intuitive: By
aggregating the insights of multiple weak models, a much more powerful
complex model can be formed. The new model ensemble is argued to be
superior in terms of predictive performance. Boosting is quite
versatile, but we will restrict our scope to decision trees as
introduced in the [previous
unit](Decision_trees_and_random_forests.qmd).

Core to building a boosting model is minimising a **loss function**
$L(y, f(x))$ which represents the model's error. The choice of the loss
metric crucially depends on whether the response $Y$ is continuous
(regression) or discrete (classification). A simple loss function for
regression would be the sum of squared-errors
$L(y, f(x)) = (y -f(x))^2$. For classification, metrics that are known
from logistic regression would be more appropriate, such as the model's
deviance.

## Gradient descent

::: callout-warning
This page is still under construction. More content will be added soon!
:::
