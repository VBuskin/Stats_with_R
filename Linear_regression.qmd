---
title: "Linear regression"
author: Vladimir Buskin
format:
  html:
    self-contained: false
    logo: logo.png
    footer: "Regression"
    theme: default
    toc: true
    number-sections: true
    slide-number: true
    incremental: false
    slide-level: 4
    scrollable: true
editor: visual
bibliography: R.bib
---

## Recommended reading

> The following introduction is based on Heumann et al.
[-@heumann_introduction_2022: Chapter 11], James et al.
[-@james_introduction_2021: Chapter 3], Levshina [-@levshina_how_2015:
Chapter 7] and Winter [-@winter_statistics_2020: Chapter 4].

## Preparation {.smaller}

```{r, echo = FALSE, output = FALSE}
library("readxl")
library("writexl")
library("tidyverse")
```

```{r, echo = TRUE, output = FALSE}

# Load libraries

library("readxl")
library("writexl")
library("tidyverse")

# Load file
ELP <- read_xlsx("ELP.xlsx")

# Inspect file structure
str(ELP)

```

## Introduction

Consider the distribution of the continuous variable `RT` (reaction
times) from the `ELP` (English Lexicon Project) dataset. We will
$log$-transform the reaction times to even out the differences between
extremely high and extremely low frequency counts [cf.
@winter_statistics_2020: 90-94].

::: panel-tabset
### Log-transformed

```{r, echo = FALSE, output = TRUE}
# Log-transformed

ggplot(ELP, aes(x = log(RT))) +
  geom_histogram() +
  geom_vline(xintercept = log(mean(ELP$RT)), color = "steelblue") +
  theme_minimal() +
  labs(
    x = "Reaction time (log)"
  )


```

### Default

```{r, echo = FALSE, output = TRUE}
# Skewed

ggplot(ELP, aes(x = RT)) +
  geom_histogram() +
  geom_vline(xintercept = mean(ELP$RT), color = "steelblue") +
  theme_minimal() +
  labs(
    x = "Reaction time"
  )
```
:::

We are investigating the relationship between reaction times `RT` and
the frequency `Freq` of a lexical stimulus.

```{r, echo = FALSE, output = TRUE}
ggplot(ELP, aes(x = log(RT), y = log(Freq))) +
  geom_point() +
  theme_minimal() +
  labs(
    x = "Log-transformed reaction time",
    y = "Log-transformed word frequency"
  )



```

**Some open questions**:

-   Can word frequency help us explain variation in reaction times?

-   If it can, then how could we characterise the effect of word
    frequency? In other words, does it increase or decrease reaction
    times?

-   What reaction times should we expect for new observations?

### Building a statistical model

Here `RT` is a *response* or *target* that we wish to explain. We
generically refer to the response as $Y$.

`Freq` is a *feature*, *input*, or *predictor*, which we name $X$.

We can thus summarise our preliminary and fairly general statistical
model as

$$ Y = f(X) + \epsilon. $$While the term $f(X)$ denotes the contribution
of $X$ to the explanation of $Y$, $\epsilon$ describes the errors of the
model.

## Linear Regression

Linear regression is a simple approach to supervised machine learning
where the response variable is known. It assumes that the dependence of
$Y$ on $X$ is **linear**. This approach is suitable for **numerical
response variables**. The predictors, however, can be either numerical
or discrete/categorical. Although it may seem overly simplistic, linear
regression is **extremely useful** both conceptually and practically.

### Model with a single predictor $X$ {.smaller}

A linear model of our data would have the form

$$ Y = \beta_0 + \beta_1X + \epsilon $$

or, in more concrete terms,

$$ \text{Reaction time} = \beta_0 + \beta_1\text{Frequency} + \text{Model Error,} $$

where $\beta_0$ and $\beta_1$ are two unknown constants that represent
the **intercept** and **slope** of the regression line, respectively.
Together they are referred to as the model **coefficients** (or
parameters), and $\epsilon$ is the error term. The fact that assumptions
are made about the form of the model renders it a **parametric model**.

Given the existing data on $X$ and $Y$, which is also known as the
training data, we **estimate** the model coefficients $\beta_0$ and
$\beta_1$. While we use the training data to estimate these
coefficients, we cannot know the true values of the coefficients, i.e.,
the exact true relationship between the variables. Their tentative nature is indicated by the **hat**
symbol \^ above them: $\hat{\beta_0}$ and $\hat{\beta_1}$.

::: {.callout-tip title="How are the coefficients estimated?" collapse="true"}

The most common way of estimating parameters for linear models is the
**Least Squares** approach. In essence, the parameters are chosen such
that the residual sum of squares, i.e., the sum of the differences
between observed and predicted values, is as low as possible. More
formally, the estimated slope then corresponds to

$$ \hat{\beta}_1 = \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i- \bar{y})}{\sum_{i=1}(x_i- \bar{x})^2}.$$

Now we can obtain the intercept:

$$
\hat{\beta_0}= \bar{y}- \hat{\beta}_1\bar{x}
$$
:::

We can then predict future sales using the formula

$$
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1x,
$$

where $\hat{y}$ indicates a prediction of $Y$ on the basis of the
predictor values $X = x$.

In R, we can fit a linear model with the `lm()` function.

```{r, echo = TRUE, output = TRUE}

# Fit linear model

rt.lm1 = lm(log(RT) ~ log(Freq), data = ELP)

# View model data

summary(rt.lm1)

```

The model statistics comprise the following elements:

::: {.callout-tip title="Call" collapse="true"}
i.e., the model formula.
:::

::: {.callout-tip title="Residuals" collapse="true"}
These indicate the difference between the observed
    values in the data set and the values predicted by the model (= the
    fitted values). These correspond to the error term $\epsilon$. The
    lower the residuals, the better the model describes the data.

```{r, echo = TRUE, output = TRUE}

# Show fitted values (= predictions) for the first six observations

head(rt.lm1$fitted.values)

# Show deviation of the fitted values from the observed values

head(rt.lm1$residuals)

```
:::

::: {.callout-tip title="Coefficients" collapse="true"}

The regression coefficients correspond to
    $\hat{\beta}_0$ ("Intercept") and $\hat{\beta}_1$ ("log(Freq)"),
    respectively. The model shows that for a one-unit increase in
    log-frequency the log-reaction time decreases by approx. -0.05.

```{r, echo = TRUE, output = TRUE, message = FALSE, warning = FALSE}

# Convert coefficients to a tibble 

library("broom")

tidy_model <- tidy(rt.lm1)

tidy_model

```

:::

::: {.callout-tip title="$p$-values and $t$-statistic" collapse="true"}

$p$**-values and** $t$**-statistic**: Given the null hypothesis
    $H_0$ that there is no correlation between `log(RT)` and `log(Freq)`
    (i.e., $H_0: \beta_1 = 0$), a $p$-value lower than 0.05 indicates
    that $\beta_1$ considerably deviates from 0, thus providing evidence
    for the alternative hypothesis $H_1: \beta_1 \ne 0$. Since
    $p < 0.001$, we can reject $H_0$.

    The $p$-value itself crucially depends on the
    $t$-statistic[^linear_regression-1], which measures "the number of
    standard deviations that $\hat{\beta_1}$ is away from 0"
    [@james_introduction_2021: 67] . The standard error (SE) reflects
    how much an estimated coefficient differs on average from the true
    values of $\beta_0$ and $\beta_1$. They can be used to compute the
    95% confidence interval
    $[\hat{\beta}_1 - 2 \cdot SE(\hat{β}_1), \hat{\beta}_1 + 2 \cdot SE(\hat{β}_1)]$;
    the true estimate of the parameter $\beta_1$ lies within the
    specified range 95% of the time.

[^linear_regression-1]: $$t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_0})}$$

```{r, echo = TRUE, output = TRUE}

# Compute confidence intervals for intercept and log(Freq)

tidy_model_ci <- tidy(rt.lm1, conf.int = TRUE)

tidy_model_ci

```

The estimated parameter for `log(Freq)`, which is -0.049, thus has the
95% confidence interval \[-0.053, -0.044\].

:::

::: {.callout-tip title="**Residual standard error** (RSE)" collapse="true"}
This is an estimation of the average deviation of the predictions from the observed values.

$$RSE = \sqrt{\frac{1}{n-2}\sum_{i=1}^n (y_i - \hat{y_i}})^2$$

:::

::: {.callout-tip title="$R^2$" collapse="true"}

The $R^2$ score is important for assessing model fit
    because it "measures the proportion of variability in $Y$ that can
    be explained using $X$" [@james_introduction_2021: 70; emphasis
    removed], varying between 0 and 1.
    
$$R^2 = 1-\frac{TSS}{RSS} = 1-\frac{\sum_{i=1}^n (y_i - \hat{y_i})^2}{\sum_{i=1}^n (y_i - \bar{y_i})^2}$$

:::

::: {.callout-tip title="$F$-statistic" collapse="true"}

It is used to measure the association between the
    dependent variable and the independent variable(s). Generally
    speaking, values greater than 1 indicate a possible correlation. A sufficiently very low $p$-value suggests that the null hypothesis
    $H_0: \beta_1 = 0$ can be rejected.
    
:::


### Multiple linear regression

In multiple linear regression, more than one predictor variable is taken
into account. For instance, modelling `log(RT)` as a function of
`log(Freq)`, `POS` and `Length` requires a more complex model of the
form

$$ Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon.$$

Predictions are then obtained via the formula

$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1x_1 + \hat{\beta}_2x_2 + ... + \hat{\beta}_px_p.
$$

In R, a multiple regression model is fitted as in the code example
below:

```{r, echo = TRUE, output = TRUE, warning = FALSE, message = FALSE}
# Fit multiple regression model

rt.lm2 <- lm(log(RT) ~ log(Freq) + POS + Length, data = ELP)

# View model statistics

summary(rt.lm2)

```

## Visualising regression models

Plot coefficient estimates:

```{r, echo = TRUE, output = TRUE}

# Tidy the model output
tidy_model <- tidy(rt.lm2, conf.int = TRUE)

# Remove intercept
tidy_model <- tidy_model %>% filter(term != "(Intercept)")

# Create the coefficient plot
ggplot(tidy_model, aes(x = estimate, y = term)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "steelblue") +
  theme_minimal() +
  labs(
    x = "Coefficient Estimate",
    y = "Predictor",
    title = "Coefficient Estimates with Confidence Intervals"
  )

```

Plot contributions of individual variable values:

```{r, echo = TRUE, output = TRUE, warning = FALSE, message = FALSE}
# Plot marginal effects

library("effects")

plot(Effect("Freq", mod = rt.lm2))
plot(Effect("POS", mod = rt.lm2))
plot(Effect("Length", mod = rt.lm2))

```

## Model assumptions and diagnostics

As a parametric method, linear regression makes numerous assumptions
about the training data. It is, therefore, essential to run further
tests to rule out possible violations. Among other things, the model
assumptions include:

-   A **linear relationship** between the response and the quantitative
    predictors: The residuals should not display a clear pattern. For
    this reason, it is recommended to use component residual plots
    (e.g., `crPlot()` from the `car` library) for the visual
    identification of potentially non-linear trends.

-   **No heteroscedasticity** (i.e, non-constant variance of error
    terms): Visually, a violation of this assumption becomes apparent if
    the residuals form a funnel-like shape. It is also possible to
    conduct a non-constant variance test `ncvTest()`: If it returns
    $p$-values \< 0.05, this suggests non-constant variance.

-   **No multicollinearity**: Predictors should not be correlated with
    each other. In the model data, correlated variables have unusually
    high standard errors, thereby decreasing the explanatory power of
    both the coefficients and the model as a whole. Another diagnostic
    measure are variance inflation factors (VIF-scores); predictors with
    VIF scores \> 5 are potentially collinear. They can be computed
    using the `vif()` function.

-   **Normally distributed residuals**: The residuals should follow the
    normal distribution. Usually, a visual inspection using `qqnorm()`
    is sufficient, but the Shapiro-Wilke test `shapiro.test()` can also
    be run on the model residuals. Note that a $p$-value below 0.05
    provides evidence for non-normality.

::: callout-important
Beside the points mentioned above, it is always recommend to examine the
model with regard to

-   **outliers** that might skew the regression estimates,

-   **interactions**, i.e., combined effects of predictors, and

-   **overfitting**, which results in poor model performance outside the
    training data.
:::
