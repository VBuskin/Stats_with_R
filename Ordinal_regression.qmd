---
title: "Ordinal regression"
author: Vladimir Buskin
format:
  html:
    self-contained: false
    logo: logo.png
    footer: "Regression"
    theme: Reference
    toc: true
    number-sections: true
    slide-number: true
    incremental: false
    slide-level: 4
    scrollable: true
editor: visual
bibliography: R.bib
---

## Suggested reading

> @powersStatisticalMethodsCategorical2008: Chapter 7
> @baguleySeriousStatsGuide2012: Chapter 17.4.5

## Introduction

In her recent contribution, @glassEnglishVerbsCan2021 examines possible
reasons why certain transitive verbs have a stronger affinity towards
object omission compared to others, placing special emphasis on the
routinisation of the actions denoted by transitive verbs.

### Research question and hypotheses

How do high/low-routine contexts affect the acceptability of object
omission for transitive verbs of varying frequency of occurrence?

**Low-frequency verbs**:

-   $H_0:$ Mean Likert rating in low-routine contexts (low freq) $=$
    Mean Likert rating in high-routine contexts (low freq)

-   $H_1:$ Mean Likert rating in low-routine contexts (low freq) $\neq$
    Mean Likert rating in high-routine contexts (low freq)

**High-frequency verbs**:

-   $H_0:$ Mean Likert rating in low-routine contexts (high freq) = Mean
    Likert rating in high-routine contexts (high freq)

-   $H_1:$ Mean Likert rating in low-routine contexts (high freq) $\neq$
    Mean Likert rating in high-routine contexts (high freq)

### Preparation

Load the data `Glass_2021_survey_processed.csv`[^ordinal_regression-1]:

[^ordinal_regression-1]: The original dataset can be retrieved from
    Lelia Glass's OSF repository: <https://osf.io/t6zw5> \[Last
    accessed: 27th September, 2024\].

```{r}
library(tidyverse)
library(ordinal)


survey <- read.csv("Glass_2021_survey_processed.csv")

str(survey)

head(survey)
```

::: {.callout-note collapse="true" title="Short breakdown of the variables"}
-   `routine`: In Glass's study, transitive verbs were randomly assigned
    to one of the following conditions:

> -   (High routine condition:) I worked at my poultry farm. Just like I
>     always do, I butchered some chickens. Then I gathered some eggs.
>
> -   (Low-routine condition:) I visited a friend's job. Just because
>     people wanted me to try it, I butchered some chickens. Then I went
>     for a walk.
>
> Cf. Glass [-@glassEnglishVerbsCan2021: 66]

```{r}
unique(survey$routine)
```

-   `rating` records the responses of participants to a follow-up
    question regarding the acceptability of object omission. The answers
    are recorded on a 1-5 Likert scale.

> *The next time Caroline talks about butchering chickens the day
> before, how likely do you think she is to say the following?*
>
> 'I butchered yesterday'
>
> Cf. Glass [-@glassEnglishVerbsCan2021: 66]

```{r}
unique(survey$rating)
```

-   `verb` contains the items to be rated for the conditions in
    `routine`

```{r}
unique(survey$verb)
```

-   `frequency` relates to the frequency bins of the verbs:

```{r}
unique(survey$freq)
```

-   `ParticipantID` identifies each of the 98 subjects who provided
    ratings
:::

## Modelling ordinal data

In order to answer our research question, we need to assess how the
features `freq` and `routine` affect the variability in the
acceptability ratings. Furthermore, we'll need to control for `verb` and
`ParticipantID`, which impose a hierarchical structure on our dataset.

We will denote these predictor variables $X_p$ where $p$ represents the
number of predictors. The target variable `rating` is our $Y$, i.e., our
response variable with the ordered, discrete outcomes
$y \in \{1, 2, 3, 4, 5\}$. Our goal is to find a model $f$ that
describes the relationship between $Y$ and $X_p$ as accurately as
possible and minimises the erorr term $\epsilon$:

$$
Y = f(X_1, X_2, ..., X_p) + \epsilon
$$

### Ordered logistic regression

One family of models that respects the ordered, yet categorical nature
of $Y$ is ordered (or ordinal) logistic regression. Other terms include
**proportional odds models**, **cumulative logit/link models** etc.

::: {.callout-note collapse="true" title="Recap: Logistic regression"}
Logistic regression is used to model categorical response variables with
two or more levels. For instance, let's assume our $Y$ is dichotomous
with the following two outcomes:

$$
Y =
\begin{cases}
\text{yes} \\
\text{no}
\end{cases}
$$

Using the logistic function, we can estimate the probability of one
outcome versus the other **given** the predictors $X_p$. Their
log-transformed odds ratio (**log odds**) is equivalent of the
all-too-familiar linear model:

$$
\log\left(\frac{P(Y = yes \mid X_p)}{1 - P(Y = yes \mid X_p)}\right) = \beta_0 + \beta_1X_1 + ... + \beta_pX_p.
$$
:::

::: {.callout-note collapse="true" title="Recap: Mixed-effects models"}
Certain datasets display a nested (or hierarchical) structure; this
becomes particularly apparent in the case of multiple observations.

(Add violation of independence)
:::

Core to this approach is the notion of **cumulative probabilities**. Let
$J$ denote the number of ordered categories in $Y$. In Glass's case
study ($J = 5$), the estimated cumulative probabilities for each ordered
outcome would have the following form:

$$
\begin{array}{rcl}
P(Y \leq 1) & = & P(Y = 1) \\
P(Y \leq 2) & = & P(Y = 1) + P(Y = 2) \\
P(Y \leq 3) & = & P(Y = 1) + P(Y = 2) + P(Y = 3) \\
P(Y \leq 4) & = & P(Y = 1) + P(Y = 2) + P(Y = 3) + P(Y = 4) \\
P(Y \leq 5) & = & P(Y = 1) + P(Y = 2) + P(Y = 3) + P(Y = 4) + P(Y = 5)
\end{array}
$$ We can generalise this pattern to

$$
P(Y \leq j) = \hat{P_1} + ... + \hat{P_j}
$$

We can now update our logistic regression equation to take into account
cumulative probabilities:

$$
\log\left(\frac{P(Y \leq j \mid X_p)}{1 - P(Y \leq j \mid X_p)}\right) = \beta_0 + \beta_1X_1 + ... + \beta_pX_p \qquad j = 1, ..., J-1
$$

### Application in R

### Using `polr()` from the `MASS` library

```{r}
library(MASS)
survey$rating <- factor(survey$rating, ordered = TRUE, levels = c("1", "2", "3", "4", "5"))

survey.polr <- polr(rating ~ 
                      freq +
                      routine,
                      data = survey)

summary(survey.polr)

## Get diagnostics
library(DescTools)
PseudoR2(survey.polr, c("Nagelkerke", "AIC"))

## Goodness-of-fit tests
library(generalhoslem)
lipsitz.test(survey.polr)
pulkrob.chisq(survey.polr, catvars = c("freq", "routine"))

## Test proportional odds assumption
library(brant)
brant(survey.polr) # p < 0.05 is a violation of the assumption

## Plot (best so far)
library(effects)
library(ggeffects)

## Frequency effect plot
plot(Effect(focal.predictors = c("freq"), mod = survey.polr), rug = FALSE, style = "stacked")

## Routine effect plot
plot(Effect(focal.predictors = c("routine"), mod = survey.polr), rug = FALSE, style="stacked")

## Interaction
plot(Effect(focal.predictors = c("routine", "freq"), mod = survey.polr), rug = FALSE,
     style="stacked")

```


```{r, echo = FALSE, output = FALSE}
# Get the ggeffects data
eff <- ggeffects::ggeffect(survey.polr, "freq")

# Convert to a data frame
plot_data <- as.data.frame(eff)

# Ensure the response.level has the desired levels
plot_data$response.level <- factor(plot_data$response.level, 
                                   levels = c("X1", "X2", "X3", "X4", "X5"),
                                   labels = c("1", "2", "3", "4", "5"))

# Create the plot with confidence intervals
p1 <- ggplot(plot_data, aes(x = x, y = predicted, color = response.level, group = response.level)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1, linewidth = 0.5) +
  scale_color_viridis_d() +
  labs(
    x = "Frequency",
    y = "Predicted Probability",
    color = "Rating"
  ) +
  facet_grid(~ response.level) +
  theme_minimal() +
  theme(
    legend.position = "right",
    panel.grid.minor = element_blank(),
    axis.title = element_text(face = "bold"),
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(margin = margin(b = 10))
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

p1


```

```{r, echo = FALSE, output = FALSE}
# Get the ggeffects data for "routine"
eff_routine <- ggeffects::ggeffect(survey.polr, "routine")

# Convert to a data frame
plot_data_routine <- as.data.frame(eff_routine)

# Ensure the response.level has the desired levels for "routine"
plot_data_routine$response.level <- factor(plot_data_routine$response.level, 
                                           levels = c("X1", "X2", "X3", "X4", "X5"),
                                           labels = c("1", "2", "3", "4", "5"))

# Create the second plot for "routine"
p2 <- ggplot(plot_data_routine, aes(x = x, y = predicted, color = response.level, group = response.level)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1, linewidth = 0.5) +
  scale_color_viridis_d() +
  labs(
    x = "Routine",
    y = "Predicted Probability",
    color = "Rating"
  ) +
  facet_grid(~ response.level) +
  theme_minimal() +
  theme(
    legend.position = "right",
    panel.grid.minor = element_blank(),
    axis.title = element_text(face = "bold"),
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(margin = margin(b = 10))
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

p2
```

```{r}
library(ggpubr)
ggarrange(p1, p2, ncol = 2, common.legend = TRUE, legend = "right")
```

```{r, echo = FALSE}

## Interaction plot
# Generate the interaction effects for "freq" and "routine"
eff_interaction <- ggeffect(survey.polr, terms = c("freq", "routine"))

# Convert to a data frame
plot_data_interaction <- as.data.frame(eff_interaction)

# Ensure the response.level has the desired levels
plot_data_interaction$response.level <- factor(plot_data_interaction$response.level, 
                                               levels = c("X1", "X2", "X3", "X4", "X5"),
                                               labels = c("1", "2", "3", "4", "5"))


# Create the interaction plot with facet by 'x' and color by 'response.level'
p_interaction <- ggplot(plot_data_interaction, aes(x = group, y = predicted, color = response.level, group = response.level)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1, linewidth = 0.5) +
  scale_color_viridis_d() +
  labs(
    title = "Predicted Probabilities for Interaction of Frequency and Routine",
    x = "Routine",
    y = "Predicted Probability",
    color = "Rating"
  ) +
  facet_wrap(~ x, labeller = labeller(x = c("hi" = "High Frequency", "lo" = "Low Frequency"))) +  # Facet by 'freq'
  theme_minimal() +
  theme(
    legend.position = "right",
    panel.grid.minor = element_blank(),
    axis.title = element_text(face = "bold"),
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(margin = margin(b = 10))
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

# Display the interaction plot
p_interaction



```



### Using `clm()` from the `ordinal` library

```{r}
# Convert to factors
survey$rating <- factor(survey$rating, ordered = TRUE, levels = c("1", "2", "3", "4", "5"))

clm.1 <- ordinal::clm(rating ~ 
                    freq +
                    routine,
                    data = survey, Hess=TRUE)

```


### Mixed-effects


```{r}

survey$rating <- factor(survey$rating, ordered = TRUE, levels = c("1", "2", "3", "4", "5"))
#survey$verb <- factor(survey$verb)
#survey$ParticipantID <- factor(survey$ParticipantID)
#survey$routine <- factor(survey$routine)
#survey$freq <- factor(survey$freq)

clm.2 <- ordinal::clmm(rating ~ 
                    freq * routine +
                    (1 | verb) +
                    (1 | ParticipantID),
                    data = survey, Hess=TRUE)

summary(clm.2)



```

```{r, echo = F, output = F}
## Plot random effects

# 1. Extract random effects
re_verb <- ranef(clm.2)$verb
re_participant <- ranef(clm.2)$ParticipantID

# 2. Create dataframes for random effects
df_verb <- data.frame(verb = rownames(re_verb), re = re_verb[,1])
df_participant <- data.frame(ParticipantID = rownames(re_participant), re = re_participant[,1])

# 3. Get predictions for an average case
pred_avg <- ggpredict(clm.2, terms = c("freq", "routine"))

# 4. Add random effects to predictions
pred_verb <- crossing(pred_avg, df_verb) %>%
  mutate(predicted = predicted + re)

pred_participant <- crossing(pred_avg, df_participant) %>%
  mutate(predicted = predicted + re)

# 5. Plot random effects for verbs
p_verb <- ggplot(pred_verb, aes(x = x, y = predicted, color = group, group = interaction(group, verb))) +
  geom_line(alpha = 1) +
  facet_wrap(~ response.level) +
  labs(title = "Random Effects for Verbs",
       x = "Frequency",
       y = "Predicted Probability",
       color = "Routine") +
  theme_minimal()

print(p_verb)

# 6. Plot random effects for participants
p_participant <- ggplot(pred_participant, aes(x = x, y = predicted, color = group, group = interaction(group, ParticipantID))) +
  geom_line(alpha = 1) +
  facet_wrap(~ response.level) +
  labs(title = "Random Effects for Participants",
       x = "Frequency",
       y = "Predicted Probability",
       color = "Routine") +
  theme_minimal()

print(p_participant)

```

```{r, echo = F, output = F}
# Second attempt
# Create a horizontal dot plot for random effects of participants
p_caterpillar <- ggplot(df_participant, aes(x = re, y = reorder(ParticipantID, re))) +
  geom_point(size = 3, color = "steelblue3") +  # Dots representing the random effects
  labs(title = "Random Effects for Participants", 
       x = "Random Effect Estimate (log odds)", 
       y = "Participant ID") +
   geom_vline(xintercept = 0, linetype = "dashed", color = "grey20") +
  theme_minimal() +
  theme(panel.grid.major.y = element_blank())  # Removes the gridlines for y-axis

p_caterpillar2 <- ggplot(df_verb, aes(x = re, y = reorder(verb, re))) +
  geom_point(size = 3, color = "steelblue3") +  # Dots representing the random effects
  labs(title = "Random Effects for Verbs", 
       x = "Random Effect Estimate (log odds)", 
       y = "Verb") +
   geom_vline(xintercept = 0, linetype = "dashed", color = "grey20") +
  theme_minimal() +
  theme(panel.grid.major.y = element_blank())  # Removes the gridlines for y-axis


```


```{r}
ggarrange(p_caterpillar, p_caterpillar2, ncol = 2, common.legend = TRUE, legend = "right")

```

