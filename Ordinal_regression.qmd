---
title: "Ordinal regression"
author: Vladimir Buskin
format:
  html:
    self-contained: true
    logo: logo.png
    footer: "Regression"
    theme: Reference
    toc: true
    number-sections: true
    slide-number: true
    incremental: false
    slide-level: 4
    scrollable: true
editor: visual
bibliography: R.bib
---

## Suggested reading

> @powersStatisticalMethodsCategorical2008: Chapter 7;
> @baguleySeriousStatsGuide2012: Chapter 17.4.5

## Introduction

In her recent contribution, @glassEnglishVerbsCan2021 examines possible
reasons why certain transitive verbs have a stronger affinity towards
object omission compared to others, placing special emphasis on the
routinisation of the actions denoted by the verbs. Specifically, she
analyses **how high/low-routine contexts affect the acceptability of
object omission** for transitive verbs from different frequency bins.

We will replicate her findings using her survey data
`Glass_2021_survey_processed.csv`[^ordinal_regression-1]:

[^ordinal_regression-1]: The original dataset can be retrieved from
    Lelia Glass's OSF repository: <https://osf.io/t6zw5> \[Last
    accessed: 27th September, 2024\].

```{r}
# Load libraries
library(tidyverse)
library(ordinal)

# Load data
survey <- read.csv("Glass_2021_survey_processed.csv")

# Inspect dataset
str(survey)
head(survey)
```

::: {.callout-note collapse="true" title="Short breakdown of the variables"}
-   `routine`: In Glass's study, transitive verbs were randomly assigned
    to one of the following conditions:

> -   (High routine condition:) I worked at my poultry farm. Just like I
>     always do, I butchered some chickens. Then I gathered some eggs.
>
> -   (Low-routine condition:) I visited a friend's job. Just because
>     people wanted me to try it, I butchered some chickens. Then I went
>     for a walk.
>
> Cf. Glass [-@glassEnglishVerbsCan2021: 66]

```{r}
unique(survey$routine)
```

-   `rating` records the responses of participants to a follow-up
    question regarding the acceptability of object omission. The answers
    are recorded on a 1-5 Likert scale.

> *The next time Caroline talks about butchering chickens the day
> before, how likely do you think she is to say the following?*
>
> 'I butchered yesterday'
>
> Cf. Glass [-@glassEnglishVerbsCan2021: 66]

```{r}
unique(survey$rating)
```

-   `verb` contains the items to be rated for the conditions in
    `routine`

```{r}
unique(survey$verb)
```

-   `frequency` relates to the frequency bins of the verbs:

```{r}
unique(survey$freq)
```

-   `ParticipantID` identifies each of the 98 subjects who provided
    ratings
:::

## Modelling ordinal data

Answering the research question means assessing how the features
`routine` and `freq` affect the variability in the acceptability
ratings. Furthermore, it is important to control for repeated
measurements for `verb` and `ParticipantID`, which impose a hierarchical
structure on the dataset.

Formally speaking, we have $p$ explanatory variables
$X_1, X_2, ..., X_p$ for $p \in \{1, ..., P\}$. The target variable
`rating` is our target variable $Y$ with the ordered, discrete outcomes
$y \in \{1, 2, 3, 4, 5\}$.

Our goal is to find a model $f$ that describes the relationship between
$Y$ and $X_p$ as accurately as possible and minimises the error term
$\epsilon$:

$$
Y = f(X_1, X_2, ..., X_p) + \epsilon
$$

### Ordered logistic regression

One family of models that respects the ordered, yet categorical nature
of $Y$ is ordered (or ordinal) logistic regression. Other terms include
**proportional odds models**, **cumulative logit/link models** etc.

::: {.callout-note collapse="true" title="Recap: Logistic regression"}
Logistic regression is used to model categorical response variables with
two or more levels. For instance, let's assume our $Y$ is dichotomous
with the following two outcomes:

$$
Y =
\begin{cases}
\text{yes} \\
\text{no}
\end{cases}
$$

Using the logistic function, we can estimate the probability of one
outcome versus the other **given** the predictors $X_p$. Their
log-transformed odds ratio (**log odds**) is equivalent of the
all-too-familiar linear model:

$$
\log\left(\frac{P(Y = yes \mid X_p)}{1 - P(Y = yes \mid X_p)}\right) = \beta_0 + \beta_1X_1 + ... + \beta_pX_p.
$$
:::

Core to this approach is the notion of **cumulative probabilities**. Let
$J$ denote the number of ordered categories in $Y$. In Glass's case
study ($J = 5$), the estimated cumulative probabilities for each ordered
outcome would have the following form:

$$
\begin{array}{rcl}
P(Y \leq 1) & = & P(Y = 1) \\
P(Y \leq 2) & = & P(Y = 1) + P(Y = 2) \\
P(Y \leq 3) & = & P(Y = 1) + P(Y = 2) + P(Y = 3) \\
P(Y \leq 4) & = & P(Y = 1) + P(Y = 2) + P(Y = 3) + P(Y = 4) \\
P(Y \leq 5) & = & P(Y = 1) + P(Y = 2) + P(Y = 3) + P(Y = 4) + P(Y = 5)
\end{array}
$$ We can generalise this pattern to

$$
P(Y \leq j) = \hat{P_1} + ... + \hat{P_j}
$$

We can now update our logistic regression equation to take into account
cumulative probabilities:

$$
\log\left(\frac{P(Y \leq j \mid X_p)}{1 - P(Y \leq j \mid X_p)}\right) = \beta_0 + \beta_1X_1 + ... + \beta_pX_p \qquad j = 1, ..., J-1
$$

### Application in R

### Using `polr()` from the `MASS` library

```{r, echo = TRUE, output = FALSE}
library(MASS)
# Convert to factor and determine ordering
survey$rating <- factor(survey$rating, ordered = TRUE, levels = c("1", "2", "3", "4", "5"))

# Fit polr model
survey.polr <- polr(rating ~ 
                      freq +
                      routine,
                      data = survey)

# Model summary
summary(survey.polr)
```

```{r}
library(sjPlot)
tab_model(survey.polr, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, transform = NULL)

## Get diagnostics
library(DescTools)
PseudoR2(survey.polr, c("Nagelkerke", "AIC"))

## Goodness-of-fit tests
library(generalhoslem)
lipsitz.test(survey.polr)
pulkrob.chisq(survey.polr, catvars = c("freq", "routine"))

## Test proportional odds assumption
library(brant)
brant(survey.polr) # p < 0.05 is a violation of the assumption

## Plot (best so far)
library(effects)
library(ggeffects)

## Frequency effect plot
#plot(Effect(focal.predictors = c("freq"), mod = survey.polr), rug = FALSE, style = "stacked")

## Routine effect plot
plot(Effect(focal.predictors = c("routine"), mod = survey.polr), rug = FALSE, style="stacked")

## Interaction
#plot(Effect(focal.predictors = c("routine", "freq"), mod = survey.polr), rug = FALSE,
 #    style="stacked")

```

```{r, echo = FALSE, output = FALSE}
# Get the ggeffects data
eff <- ggeffects::ggeffect(survey.polr, "freq")

# Convert to a data frame
plot_data <- as.data.frame(eff)

# Ensure the response.level has the desired levels
plot_data$response.level <- factor(plot_data$response.level, 
                                   levels = c("X1", "X2", "X3", "X4", "X5"),
                                   labels = c("1", "2", "3", "4", "5"))

# Create the plot with confidence intervals
p1 <- ggplot(plot_data, aes(x = x, y = predicted, color = response.level, group = response.level)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1, linewidth = 0.5) +
  scale_color_viridis_d() +
  labs(
    x = "Frequency",
    y = "Predicted Probability",
    color = "Rating"
  ) +
  facet_grid(~ response.level) +
  theme_minimal() +
  theme(
    legend.position = "right",
    panel.grid.minor = element_blank(),
    axis.title = element_text(face = "bold"),
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(margin = margin(b = 10))
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

p1


```

```{r, echo = FALSE, output = FALSE}
# Get the ggeffects data for "routine"
eff_routine <- ggeffects::ggeffect(survey.polr, "routine")

# Convert to a data frame
plot_data_routine <- as.data.frame(eff_routine)

# Ensure the response.level has the desired levels for "routine"
plot_data_routine$response.level <- factor(plot_data_routine$response.level, 
                                           levels = c("X1", "X2", "X3", "X4", "X5"),
                                           labels = c("1", "2", "3", "4", "5"))

# Create the second plot for "routine"
p2 <- ggplot(plot_data_routine, aes(x = x, y = predicted, color = response.level, group = response.level)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1, linewidth = 0.5) +
  scale_color_viridis_d() +
  labs(
    x = "Routine",
    y = "Predicted Probability",
    color = "Rating"
  ) +
  facet_grid(~ response.level) +
  theme_minimal() +
  theme(
    legend.position = "right",
    panel.grid.minor = element_blank(),
    axis.title = element_text(face = "bold"),
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(margin = margin(b = 10))
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

p2
```

```{r}
library(ggpubr)
ggarrange(p1, p2, ncol = 2, common.legend = TRUE, legend = "right")
```

```{r, echo = FALSE}

## Interaction plot
# Generate the interaction effects for "freq" and "routine"
eff_interaction <- ggeffect(survey.polr, terms = c("freq", "routine"))

# Convert to a data frame
plot_data_interaction <- as.data.frame(eff_interaction)

# Ensure the response.level has the desired levels
plot_data_interaction$response.level <- factor(plot_data_interaction$response.level, 
                                               levels = c("X1", "X2", "X3", "X4", "X5"),
                                               labels = c("1", "2", "3", "4", "5"))


# Create the interaction plot with facet by 'x' and color by 'response.level'
p_interaction <- ggplot(plot_data_interaction, aes(x = group, y = predicted, color = response.level, group = response.level)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1, linewidth = 0.5) +
  scale_color_viridis_d() +
  labs(
    title = "Predicted Probabilities for Interaction of Frequency and Routine",
    x = "Routine",
    y = "Predicted Probability",
    color = "Rating"
  ) +
  facet_wrap(~ x, labeller = labeller(x = c("hi" = "High Frequency", "lo" = "Low Frequency"))) +  # Facet by "freq"
  theme_minimal() +
  theme(
    legend.position = "right",
    panel.grid.minor = element_blank(),
    axis.title = element_text(face = "bold"),
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(margin = margin(b = 10))
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

# Display the interaction plot
p_interaction



```

### Using `clm()` from the `ordinal` library

```{r, echo = T, output = F}
# Convert to factor and determine ordering
survey$rating <- factor(survey$rating, ordered = TRUE, levels = c("1", "2", "3", "4", "5"))

# Fit cumulative link model
clm.1 <- ordinal::clm(rating ~ 
                    freq +
                    routine,
                    data = survey, Hess=TRUE)

# Model summary
summary(clm.1)
```

```{r}
tab_model(clm.1, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, transform = NULL)
```

### Mixed-effects

::: {.callout-note collapse="true" title="Recap: Mixed-effects models"}
If the data is nested according to some grouping factor with $1, ..., K$
groups, we can let the intercept and/or slopes vary by group. For
instance, recall the varying-intercept model:

$$
Y = \alpha_{k} + \beta_1X_{1} + \beta_2X_{2} + ... + \epsilon \qquad \alpha_{j} \sim N(\mu_{\alpha}, \sigma_{\alpha}^2).
$$ In this case we also speak of **random effects**.
:::

```{r, echo T, output = F}
# Convert to factor and determine ordering
survey$rating <- factor(survey$rating, ordered = TRUE, levels = c("1", "2", "3", "4", "5"))


# Fit mixed model with random intercepts for "verb" and "ParticipantID"
clm.2 <- ordinal::clmm(rating ~ 
                    freq * routine +
                    (1 | verb) +
                    (1 | ParticipantID),
                    data = survey, Hess=TRUE)

# Model summary
summary(clm.2)
```

```{r}
tab_model(clm.2, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, transform = NULL)
```

```{r, echo = F, output = F}
## Plot random effects

# 1. Extract random effects
re_verb <- ranef(clm.2)$verb
re_participant <- ranef(clm.2)$ParticipantID

# 2. Create dataframes for random effects
df_verb <- data.frame(verb = rownames(re_verb), re = re_verb[,1])
df_participant <- data.frame(ParticipantID = rownames(re_participant), re = re_participant[,1])

# 3. Get predictions for an average case
pred_avg <- ggpredict(clm.2, terms = c("freq", "routine"))

# 4. Add random effects to predictions
pred_verb <- crossing(pred_avg, df_verb) %>%
  mutate(predicted = predicted + re)

pred_participant <- crossing(pred_avg, df_participant) %>%
  mutate(predicted = predicted + re)

# 5. Plot random effects for verbs
p_verb <- ggplot(pred_verb, aes(x = x, y = predicted, color = group, group = interaction(group, verb))) +
  geom_line(alpha = 1) +
  facet_wrap(~ response.level) +
  labs(title = "Random Effects for Verbs",
       x = "Frequency",
       y = "Predicted Probability",
       color = "Routine") +
  theme_minimal()

print(p_verb)

# 6. Plot random effects for participants
p_participant <- ggplot(pred_participant, aes(x = x, y = predicted, color = group, group = interaction(group, ParticipantID))) +
  geom_line(alpha = 1) +
  facet_wrap(~ response.level) +
  labs(title = "Random Effects for Participants",
       x = "Frequency",
       y = "Predicted Probability",
       color = "Routine") +
  theme_minimal()

print(p_participant)

```

```{r, echo = F, output = F}
# Second attempt
# Create a horizontal dot plot for random effects of participants
p_caterpillar <- ggplot(df_participant, aes(x = re, y = reorder(ParticipantID, re))) +
  geom_point(size = 3, color = "steelblue3") +  # Dots representing the random effects
  labs(title = "Random Effects for Participants", 
       x = "Random Effect Estimate (log odds)", 
       y = "Participant ID") +
   geom_vline(xintercept = 0, linetype = "dashed", color = "grey20") +
  theme_minimal() +
  theme(panel.grid.major.y = element_blank())  # Removes the gridlines for y-axis

p_caterpillar2 <- ggplot(df_verb, aes(x = re, y = reorder(verb, re))) +
  geom_point(size = 3, color = "steelblue3") +  # Dots representing the random effects
  labs(title = "Random Effects for Verbs", 
       x = "Random Effect Estimate (log odds)", 
       y = "Verb") +
   geom_vline(xintercept = 0, linetype = "dashed", color = "grey20") +
  theme_minimal() +
  theme(panel.grid.major.y = element_blank())  # Removes the gridlines for y-axis

```

```{r, echo = FALSE, output = TRUE}
ggarrange(p_caterpillar, p_caterpillar2, ncol = 2, common.legend = TRUE, legend = "right")

```

## Generalised additive models (GAMs)

```{r}
library(mgcv)
library(itsadug)
library(gratia)

survey$ParticipantID <- as.factor(survey$ParticipantID)
survey$verb <- as.factor(survey$verb)

gam1 <- bam(as.numeric(rating) ~ 
              freq +
              routine +
              s(ParticipantID, bs = "re") +
              s(verb, bs = "re"),
              data = survey, 
              family = ocat(R=5)
            )

summary(gam1)

# Extract the thresholds on the linear predictor from the model
thresh <- gratia::theta(gam1) |>
  tibble::as_tibble() |>
  setNames(c("threshold"))

routine_pred <- ggpredict(gam1, terms = "routine")

routine_pred |>
  ggplot(aes(x = x, y = predicted)) +
  geom_point(col = "steelblue") +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = 0), col = "steelblue") +
  #geom_line() +
  geom_hline(data = thresh, aes(yintercept = threshold), linetype = "dashed") +
  theme_minimal()


verb_pred <- ggpredict(gam1, terms = "verb")

verb_pred |>
  ggplot(aes(x = x, y = predicted)) +
  geom_point(col = "steelblue") +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = 0), col = "steelblue") +
  #geom_line() +
  geom_hline(data = thresh, aes(yintercept = threshold), linetype = "dashed") +
  theme_minimal()



subj_pred <- ggpredict(gam1, terms = "ParticipantID")

subj_pred |>
  ggplot(aes(x = x, y = predicted)) +
  geom_point(col = "steelblue") +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = 0), col = "steelblue") +
  #geom_line() +
  geom_hline(data = thresh, aes(yintercept = threshold), linetype = "dashed") +
  theme_minimal()




```
