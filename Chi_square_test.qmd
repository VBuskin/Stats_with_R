---
title: "Chi-squared test"
author: Vladimir Buskin
format:
  html:
    self-contained: true
    code-fold: false
    theme: default
    toc: true
    toc-depth: 4
    number-sections: true
    slide-number: true
    incremental: false
    slide-level: 3
    scrollable: true
editor: visual
bibliography: R.bib
metadata-files:
  - _quarto.yml
google-scholar: false
---

## Preparation

```{r, echo = FALSE, include = FALSE}
library("readxl")
library("tidyverse")
```

-   Load packages:

```{r, echo = TRUE, eval = FALSE}
library("readxl")
library("tidyverse")
```

-   Load the data sets:

```{r, echo = TRUE, output = TRUE, warning = FALSE, message = FALSE}

cl.order <- read_xlsx("Paquot_Larsson_2020_data.xlsx")

```

## Overview

::: {.callout-note collapse="true" title="This unit in a nutshell"}
The chi-square test helps determine if there is a statistically
significant association between two categorical variables. It compares
observed frequencies (actual data) with expected frequencies (if the
null hypothesis were true). The greater the difference, the higher the
$\chi^2$ score, providing evidence against $H_0$

-   $p$-value: Indicates significance level.
-   $\chi^2$ score: Shows the deviation between observed and expected
    frequencies.
-   Degrees of freedom: Affects the shape of the chi-square
    distribution.

If the $p$-value is below 0.05, we reject $H_0$, indicating a
significant association between the variables.

Example:

```{r}
# Hypotheses:
# H0: ORDER and SUBORDTYPE are independent.
# H1: ORDER and SUBORDTYPE are not independent.

chisq.test(cl.order$ORDER, cl.order$SUBORDTYPE)

# We can reject H0 and accept H1.
```
:::

## The $\chi^2$-test

Having clearly stated our hypotheses in the previous unit, the next step
is to compute a test statistic that indicates how strongly our data
conforms to $H_0$. For instance, the Pearson $\chi^2$ statistic is
commonly used for categorical variables.

It requires two types of values: the **observed frequencies** $n_{ij}$
in our data set and the **expected frequencies** $m_{ij}$, which we
would expect to see if $H_0$ were true. The indices $i$ and $j$ uniquely
identify the frequencies found in all column-row combinations of a
cross-table.

::: {.callout-note collapse="true" title="Mathematical details: The structure of a contingency table"}
The table below represents a generic contingency table where $X$ and $Y$
are categorical variables. Each $x_i$ represents a category of $X$ and
each $y_j$ represents a category of $Y$. In the table, each cell
indicates the count of observation $n_{ij}$ corresponding to the $i$-th
row and $j$-th column.

|     |       |          | $Y$      |     |          |     |
|-----|-------|----------|----------|-----|----------|-----|
|     |       | $y_1$    | $y_2$    | ... | $y_J$    |     |
|     | $x_1$ | $n_{11}$ | $n_{12}$ | ... | $n_{1J}$ |     |
|     | $x_2$ | $n_{21}$ | $n_{22}$ | ... | $n_{2J}$ |     |
| $X$ | ...   | ...      | ...      | ... | ...      |     |
|     | $x_I$ | $n_{I1}$ | $n_{I2}$ | ... | $n_{3J}$ |     |

This is really just an abstraction of the cross-tables we've computed so
many times before.
:::

These are our observed frequencies $n_{ij}$:

```{r, echo = FALSE}
# Cross-tabulate the frequencies for the variables of interest

freqs <- table(cl.order$ORDER, cl.order$SUBORDTYPE)

print(freqs)

```

The expected frequencies $m_{ij}$ are calculated by

$$
m_{ij} = \frac{i\textrm{th row sum} \times j \textrm{th column sum}}{\textrm{number of observations}}.
$$

The expected frequencies for our combination of variables is shown
below. In which cells can you see the greatest deviations between
observed and expected frequencies?

```{r, echo = FALSE}
#| code-fold: true
#| code-summary: "Show the code"

# Compute expected frequencies

## Calculate row totals
row_totals <- rowSums(freqs)

## Calculate column totals
col_totals <- colSums(freqs)

## Total number of observations
total_obs <- sum(freqs)

## Calculate expected frequencies
expected_table <- outer(row_totals, col_totals) / total_obs

expected_table
```

The $\chi^2$-test now offers a convenient way of quantifying the
differences between the two tables above. Given $n$ observations and $k$
degrees of freedom ($df$)[^chi_square_test-1], the $\chi^2$-statistic
measures how much the observed frequencies **deviate** from the expected
frequencies **for each cell** in a contingency table [cf.
@heumann_introduction_2022: 249-251].

[^chi_square_test-1]: The degrees of freedom are calculated by
    $(\textrm{number of rows} -1) \times (\textrm{number of columns} - 1)$.

::: {.callout-note collapse="true" title="Mathematical details: Formal definition of the chi-square test"}
The joint deviations make up the final $\chi^2$-score, which is defined
as

$$
\chi^2 = \sum_{i=1}^{I}\sum_{i=j}^{J}{\frac{(n_{ij} - m_{ij})^2}{m_{ij}}}.
$$
:::

Note, however, that this test comes with certain statistical
assumptions. Violations of these assumptions decrease the validity of
the result and could, therefore, lead to wrong conclusions about
relationships in the data.

1.  All observations are independent of each other.
2.  80% of the expected frequencies are $\geq$ 5.
3.  All observed frequencies are $\geq$ 1.

In R, conducting this test is a simple one-liner:

```{r}
freqs_test <- chisq.test(freqs)

print(freqs_test)
```

The test object `freqs_test` also stores the expected frequencies, which
we can also access. They appear to be all right:

```{r}
# Expected frequencies
freqs_test$expected
```

::: callout-important
If the data does not meet the (expected) frequency requirements for the
$\chi^2$-test, the Fisher's Exact Test is a viable alternative (see
`?fisher.test()` for details).
:::

## Workflow in R

### Define hypotheses

-   $H_0:$ The variables `ORDER` and `SUBORDTYPE` are independent.

-   $H_1:$ The variables `ORDER` and `SUBORDTYPE` are **not**
    independent.

### Cross-tabulate the categories of interest

```{r}
# Cross-tabulate the frequencies for the variables of interest

freqs <- table(cl.order$ORDER, cl.order$SUBORDTYPE)

freqs ## Assumption met: all observed freqs => 1
```

### Run the test

```{r}
# Run a chis-quared test on the absolute frequencies and print the results

test <- chisq.test(freqs, correct = FALSE)

# Inspect expected frequencies

test$expected # Assumption met: all expected frequences => 5

```

::: {.callout-tip collapse="true" title="Advanced: Effect size"}
The sample-size independent effect size measure **Cramer's *V***
($\phi$) is defined as

$$V = \sqrt{\frac{\chi^2}{N \times df}}.$$ The outcome varies between
$0$ (= no correlation) and $1$ (= perfect correlation); cf. also Gries
[-@gries_statistics_2013: 186].

```{r, echo = TRUE, output = TRUE}
#| code-fold: true
# Compute Cramer's V

## By hand:

# Given chi-squared statistic
chi_squared <- unname(test$statistic)

# Total number of observations
total_obs <- sum(freqs)

sqrt(chi_squared / total_obs * (min(dim(freqs)) - 1))

## Automatically:
library("confintr") # Load library

cramersv(test)
```
:::

#### Reporting the results

According to a $\chi^2$-test, there is a highly significant association
between clause `ORDER`and `SUBORDTYPE` at $p < 0.001$
($\chi^2 = 106.44, df = 1$), thus justifying the rejection of $H_0$.
