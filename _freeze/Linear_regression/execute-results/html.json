{
  "hash": "7df6ea7922832b026b2baf3cc1f0f2f7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear regression\"\nauthor: Vladimir Buskin\nformat:\n  html:\n    self-contained: false\n    logo: logo.png\n    footer: \"Regression\"\n    theme: default\n    toc: true\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 4\n    scrollable: true\neditor: visual\nbibliography: R.bib\n---\n\n\n## Recommended reading\n\nSpecifically for linguists:\n\n> Levshina [-@levshina_how_2015: Chapter 7]\n>\n> Winter [-@winter_statistics_2020: Chapter 4]\n\nGeneral:\n\n> Heumann et al. [-@heumann_introduction_2022: Chapter 11]\n>\n> James et al. [-@james_introduction_2021: Chapter 3]\n\n## Preparation {.smaller}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(readxl)\nlibrary(writexl)\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(ggeffects)\n\n# Load data\nELP <- read_xlsx(\"ELP.xlsx\")\n\n# Inspect data structure\nstr(ELP)\n```\n:::\n\n\n## Introduction\n\nConsider the distribution of the continuous variable `RT` (reaction\ntimes) from the `ELP` (English Lexicon Project) dataset. We will apply a\n$\\log$-transformation to the the reaction times in order to even out the\ndifferences between extremely high and extremely low frequency counts\n[cf. @winter_statistics_2020: 90-94].\n\n::: panel-tabset\n### Log-transformed\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Linear_regression_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n### Untransformed\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Linear_regression_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n:::\n\nWe are particularly interested in the relationship between reaction\ntimes `RT` and the (log-)frequency `Freq` of a lexical stimulus. What\nkind of pattern does the scatter plot below suggest?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Linear_regression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n::: {.callout-caution title=\"Some open questions\" collapse=\"false\"}\n-   Can word frequency help us **explain variation** in reaction times?\n\n-   If it can, then how could we characterise the **effect** of word\n    frequency? In other words, does it increase or decrease reaction\n    times?\n\n-   What reaction times should we expect for **new observations**?\n:::\n\n### A simple statistical model\n\n`RT` is the *response* or *target* that we wish to explain. We\ngenerically refer to the response as $Y$.\n\n`Freq` is the *feature*, *input*, or *predictor*, which we will call\n$X$.\n\nWe can thus summarise our preliminary and fairly general statistical\nmodel as\n\n$$Y = f(X) + \\epsilon.$$\n\nThe term $f(X)$ describes the contribution of $X$ to the explanation of\n$Y$. Since no model can explain everything perfectly, we expect there to\nbe some degree of error $\\epsilon$.\n\n## Linear regression\n\n-   Linear regression is a simple approach to supervised machine\n    learning where the response variable is known.[^linear_regression-1]\n\n-   It assumes that the dependence of $Y$ on $X$ is **linear**, i.e.,\n    their relationship is a straight line.\n\n-   This approach is suitable for **numerical response variables**. The\n    predictors, however, can be either continuous or discrete.\n\n[^linear_regression-1]: If the response variable is unknown or\n    irrelevant, we speak of **unsupervised machine learning**.\n    Unsupervised models are mostly concerned with finding patterns in\n    high-dimensional datasets with dozens or even hundreds of variables.\n\nAlthough it may seem overly simplistic, linear regression is **extremely\nuseful** both conceptually and practically.\n\n### Model with a single predictor $X$ {.smaller}\n\nThe simple linear model has the general form\n\n$$ Y = \\beta_0 + \\beta_1X + \\epsilon. $$ The model parameters (or\ncoefficients) $\\beta_0$ and $\\beta_1$ specify the functional\nrelationship $f$ between $Y$ and $X$. The first parameter $\\beta_0$\ndetermines the **intercept** of the regression line, and $\\beta_1$\nindicates the **slope**. Once again, $\\epsilon$ captures the model\nerror.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Linear_regression_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nApplying the model formula to our dataset, we get the following updated\nregression equation:\n\n$$ \\text{Reaction time} = \\beta_0 + \\beta_1\\text{Frequency} + \\text{Model Error}. $$But\nhow do we find the right values for the intercept and the slope? In\nshort: We can't. We are dealing with population parameters and can,\ntherefore, only provide an approximation of the true relationship\nbetween `RT` and `Frequency`, but will never capture the true one. To\nreflect the tentative nature of the model coefficients, we use the\n**hat** symbol \\^ (e.g., $\\hat{\\beta_0}$) to indicate **estimations**\nrather than true values. The estimation procedure requires **training\ndata**, based on which the algorithm \"learns\" the relationship between\n$Y$ and $X$ (hence \"Machine Learning\").\n\n::: {.callout-tip title=\"How exactly do you estimate the model parameters?\" collapse=\"true\"}\nThe most common way of estimating parameters for linear models is the\n**Least Squares** approach. In essence, the parameters are chosen such\nthat the residual sum of squares, i.e., the sum of the differences\nbetween observed and predicted values, is as low as possible. In other\nwords, the distances between the data points and the regression line\nshould be minimised. More formally, the estimated slope then corresponds\nto\n\n$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i- \\bar{y})}{\\sum_{i=1}^n(x_i- \\bar{x})^2}.$$\n\nWe can then obtain the intercept:\n\n$$\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta}_1\\bar{x}\n$$\n:::\n\nOnce we've fitted the model, we can then predict reaction times if we\nknow the frequency of a lexical stimulus:\n\n$$\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x,\n$$\n\nwhere $\\hat{y}$ indicates a prediction of $Y$ on the basis of the\npredictor values $X = x$.\n\n### Application in R\n\nIn R, we can fit a linear model with the `lm()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit linear model\nrt.lm1 = lm(log(RT) ~ log(Freq), data = ELP)\n\n# View model data\nsummary(rt.lm1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(RT) ~ log(Freq), data = ELP)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.29765 -0.08203 -0.01205  0.07298  0.43407 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.633361   0.004286 1547.82   <2e-16 ***\nlog(Freq)   -0.048602   0.002201  -22.08   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1235 on 878 degrees of freedom\nMultiple R-squared:  0.357,\tAdjusted R-squared:  0.3563 \nF-statistic: 487.5 on 1 and 878 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe model statistics comprise the following elements:\n\n::: {.callout-tip title=\"Call\" collapse=\"true\"}\ni.e., the model formula.\n:::\n\n::: {.callout-tip title=\"Residuals\" collapse=\"true\"}\nThese indicate the difference between the observed values in the data\nset and the values predicted by the model (= the fitted values). These\ncorrespond to the error term $\\epsilon$. The lower the residuals, the\nbetter the model describes the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Show fitted values (= predictions) for the first six observations\nhead(rt.lm1$fitted.values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2        3        4        5        6 \n6.635345 6.563152 6.789806 6.613980 6.630529 6.574894 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Show deviation of the fitted values from the observed values\nhead(rt.lm1$residuals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          1           2           3           4           5           6 \n 0.03778825 -0.02277165  0.07759556  0.03387713  0.15222949 -0.10432670 \n```\n\n\n:::\n:::\n\n:::\n\n::: {.callout-tip title=\"Coefficients\" collapse=\"true\"}\nThe regression coefficients correspond to $\\hat{\\beta}_0$ (\"Intercept\")\nand $\\hat{\\beta}_1$ (\"log(Freq)\"), respectively. The model shows that\nfor a one-unit increase in log-frequency the log-reaction time decreases\nby approx. -0.05.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert coefficients to a tibble \nlibrary(\"broom\")\n\ntidy_model <- tidy(rt.lm1)\n\ntidy_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   6.63     0.00429    1548.  0       \n2 log(Freq)    -0.0486   0.00220     -22.1 2.88e-86\n```\n\n\n:::\n:::\n\n:::\n\n::: {.callout-tip title=\"$p$-values and $t$-statistic\" collapse=\"true\"}\n$p$**-values and** $t$**-statistic**: Given the null hypothesis $H_0$\nthat there is no correlation between `log(RT)` and `log(Freq)` (i.e.,\n$H_0: \\beta_1 = 0$), a $p$-value lower than 0.05 indicates that\n$\\beta_1$ considerably deviates from 0, thus providing evidence for the\nalternative hypothesis $H_1: \\beta_1 \\ne 0$. Since $p < 0.001$, we can\nreject $H_0$.\n\n```         \nThe $p$-value itself crucially depends on the\n$t$-statistic[^linear_regression-1], which measures \"the number of\nstandard deviations that $\\hat{\\beta_1}$ is away from 0\"\n[@james_introduction_2021: 67] . The standard error (SE) reflects\nhow much an estimated coefficient differs on average from the true\nvalues of $\\beta_0$ and $\\beta_1$. They can be used to compute the\n95% confidence interval\n$[\\hat{\\beta}_1 - 2 \\cdot SE(\\hat{β}_1), \\hat{\\beta}_1 + 2 \\cdot SE(\\hat{β}_1)]$;\nthe true estimate of the parameter $\\beta_1$ lies within the\nspecified range 95% of the time.\n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute confidence intervals for intercept and log(Freq)\ntidy_model_ci <- tidy(rt.lm1, conf.int = TRUE)\n\ntidy_model_ci\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   6.63     0.00429    1548.  0          6.62      6.64  \n2 log(Freq)    -0.0486   0.00220     -22.1 2.88e-86  -0.0529   -0.0443\n```\n\n\n:::\n:::\n\n\nThe estimated parameter for `log(Freq)`, which is -0.049, thus has the\n95% confidence interval \\[-0.053, -0.044\\].\n:::\n\n::: {.callout-tip title=\"**Residual standard error** (RSE)\" collapse=\"true\"}\nThis is an estimation of the average deviation of the predictions from\nthe observed values.\n\n$$RSE = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n (y_i - \\hat{y_i}})^2$$\n:::\n\n::: {.callout-tip title=\"$R^2$\" collapse=\"true\"}\nThe $R^2$ score is important for assessing model fit because it\n\"measures the proportion of variability in $Y$ that can be explained\nusing $X$\" [@james_introduction_2021: 70; emphasis removed], varying\nbetween 0 and 1.\n\n$$R^2 = 1-\\frac{TSS}{RSS} = 1-\\frac{\\sum_{i=1}^n (y_i - \\hat{y_i})^2}{\\sum_{i=1}^n (y_i - \\bar{y_i})^2}$$\n:::\n\n::: {.callout-tip title=\"$F$-statistic\" collapse=\"true\"}\nIt is used to measure the association between the dependent variable and\nthe independent variable(s). Generally speaking, values greater than 1\nindicate a possible correlation. A sufficiently low $p$-value suggests\nthat the null hypothesis $H_0: \\beta_1 = 0$ can be rejected. The $F$\nstatistic is computed as shown below [cf.\n@agrestiFoundationsStatisticsData2022: 232] and follows an\n$F$-distribution with two different $df$ values.\n\n$$\nF = \\frac{(TSS - SSE) / p}{SSE / [n - (p + 1)]}\n$$\n:::\n\n### Multiple linear regression\n\nIn multiple linear regression, more than one predictor variable is taken\ninto account. For instance, modelling `log(RT)` as a function of\n`log(Freq)`, `POS` and `Length` requires a more complex model of the\nform\n\n$$ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon.$$\n\nPredictions are then obtained via the formula\n\n$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2 + ... + \\hat{\\beta}_px_p.$$\n\n### Application in R\n\nIn R, a multiple regression model is fitted as in the code example\nbelow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit multiple regression model\nrt.lm2 <- lm(log(RT) ~ log(Freq) + POS + Length, data = ELP)\n\n# View model statistics\nsummary(rt.lm2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(RT) ~ log(Freq) + POS + Length, data = ELP)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.26955 -0.07853 -0.00672  0.07067  0.39528 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.459742   0.016946 381.205  < 2e-16 ***\nlog(Freq)   -0.038071   0.002130 -17.874  < 2e-16 ***\nPOSNN       -0.006242   0.010157  -0.615  0.53902    \nPOSVB       -0.035234   0.012125  -2.906  0.00375 ** \nLength       0.023094   0.001711  13.495  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1114 on 875 degrees of freedom\nMultiple R-squared:  0.4784,\tAdjusted R-squared:  0.476 \nF-statistic: 200.6 on 4 and 875 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n## Visualising regression models\n\nPlot coefficient estimates:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\n# Tidy the model output\ntidy_model <- tidy(rt.lm2, conf.int = TRUE)\n\n# Remove intercept\ntidy_model <- tidy_model %>% filter(term != \"(Intercept)\")\n\n# Create the coefficient plot\nggplot(tidy_model, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient Estimate\",\n    y = \"Predictor\",\n    title = \"Coefficient Estimates with Confidence Intervals\"\n  )\n```\n\n::: {.cell-output-display}\n![](Linear_regression_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n### Plotting effects\n\nPlot contributions of individual variable values:\n\n\n\n\n\n### Plotting predictions\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\nplot(ggeffect(rt.lm2, \"Freq\"), residuals = TRUE) + geom_line(col = \"steelblue\")\n```\n\n::: {.cell-output-display}\n![](Linear_regression_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\nplot(ggeffect(rt.lm2, \"POS\"), residuals = TRUE) + geom_line(col = \"steelblue\")\n```\n\n::: {.cell-output-display}\n![](Linear_regression_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\nplot(ggeffect(rt.lm2, \"Length\"), residuals = TRUE) + geom_line(col = \"steelblue\")\n```\n\n::: {.cell-output-display}\n![](Linear_regression_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\n#ggarrange(eff1, eff2, eff3)\n```\n:::\n\n\n## Model assumptions and diagnostics\n\nAs a parametric method, linear regression makes numerous assumptions\nabout the training data. It is, therefore, essential to run further\ntests to rule out possible violations. Among other things, the model\nassumptions include:\n\n-   A **linear relationship** between the response and the quantitative\n    predictors: The residuals should not display a clear pattern. For\n    this reason, it is recommended to use component residual plots\n    (e.g., `crPlot()` from the `car` library) for the visual\n    identification of potentially non-linear trends.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\nlibrary(car)\n\n# pink line = main tendency vs. blue line = slope coefficients;\n# some minor non-linearity can be observed\n\ncrPlot(rt.lm2, var = \"log(Freq)\") \n```\n\n::: {.cell-output-display}\n![](Linear_regression_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\ncrPlot(rt.lm2, var = \"POS\")\n```\n\n::: {.cell-output-display}\n![](Linear_regression_files/figure-html/unnamed-chunk-15-2.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\ncrPlot(rt.lm2, var = \"Length\") # potentially problematic\n```\n\n::: {.cell-output-display}\n![](Linear_regression_files/figure-html/unnamed-chunk-15-3.png){width=672}\n:::\n:::\n\n\n-   **No heteroscedasticity** (i.e, non-constant variance of error\n    terms): Visually, a violation of this assumption becomes apparent if\n    the residuals form a funnel-like shape. It is also possible to\n    conduct a non-constant variance test `ncvTest()`: If it returns\n    $p$-values \\< 0.05, it suggests non-constant variance.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\nplot(rt.lm2, which = 1)\n```\n\n::: {.cell-output-display}\n![](Linear_regression_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\nncvTest(rt.lm2) # significant, meaning that errors do not vary constantly\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 30.0101, Df = 1, p = 4.298e-08\n```\n\n\n:::\n:::\n\n\n-   **No multicollinearity**: Predictors should not be correlated with\n    each other. In the model data, correlated variables have unusually\n    high standard errors, thereby decreasing the explanatory power of\n    both the coefficients and the model as a whole. Another diagnostic\n    measure are variance inflation factors (VIF-scores); predictors with\n    VIF scores \\> 5 are potentially collinear. They can be computed\n    using the `vif()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\nvif(rt.lm2) # vif < 5 indicates that predictors are not correlated\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              GVIF Df GVIF^(1/(2*Df))\nlog(Freq) 1.150140  1        1.072446\nPOS       1.026925  2        1.006664\nLength    1.151054  1        1.072872\n```\n\n\n:::\n:::\n\n\n-   **Normally distributed residuals**: The residuals should follow the\n    normal distribution. Usually, a visual inspection using `qqnorm()`\n    is sufficient, but the Shapiro-Wilke test `shapiro.test()` can also\n    be run on the model residuals. Note that a $p$-value below 0.05\n    provides evidence for non-normality.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\nqqnorm(rt.lm2$residuals) # or\n```\n\n::: {.cell-output-display}\n![](Linear_regression_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\nplot(rt.lm2, which = 2)\n```\n\n::: {.cell-output-display}\n![](Linear_regression_files/figure-html/unnamed-chunk-18-2.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\nshapiro.test(residuals(rt.lm2)) # residuals are not normally distributed because p < 0.05\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(rt.lm2)\nW = 0.99062, p-value = 2.139e-05\n```\n\n\n:::\n:::\n\n\n::: callout-important\nBeside the points mentioned above, it is always recommend to examine the\nmodel with regard to\n\n-   **outliers** that might skew the regression estimates,\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\ninfluencePlot(rt.lm2, id.method = \"identify\")\n```\n\n::: {.cell-output-display}\n![](Linear_regression_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       StudRes         Hat       CookD\n16   0.4910868 0.027128347 0.001346143\n207 -0.7674391 0.030963411 0.003765568\n452  3.3366052 0.009338916 0.020749639\n498  3.5794954 0.004047706 0.010275907\n660  3.0847082 0.008750676 0.016638370\n```\n\n\n:::\n:::\n\n\n-   **interactions**, i.e., combined effects of predictors, and\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\nrt.lm.int <- lm(log(RT) ~ log(Freq) + POS + Length + log(Freq):Length, data = ELP)\n\nsummary(rt.lm.int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(RT) ~ log(Freq) + POS + Length + log(Freq):Length, \n    data = ELP)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27355 -0.07723 -0.00651  0.06623  0.39971 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       6.4669304  0.0171836 376.343  < 2e-16 ***\nlog(Freq)        -0.0243748  0.0062631  -3.892 0.000107 ***\nPOSNN            -0.0054973  0.0101366  -0.542 0.587735    \nPOSVB            -0.0344559  0.0120992  -2.848 0.004506 ** \nLength            0.0217620  0.0018007  12.085  < 2e-16 ***\nlog(Freq):Length -0.0017681  0.0007606  -2.325 0.020319 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1111 on 874 degrees of freedom\nMultiple R-squared:  0.4816,\tAdjusted R-squared:  0.4786 \nF-statistic: 162.4 on 5 and 874 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\n# ANOVA (analysis of variance)\n\n## Compare interaction model with main effects model\n\nanova(rt.lm.int, rt.lm2) # interaction term improves the model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: log(RT) ~ log(Freq) + POS + Length + log(Freq):Length\nModel 2: log(RT) ~ log(Freq) + POS + Length\n  Res.Df    RSS Df Sum of Sq     F  Pr(>F)  \n1    874 10.792                             \n2    875 10.858 -1 -0.066726 5.404 0.02032 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n-   **overfitting**, which results in poor model performance outside the\n    training data.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\nlibrary(\"rms\")\n\n# Refit the model with ols(), which is equivalent to lm()\nols.rt <- ols(log(RT) ~ log(Freq) + POS + Length, data = ELP, x = TRUE, y = TRUE)\n\n# Cross-validate\nols.val <- validate(ols.rt, bw = TRUE, B = 200) # Perform 200 random resampling iterations (= bootstrapping); compare model performance on training vs. test (= new) data. The slope optimism should be below 0.05 to rule out overfitting.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t\tBackwards Step-down - Original Model\n\nNo Factors Deleted\n\nFactors in Final Model\n\n[1] Freq   POS    Length\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\nols.val[,1:5] # The model does not overfit.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          index.orig   training       test      optimism index.corrected\nR-square  0.47839094 0.48018549 0.47545976  0.0047257363      0.47366520\nMSE       0.01233904 0.01224972 0.01240838 -0.0001586524      0.01249769\ng         0.11875152 0.11875584 0.11855692  0.0001989261      0.11855260\nIntercept 0.00000000 0.00000000 0.01792553 -0.0179255255      0.01792553\nSlope     1.00000000 1.00000000 0.99725477  0.0027452325      0.99725477\n```\n\n\n:::\n:::\n\n:::\n",
    "supporting": [
      "Linear_regression_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}