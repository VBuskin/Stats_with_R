{
  "hash": "683c537b9c040f3f7b18ec85d191f578",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear regression\"\nauthor: Vladimir Buskin\nformat:\n  html:\n    self-contained: false\n    logo: logo.png\n    footer: \"Regression\"\n    theme: default\n    toc: true\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 4\n    scrollable: true\neditor: visual\nbibliography: R.bib\n---\n\n\n\n## Recommended reading\n\n> The following introduction is based on Heumann et al.\n[-@heumann_introduction_2022: Chapter 11], James et al.\n[-@james_introduction_2021: Chapter 3], Levshina [-@levshina_how_2015:\nChapter 7] and Winter [-@winter_statistics_2020: Chapter 4].\n\n## Preparation {.smaller}\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\n\nlibrary(\"readxl\")\nlibrary(\"writexl\")\nlibrary(\"tidyverse\")\n\n# Load file\nELP <- read_xlsx(\"ELP.xlsx\")\n\n# Inspect file structure\nstr(ELP)\n```\n:::\n\n\n\n## Introduction\n\nConsider the distribution of the continuous variable `RT` (reaction\ntimes) from the `ELP` (English Lexicon Project) dataset. We will\n$log$-transform the reaction times to even out the differences between\nextremely high and extremely low frequency counts [cf.\n@winter_statistics_2020: 90-94].\n\n::: panel-tabset\n### Log-transformed\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Linear_regression_files/figure-pdf/unnamed-chunk-3-1.pdf)\n:::\n:::\n\n\n\n### Default\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Linear_regression_files/figure-pdf/unnamed-chunk-4-1.pdf)\n:::\n:::\n\n\n:::\n\nWe are investigating the relationship between reaction times `RT` and\nthe frequency `Freq` of a lexical stimulus.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Linear_regression_files/figure-pdf/unnamed-chunk-5-1.pdf)\n:::\n:::\n\n\n\n**Some open questions**:\n\n-   Can word frequency help us explain variation in reaction times?\n\n-   If it can, then how could we characterise the effect of word\n    frequency? In other words, does it increase or decrease reaction\n    times?\n\n-   What reaction times should we expect for new observations?\n\n### Building a statistical model\n\nHere `RT` is a *response* or *target* that we wish to explain. We\ngenerically refer to the response as $Y$.\n\n`Freq` is a *feature*, *input*, or *predictor*, which we name $X$.\n\nWe can thus summarise our preliminary and fairly general statistical\nmodel as\n\n$$ Y = f(X) + \\epsilon. $$While the term $f(X)$ denotes the contribution\nof $X$ to the explanation of $Y$, $\\epsilon$ describes the errors of the\nmodel.\n\n## Linear Regression\n\nLinear regression is a simple approach to supervised machine learning\nwhere the response variable is known. It assumes that the dependence of\n$Y$ on $X$ is **linear**. This approach is suitable for **numerical\nresponse variables**. The predictors, however, can be either numerical\nor discrete/categorical. Although it may seem overly simplistic, linear\nregression is **extremely useful** both conceptually and practically.\n\n### Model with a single predictor $X$ {.smaller}\n\nA linear model of our data would have the form\n\n$$ Y = \\beta_0 + \\beta_1X + \\epsilon $$\n\nor, in more concrete terms,\n\n$$ \\text{Reaction time} = \\beta_0 + \\beta_1\\text{Frequency} + \\text{Model Error,} $$\n\nwhere $\\beta_0$ and $\\beta_1$ are two unknown constants that represent\nthe **intercept** and **slope** of the regression line, respectively.\nTogether they are referred to as the model **coefficients** (or\nparameters), and $\\epsilon$ is the error term. The fact that assumptions\nare made about the form of the model renders it a **parametric model**.\n\nGiven the existing data on $X$ and $Y$, which is also known as the\ntraining data, we **estimate** the model coefficients $\\beta_0$ and\n$\\beta_1$. While we use the training data to estimate these\ncoefficients, we cannot know the true values of the coefficients, i.e.,\nthe exact true relationship between the variables. Their tentative nature is indicated by the **hat**\nsymbol \\^ above them: $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$.\n\n::: {.callout-tip title=\"How are the coefficients estimated?\" collapse=\"true\"}\n\nThe most common way of estimating parameters for linear models is the\n**Least Squares** approach. In essence, the parameters are chosen such\nthat the residual sum of squares, i.e., the sum of the differences\nbetween observed and predicted values, is as low as possible. More\nformally, the estimated slope then corresponds to\n\n$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i- \\bar{y})}{\\sum_{i=1}(x_i- \\bar{x})^2}.$$\n\nNow we can obtain the intercept:\n\n$$\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta}_1\\bar{x}\n$$\n:::\n\nWe can then predict future sales using the formula\n\n$$\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x,\n$$\n\nwhere $\\hat{y}$ indicates a prediction of $Y$ on the basis of the\npredictor values $X = x$.\n\nIn R, we can fit a linear model with the `lm()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit linear model\n\nrt.lm1 = lm(log(RT) ~ log(Freq), data = ELP)\n\n# View model data\n\nsummary(rt.lm1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(RT) ~ log(Freq), data = ELP)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.29765 -0.08203 -0.01205  0.07298  0.43407 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.633361   0.004286 1547.82   <2e-16 ***\nlog(Freq)   -0.048602   0.002201  -22.08   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1235 on 878 degrees of freedom\nMultiple R-squared:  0.357,\tAdjusted R-squared:  0.3563 \nF-statistic: 487.5 on 1 and 878 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\nThe model statistics comprise the following elements:\n\n::: {.callout-tip title=\"Call\" collapse=\"true\"}\ni.e., the model formula.\n:::\n\n::: {.callout-tip title=\"Residuals\" collapse=\"true\"}\nThese indicate the difference between the observed\n    values in the data set and the values predicted by the model (= the\n    fitted values). These correspond to the error term $\\epsilon$. The\n    lower the residuals, the better the model describes the data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Show fitted values (= predictions) for the first six observations\n\nhead(rt.lm1$fitted.values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2        3        4        5        6 \n6.635345 6.563152 6.789806 6.613980 6.630529 6.574894 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Show deviation of the fitted values from the observed values\n\nhead(rt.lm1$residuals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          1           2           3           4           5           6 \n 0.03778825 -0.02277165  0.07759556  0.03387713  0.15222949 -0.10432670 \n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.callout-tip title=\"Coefficients\" collapse=\"true\"}\n\nThe regression coefficients correspond to\n    $\\hat{\\beta}_0$ (\"Intercept\") and $\\hat{\\beta}_1$ (\"log(Freq)\"),\n    respectively. The model shows that for a one-unit increase in\n    log-frequency the log-reaction time decreases by approx. -0.05.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert coefficients to a tibble \n\nlibrary(\"broom\")\n\ntidy_model <- tidy(rt.lm1)\n\ntidy_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   6.63     0.00429    1548.  0       \n2 log(Freq)    -0.0486   0.00220     -22.1 2.88e-86\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.callout-tip title=\"$p$-values and $t$-statistic\" collapse=\"true\"}\n\n$p$**-values and** $t$**-statistic**: Given the null hypothesis\n    $H_0$ that there is no correlation between `log(RT)` and `log(Freq)`\n    (i.e., $H_0: \\beta_1 = 0$), a $p$-value lower than 0.05 indicates\n    that $\\beta_1$ considerably deviates from 0, thus providing evidence\n    for the alternative hypothesis $H_1: \\beta_1 \\ne 0$. Since\n    $p < 0.001$, we can reject $H_0$.\n\n    The $p$-value itself crucially depends on the\n    $t$-statistic[^linear_regression-1], which measures \"the number of\n    standard deviations that $\\hat{\\beta_1}$ is away from 0\"\n    [@james_introduction_2021: 67] . The standard error (SE) reflects\n    how much an estimated coefficient differs on average from the true\n    values of $\\beta_0$ and $\\beta_1$. They can be used to compute the\n    95% confidence interval\n    $[\\hat{\\beta}_1 - 2 \\cdot SE(\\hat{β}_1), \\hat{\\beta}_1 + 2 \\cdot SE(\\hat{β}_1)]$;\n    the true estimate of the parameter $\\beta_1$ lies within the\n    specified range 95% of the time.\n\n[^linear_regression-1]: $$t = \\frac{\\hat{\\beta_1} - 0}{SE(\\hat{\\beta_0})}$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute confidence intervals for intercept and log(Freq)\n\ntidy_model_ci <- tidy(rt.lm1, conf.int = TRUE)\n\ntidy_model_ci\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   6.63     0.00429    1548.  0          6.62      6.64  \n2 log(Freq)    -0.0486   0.00220     -22.1 2.88e-86  -0.0529   -0.0443\n```\n\n\n:::\n:::\n\n\n\nThe estimated parameter for `log(Freq)`, which is -0.049, thus has the\n95% confidence interval \\[-0.053, -0.044\\].\n\n:::\n\n::: {.callout-tip title=\"**Residual standard error** (RSE)\" collapse=\"true\"}\nThis is an estimation of the average deviation of the predictions from the observed values.\n\n$$RSE = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n (y_i - \\hat{y_i}})^2$$\n\n:::\n\n::: {.callout-tip title=\"$R^2$\" collapse=\"true\"}\n\nThe $R^2$ score is important for assessing model fit\n    because it \"measures the proportion of variability in $Y$ that can\n    be explained using $X$\" [@james_introduction_2021: 70; emphasis\n    removed], varying between 0 and 1.\n    \n$$R^2 = 1-\\frac{TSS}{RSS} = 1-\\frac{\\sum_{i=1}^n (y_i - \\hat{y_i})^2}{\\sum_{i=1}^n (y_i - \\bar{y_i})^2}$$\n\n:::\n\n::: {.callout-tip title=\"$F$-statistic\" collapse=\"true\"}\n\nIt is used to measure the association between the\n    dependent variable and the independent variable(s). Generally\n    speaking, values greater than 1 indicate a possible correlation. A sufficiently very low $p$-value suggests that the null hypothesis\n    $H_0: \\beta_1 = 0$ can be rejected.\n    \n:::\n\n\n### Multiple linear regression\n\nIn multiple linear regression, more than one predictor variable is taken\ninto account. For instance, modelling `log(RT)` as a function of\n`log(Freq)`, `POS` and `Length` requires a more complex model of the\nform\n\n$$ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon.$$\n\nPredictions are then obtained via the formula\n\n$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2 + ... + \\hat{\\beta}_px_p.\n$$\n\nIn R, a multiple regression model is fitted as in the code example\nbelow:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit multiple regression model\n\nrt.lm2 <- lm(log(RT) ~ log(Freq) + POS + Length, data = ELP)\n\n# View model statistics\n\nsummary(rt.lm2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(RT) ~ log(Freq) + POS + Length, data = ELP)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.26955 -0.07853 -0.00672  0.07067  0.39528 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.459742   0.016946 381.205  < 2e-16 ***\nlog(Freq)   -0.038071   0.002130 -17.874  < 2e-16 ***\nPOSNN       -0.006242   0.010157  -0.615  0.53902    \nPOSVB       -0.035234   0.012125  -2.906  0.00375 ** \nLength       0.023094   0.001711  13.495  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1114 on 875 degrees of freedom\nMultiple R-squared:  0.4784,\tAdjusted R-squared:  0.476 \nF-statistic: 200.6 on 4 and 875 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n## Visualising regression models\n\nPlot coefficient estimates:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tidy the model output\ntidy_model <- tidy(rt.lm2, conf.int = TRUE)\n\n# Remove intercept\ntidy_model <- tidy_model %>% filter(term != \"(Intercept)\")\n\n# Create the coefficient plot\nggplot(tidy_model, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient Estimate\",\n    y = \"Predictor\",\n    title = \"Coefficient Estimates with Confidence Intervals\"\n  )\n```\n\n::: {.cell-output-display}\n![](Linear_regression_files/figure-pdf/unnamed-chunk-11-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nPlot contributions of individual variable values:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot marginal effects\n\nlibrary(\"effects\")\n\nplot(Effect(\"Freq\", mod = rt.lm2))\n```\n\n::: {.cell-output-display}\n![](Linear_regression_files/figure-pdf/unnamed-chunk-12-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nplot(Effect(\"POS\", mod = rt.lm2))\n```\n\n::: {.cell-output-display}\n![](Linear_regression_files/figure-pdf/unnamed-chunk-12-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nplot(Effect(\"Length\", mod = rt.lm2))\n```\n\n::: {.cell-output-display}\n![](Linear_regression_files/figure-pdf/unnamed-chunk-12-3.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n## Model assumptions and diagnostics\n\nAs a parametric method, linear regression makes numerous assumptions\nabout the training data. It is, therefore, essential to run further\ntests to rule out possible violations. Among other things, the model\nassumptions include:\n\n-   A **linear relationship** between the response and the quantitative\n    predictors: The residuals should not display a clear pattern. For\n    this reason, it is recommended to use component residual plots\n    (e.g., `crPlot()` from the `car` library) for the visual\n    identification of potentially non-linear trends.\n\n-   **No heteroscedasticity** (i.e, non-constant variance of error\n    terms): Visually, a violation of this assumption becomes apparent if\n    the residuals form a funnel-like shape. It is also possible to\n    conduct a non-constant variance test `ncvTest()`: If it returns\n    $p$-values \\< 0.05, this suggests non-constant variance.\n\n-   **No multicollinearity**: Predictors should not be correlated with\n    each other. In the model data, correlated variables have unusually\n    high standard errors, thereby decreasing the explanatory power of\n    both the coefficients and the model as a whole. Another diagnostic\n    measure are variance inflation factors (VIF-scores); predictors with\n    VIF scores \\> 5 are potentially collinear. They can be computed\n    using the `vif()` function.\n\n-   **Normally distributed residuals**: The residuals should follow the\n    normal distribution. Usually, a visual inspection using `qqnorm()`\n    is sufficient, but the Shapiro-Wilke test `shapiro.test()` can also\n    be run on the model residuals. Note that a $p$-value below 0.05\n    provides evidence for non-normality.\n\n::: callout-important\nBeside the points mentioned above, it is always recommend to examine the\nmodel with regard to\n\n-   **outliers** that might skew the regression estimates,\n\n-   **interactions**, i.e., combined effects of predictors, and\n\n-   **overfitting**, which results in poor model performance outside the\n    training data.\n:::\n",
    "supporting": [
      "Linear_regression_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}