{
  "hash": "b47f98df701d407d13579756650325e0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Decision trees and random forests\"\nauthor: Vladimir Buskin\nformat:\n  html:\n    self-contained: false\n    logo: logo.png\n    footer: \"Regression\"\n    theme: default\n    toc: true\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 4\n    scrollable: true\neditor: visual\nbibliography: R.bib\n---\n\n\n## Introduction {.smaller}\n\nDecision trees and random forests are very popular **non-parametric**\nmethods. As such, \"they do not make explicit assumptions about the\nfunctional form of $f$'' [@james_introduction_2021: 23].\n\nIn this unit, we will cover the conceptual basics of these methods as\nwell as their implementation in R using the `tv` data from\n@paquot_conditional_2020 in addition to the `ELP` data from the unit on\nLinear Regression. The libraries we will need are listed below:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(\"tidyverse\")\nlibrary(\"readxl\")\nlibrary(\"tree\") # for CART\nlibrary(\"randomForest\") # for traditional random forests\nlibrary(\"party\") # for Conditional Inference Trees\nlibrary(\"pdp\") # for partial dependence plots\n\n## Reaction time data\nELP <- read_xlsx(\"data/ELP.xlsx\")\n\nELP$POS <- as.factor(ELP$POS)\n\n## Levshina's (2020) data set on T/V forms in Russian\ntv <- read.csv(\"data/Levshina_2020_tv_data.csv\", sep = \",\", header = TRUE, stringsAsFactors = TRUE)\n```\n:::\n\n\nIn the `tv` data frame, our target variable will be T/V `Form` with the\ntwo outcomes *ty* (Russian 2.p.sg., informal) and *vy* (Russian 2.p.pl.,\npolite).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(tv)\n\nhead(tv)\n```\n:::\n\n\n## Decision trees\n\nCore concepts:\n\n-   **Segmenting the feature space**: \"\\[T\\]he feature space (i.e., the\n    space spanned by all predictor variables) is recursively partitioned\n    into a set of rectangular areas\" [@strobl_introduction_2009: 325].\n\n-   **Impurity reduction**: These simplified prediction areas should\n    consist of mostly homogeneous (i.e., 'pure' rather than 'mixed')\n    observations.\n\n-   **Tree construction**: The 'decisions' made when partitioning the\n    training data can be visualised using tree structures. The nodes of\n    a tree represent variables, the branches represent decision rules,\n    and leaf nodes indicate the final outcome (e.g., a prediction).\n\n-   **CART**: The original computational implementation of decision\n    trees is known as the CART (Classification and Regression Trees)\n    algorithm developed by @breiman1984.\n\n### Classification trees\n\nIf we are dealing with a **categorical response variable**, the `tree()`\nfunction can be used to fit a classification tree in accordance with\nBreiman's CART algorithm. For illustration, consider the `tv` data\nframe. We will model the choice of the pronoun `Form` based on the\nspeaker's and hearer's social circle (`Rel_Circle`) and their difference\nin social class (`Rel_Class`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set random number generator for reproducibility\nset.seed(123)\n\n# Supply model formula\ntree.tv <- tree(Form ~ Rel_Circle + Rel_Class, data = tv)\n\n# View tree statistics\nsummary(tree.tv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nClassification tree:\ntree(formula = Form ~ Rel_Circle + Rel_Class, data = tv)\nNumber of terminal nodes:  9 \nResidual mean deviance:  0.974 = 213.3 / 219 \nMisclassification error rate: 0.2412 = 55 / 228 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualisation\nplot(tree.tv)\n\ntext(tree.tv, pretty = 3)\n```\n\n::: {.cell-output-display}\n![](Decision_trees_and_random_forests_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nAn important problem that arises during tree construction is that of\n**split selection**. When should the tree split a node into two further\nnodes and when not? Furthermore, when should the tree **stop** the\nsplitting process entirely? In this respect, CART relies on the\nprinciple of **impurity reduction**: \"The fundamental idea is to select\neach split of a subset so that the data in each of the descendent\nsubsets are 'purer' than the data in the parent subset\" [@breiman1984:\n23]. A measure for node purity is the **Gini index** $G$, which is\ndefined as\n\n$$\nG = \\sum_{k=1}^{K}{\\hat{p}_{mk}(1-\\hat{p}_{mk}),}\n$$\n\nwhere $\\hat{p}_{mk}$ measures the proportion of observations of a\nresponse level $k$ in the $m$th prediction area of the training data\nset. Values close to 0 are indicative of high node purity, meaning that\nmost observations belong to the same class (e.g., `Form = ty`). If\nsplitting a node no longer leads to a substantial increase in purity, it\nbecomes the **terminal node**, i.e., it is not split further. This\nterminal node returns the tree's class prediction.\n\nIt is worth noting that modern CART implementations rely on different\nsplitting criteria. For instance, **Conditional Inference Trees** use\nthe $p$ values of internal association tests to identify which variables\nwarrant further subdivision of the training data. The presence or\nabsence of correlation thus also determines whether or not a given node\nwill be terminal [for more details, see @greenwell2022: 122].\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fitting a conditional inference tree\nctree.tv <- ctree(Form ~ ., data = tv) # dot . means 'include all predictors'\n\nplot(ctree.tv)\n```\n\n::: {.cell-output-display}\n![](Decision_trees_and_random_forests_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n### Regression trees\n\nRegression trees are used for **continuous response variables**. Instead\nof providing class predictions, they return the mean value of\nobservations in a given prediction area. The algorithm now strives to\n**minimize the residual sum of squares** ($RSS$; cf. footnote 1 in 8.\nLinear Regression). Consider the regression tree for reaction times\ndepending on word length, frequency and part of speech:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# CART tree\ntree.rt <- tree(RT ~ Length + Freq + POS, data = ELP)\n\nsummary(tree.rt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression tree:\ntree(formula = RT ~ Length + Freq + POS, data = ELP)\nVariables actually used in tree construction:\n[1] \"Freq\"   \"Length\"\nNumber of terminal nodes:  10 \nResidual mean deviance:  8629 = 7507000 / 870 \nDistribution of residuals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-220.10  -59.09  -11.17    0.00   49.66  397.40 \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(tree.rt)\n\ntext(tree.rt, pretty = 0)\n```\n\n::: {.cell-output-display}\n![](Decision_trees_and_random_forests_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Conditional inference tree\nctree.rt <- ctree(RT ~ Length + Freq + POS, data = ELP) \n\nplot(ctree.rt)\n```\n\n::: {.cell-output-display}\n![](Decision_trees_and_random_forests_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n\n## Random forests\n\nRandom forests [@breiman_random_2001] belong to the class of **ensemble\nmethods** because they combine simpler models (e.g., individual decision\ntrees) into a more complex and possibly more accurate model. As part of\nthe RF algorithm, a great number of decision trees is trained on\nbootstrapped samples of the training data.\n\nSo far, random forests are essentially identical with **Bagging** (=\nbootstrap aggregation); however, an important additional characteristic\nof the RF algorithm is that only **a random subset of the predictors**\nis taken into consideration at each split. According to\n[@strobl_introduction_2009: 332], the resulting variability in tree\nstructure is advantageous: \"By combining the prediction of such a\ndiverse set of trees, ensemble methods utilize the fact that\nclassification trees are unstable, but, on average, produce the right\nprediction\".\n\n### Regression forest\n\nFor **regression** tasks, random forests return the average prediction\nof all trees in the ensemble.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For regression\nrt.rf.reg <- randomForest(RT ~ Length + Freq + POS, data = ELP,\n                                mtry = 1, # = sqrt(number of variables)\n                                ntree = 500) # number of trees\n\nrt.rf.reg\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = RT ~ Length + Freq + POS, data = ELP,      mtry = 1, ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 8972.927\n                    % Var explained: 43.64\n```\n\n\n:::\n\n```{.r .cell-code}\n# Conditional random forest\nrt.crf.reg <- cforest(RT ~ Length + Freq + POS, data = ELP, \n                    controls = cforest_unbiased(ntree = 500, mtry = 1))\n```\n:::\n\n\n### Classification forest\n\nFor **classification**, all trees cast a vote for one of the response\nclasses. The OOB error estimate refers to the accuracy of **out-of-bag\n(OOB) predictions**. After the initial bootstrapping procedure, roughly\na third of the training data remains unused. These observations, which\nwere not used for fitting trees, can be used as a test data set.\nPredictions based on this internal test data set are called OOB\npredictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For classification\ntv.rf.class <- randomForest(Form ~ ., data = tv,\n                            mtry = 4,\n                            ntree = 500)\n\ntv.rf.class\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = Form ~ ., data = tv, mtry = 4, ntree = 500) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n        OOB estimate of  error rate: 18.86%\nConfusion matrix:\n   ty vy class.error\nty 86 22   0.2037037\nvy 21 99   0.1750000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Conditional random forest\ntv.crf.class <- cforest(Form ~ ., data = tv,\n                    controls = cforest_unbiased(ntree = 500, mtry = 4))\n\ntv.crf.class\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t Random Forest using Conditional Inference Trees\n\nNumber of trees:  500 \n\nResponse:  Form \nInputs:  Film, Rel_Age, Rel_Sex, Rel_Power, Rel_Circle, S_Class, H_Class, S_Age, H_Age, Rel_Class, Before68, Others, Office, S_Sex, H_Sex, Place \nNumber of observations:  228 \n```\n\n\n:::\n:::\n\n\n### Variable importance\n\nRandom forests allow users to assess whether or not certain predictors\nare useful for the model. The **Gini** index can be re-used to identify\nthose variables that have led to the greatest reduction in impurity.\nHowever, this measure is **biased** towards predictors with many values\n[cf. @strobl2007].\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gini importance (Reaction times)\nvarImpPlot(rt.rf.reg)\n```\n\n::: {.cell-output-display}\n![](Decision_trees_and_random_forests_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Gini importance (Form of 2.p.)\nvarImpPlot(tv.rf.class)\n```\n\n::: {.cell-output-display}\n![](Decision_trees_and_random_forests_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n:::\n\n\nA more robust measures is (**Conditional) Permutation Accuracy\nImportance** which compares the predictive accuracy of the random forest\nmodel before and after randomly permuting the values of the predictors\n[cf. @strobl2008; @debeer2020].\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Conditional permutation accuracy importance\nlibrary(\"permimp\")\n\n# Refit RF model with additional parameters\ntv.rf.class <- randomForest(Form ~ .,\n                            data = tv,\n                            mtry = 4,\n                            ntree = 500,\n                            keep.inbag = TRUE,\n                            keep.forest = TRUE)\n\n# Compute CPI scores\ntv.rf.permimp <- permimp(tv.rf.class, conditional = TRUE, progressBar = FALSE, threshold = .95) # Choose \"Yes\" in the console\n\n# Plot CPI scores\nplot(tv.rf.permimp, horizontal = TRUE, type = \"dot\", sort = TRUE)\n```\n:::\n\n\n### Visualising random forest models\n\n-   Partial dependence plots (`pdp` package, cf. also Hastie et al.\n    [-@hastie2017]) provide averaged predictions ($\\hat{y}$) for a given\n    constellation of predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Form ~ Rel_Circle\nRel_Circle.partial <- pdp::partial(tv.rf.class, pred.var = \"Rel_Circle\", which.class = \"ty\")\n\nRel_Circle.partial %>% \n  ggplot(aes(x = Rel_Circle, y = yhat)) +\n  geom_point(col = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    title = \"Probability of 'ty' (2.p.sg.) depending on social circle\",\n    y = \"Log odds of 'ty'\"\n  )\n```\n\n::: {.cell-output-display}\n![](Decision_trees_and_random_forests_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# RT ~ POS\npos.partial <- pdp::partial(rt.rf.reg, pred.var = \"POS\")\n\npos.partial %>% \n  ggplot(aes(x = POS, y = yhat)) +\n  geom_point(col = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    title = \"Predicted reaction times by POS\",\n    y = \"Predicted reaction time\"\n  )\n```\n\n::: {.cell-output-display}\n![](Decision_trees_and_random_forests_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# RT ~ Length\nlength.partial <- pdp::partial(rt.rf.reg, pred.var = \"Length\")\n\nlength.partial %>% \n  ggplot(aes(x = Length, y = yhat)) +\n  geom_line(col = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    title = \"Predicted reaction times by word length\",\n    y = \"Predicted reaction time\"\n  )\n```\n\n::: {.cell-output-display}\n![](Decision_trees_and_random_forests_files/figure-html/unnamed-chunk-11-3.png){width=672}\n:::\n:::\n",
    "supporting": [
      "Decision_trees_and_random_forests_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}