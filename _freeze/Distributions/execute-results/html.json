{
  "hash": "88cf0539527b1faa9c18cb2fb590b093",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Probability distributions\"\nauthor: Vladimir Buskin\nformat:\n  html:\n    self-contained: true\n    theme: default\n    toc: true\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 3\n    scrollable: true\neditor: visual\nbibliography: R.bib\n---\n\n::: {.cell}\n\n:::\n\n\n## Suggested reading\n\n> @baguleySeriousStatsGuide2012: Chapter 2\n>\n> @agrestiFoundationsStatisticsData2022: Chapter 2\n>\n> @heumann_introduction_2022: Chapter 8\n\n## Continuous distributions\n\n### The normal distribution\n\nA great number of numerical variables in the world follow the well-known\n**normal** (or Gaussian) **distribution**, which includes test scores,\nweight and height, among many others. The plot below illustrates its\ncharacteristic bell-shape: Most observations are in the middle, with\nconsiderably fewer near the fringes. For example, most people are rather\n\"average\" in height; there are only few people that are extremely short\nor extremely tall.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Distributions_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe normal distribution is typically described in terms of two\nparameters: The population mean $\\mu$ and the population standard\ndeviation $\\sigma$. If a random variable $X$ is normally distributed, we\ntypically use the notation in @eq-normdistrib.\n\n$$ X \\sim N(\\mu, \\sigma^2).\n$$ {#eq-normdistrib}\n\nThese two parameters affect the shape of the **probability density\nfunction (PDF)** $f(x)$, which is formally defined as\n\n$$\nf(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}.\n$$ {#eq-norm-pdf}\n\nIn practice, this function returns a bell curve:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Distributions_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n::: callout-note\n## Quick facts about the Gaussian bell curve\n\nQuite interestingly,\n\n-   68% all values fall within one standard deviation of the mean,\n\n-   95% within two, and\n\n-   99.7% within three.\n:::\n\nIn the plot, the $y$-axis indicates the density of population values;\nnote that since the Gaussian distribution is a continuous distribution\nwith technically infinite $x$-values, the probability of any given value\nmust be 0. We can only obtain probabilities for **intervals** of values,\nwhich are given by\n\n$$\nP(a \\leq X \\leq b) = \\int_a^b f(x)dx.\n$$ {#eq-norm-prob}\n\nWe can find the population mean of $X$ with a PDF $f(x)$ via\n\n$$\nE(X) = \\mu = \\int_x xf(x)dx,\n$$ {#eq-norm-mean}\n\nwhere $E(X)$ denotes the expected value of $X$, i.e., the mean.\nEssentially, multiplying every value $x$ by its respective probability\ndensity $f(x)$ and integrating over all possible values of $x$ will\nreturn $E(X) = \\mu$.\n\n## Discrete distributions\n\n### Bernoulli distribution\n\nThe **Bernoulli distribution** is a discrete probability distribution\nfor random variables which have only two possible outcomes: \"positive\"\n(often coded as 1) and \"negative\" (often coded as 0). Examples of such\nvariables include coin tosses (heads/tails), binary response questions\n(yes/no), and defect status (defective/non-defective).\n\nIf a random variable $X$ follows a Bernoulli distribution, it is\ndetermined by the parameter $p$, which is the probability of the\npositive case:\n\n$$ X \\sim Bernoulli(p).$$ The **probability mass function (PMF)** of the\nBernoulli distribution is given by: $$\nP(X = x) = \n\\begin{cases} \np & \\text{if } x = 1 \\\\\n1 - p & \\text{if } x = 0 \n\\end{cases}\n$$\n\nwhere $0 \\leq p \\leq 1$. This function shows the probability of $X$\ntaking on the value of 1 or 0 [cf. @heumann_introduction_2022: 162-163].\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Distributions_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n### Binomial Distribution\n\n\nThe **binomial distribution** is a fairly straightforward extension of the Bernoulli distribution in that it models the number of successes in $n$\nindependent Bernoulli trials, each with probability $p$ of success. If a\nrandom variable $X$ follows a binomial distribution with parameters $n$\nand $p$, we write:\n\n$$ X \\sim Binomial(n,p) \n$$ {#eq-binom}\n\nThe **probability mass function (PMF)** for the binomial distribution\nis:\n\n$$ P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k} \n$$ {#eq-binom-pmf}\n\nwhere:\n\n-   $n$ is the number of trials\n-   $k$ is the number of successes $(0 \\leq k \\leq n)$\n-   $p$ is the probability of success on each trial $(0 \\leq p \\leq 1)$\n-   $\\binom{n}{k}$ is the binomial coefficient (\"n choose k\")\n\n",
    "supporting": [
      "Distributions_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}