{
  "hash": "3b56f2f97b070473462d2b019d67b9cb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Probability distributions\"\nauthor: Vladimir Buskin\nformat:\n  html:\n    self-contained: true\n    theme: default\n    toc: true\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 3\n    scrollable: true\neditor: visual\nbibliography: R.bib\n---\n\n::: {.cell}\n\n:::\n\n\n## Suggested reading\n\n> @baguleySeriousStatsGuide2012: Chapter 2\n>\n> @agrestiFoundationsStatisticsData2022: Chapter 2\n>\n> @heumann_introduction_2022: Chapter 8\n\n## Continuous distributions\n\n### The normal distribution\n\nA great number of numerical variables in the world follow the well-known\n**normal** (or Gaussian) **distribution**, which includes test scores,\nweight and height, among many others. The plot below illustrates its\ncharacteristic bell-shape: Most observations are in the middle, with\nconsiderably fewer near the fringes. For example, most people are rather\n\"average\" in height; there are only few people that are extremely short\nor extremely tall.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Distributions_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe normal distribution is typically described in terms of two\nparameters: The population mean $\\mu$ and the population standard\ndeviation $\\sigma$. If a random variable $X$ is normally distributed, we\ntypically use the notation in @eq-normdistrib.\n\n$$ X \\sim N(\\mu, \\sigma^2).\n$$ {#eq-normdistrib}\n\nThe Gaussian bell curve has several interesting properties:\n\n-   68% all values fall within one standard deviation of the mean,\n\n-   95% within two, and\n\n-   99.7% within three.\n\nBelow, you can see the annotated **probability density function (PDF)**\nof the normal distribution. The $y$-axis indicates the density of\npopulation values; note that since the Gaussian distribution is a\ncontinuous distribution, the probability of any given value is 0. We can\nonly obtain probabilities for **intervals** of values, which are given\nby\n\n$$\nP(a \\leq X \\leq b) = \\int_a^b f(x)dx.\n$$ {#eq-norm-prob}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Distributions_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWe can find the population mean of $X$ with a PDF $f(x)$ via\n\n$$\nE(X) = \\mu = \\int_x xf(x)dx,\n$$ {#eq-norm-mean}\n\nwhere $E(X)$ denotes the expected value of $X$, i.e., the mean.\nEssentially, multiplying every value $x$ by its respective probability\ndensity $f(x)$ and integrating over all possible values of $x$ will\nreturn $E(X) = \\mu$.\n\n## Discrete distributions\n\n### Bernoulli distribution\n\nThe **Bernoulli distribution** is a discrete probability distribution\nfor random variables which have only two possible outcomes: \"positive\"\n(often coded as 1) and \"negative\" (often coded as 0). Examples of such\nvariables include coin tosses (heads/tails), binary response questions\n(yes/no), and defect status (defective/non-defective).\n\nIf a random variable $X$ follows a Bernoulli distribution, it is\ndetermined by the parameter $p$, which is the probability of the\npositive case:\n\n$$ X \\sim Bernoulli(p).$$ The **probability mass function (PMF)** of the\nBernoulli distribution is given by: $$\nP(X = x) = \n\\begin{cases} \np & \\text{if } x = 1 \\\\\n1 - p & \\text{if } x = 0 \n\\end{cases}\n$$\n\nwhere $0 \\leq p \\leq 1$. This function shows the probability of $X$\ntaking on the value of 1 or 0 [cf. @heumann_introduction_2022: 162-163].\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Distributions_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n### Binomial Distribution\n\nThe **binomial distribution** models the number of successes in $n$\nindependent Bernoulli trials, each with probability $p$ of success. If a\nrandom variable $X$ follows a binomial distribution with parameters $n$\nand $p$, we write:\n\n$$ X \\sim Binomial(n,p) $$\n\nThe **probability mass function (PMF)** for the binomial distribution\nis:\n\n$$ P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k} $$\n\nwhere:\n\n-   $n$ is the number of trials\n-   $k$ is the number of successes $(0 \\leq k \\leq n)$\n-   $p$ is the probability of success on each trial $(0 \\leq p \\leq 1)$\n-   $\\binom{n}{k}$ is the binomial coefficient (\"n choose k\")\n\n### Multinomial Distribution\n\nThe **multinomial distribution** is a generalisation of the binomial\ndistribution to the case where each trial has $k$ possible outcomes\n(rather than just two). If a random variable\n$\\mathbf{X} = (X_1,...,X_k)$ follows a multinomial distribution, we\nwrite:\n\n$$ \\mathbf{X} \\sim Multinomial(n,\\mathbf{p}) $$\n\nThe **probability mass function (PMF)** for the multinomial distribution\nis:\n\n$$ P(\\mathbf{X} = \\mathbf{x}) = \\frac{n!}{x_1!x_2!...x_k!} p_1^{x_1}p_2^{x_2}...p_k^{x_k} $$\n\nwhere:\n\n-   $n$ is the number of trials\n\n-   $\\mathbf{x} = (x_1,...,x_k)$ represents the number of occurrences of\n    each outcome\n\n-   $\\mathbf{p} = (p_1,...,p_k)$ represents the probabilities of each\n    outcome\n\n-   $\\sum_{i=1}^k x_i = n$ and $\\sum_{i=1}^k p_i = 1$\n\n## Sampling Distributions\n\nSampling distributions describe the distribution of statistics\ncalculated from repeated random samples. Here are the most common ones:\n\n### Chi-squared Distribution\n\nThe **chi-squared distribution** with $k$ degrees of freedom, denoted\n$\\chi^2(k)$, is the distribution of the sum of squares of $k$\nindependent standard normal random variables. Its **probability density\nfunction (PDF)** is:\n\n$$ f(x) = \\frac{x^{k/2-1}e^{-x/2}}{2^{k/2}\\Gamma(k/2)} $$\n\nwhere:\n\n-   $x > 0$\n\n-   $k$ is the degrees of freedom\n\n-   $\\Gamma$ is the gamma function\n\n### Student's t-Distribution\n\nThe **Student's t-distribution** with $\\nu$ degrees of freedom has the\nfollowing **PDF**:\n\n$$ f(x) = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\sqrt{\\nu\\pi}\\Gamma(\\frac{\\nu}{2})}\\left(1+\\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}} $$\n\nwhere:\n\n-   $-\\infty < x < \\infty$\n\n-   $\\nu > 0$ is the degrees of freedom\n\n-   $\\Gamma$ is the gamma function\n\nKey properties:\n\n-   As $\\nu \\to \\infty$, the t-distribution approaches the standard\n    normal distribution\n\n-   The distribution is symmetric about 0\n\n-   The variance is $\\frac{\\nu}{\\nu-2}$ for $\\nu > 2$\n\n### F-Distribution\n\nThe **F-distribution** with parameters $d_1$ and $d_2$ (degrees of\nfreedom) has the following **PDF**:\n\n$$ f(x) = \\frac{\\sqrt{\\frac{(d_1x)^{d_1}d_2^{d_2}}{(d_1x+d_2)^{d_1+d_2}}}}{x\\beta(\\frac{d_1}{2},\\frac{d_2}{2})} $$\n\nwhere:\n\n-   $x > 0$\n\n-   $d_1, d_2 > 0$ are the degrees of freedom\n\n-   $\\beta$ is the beta function\n\nThe F-distribution is commonly used in:\n\n-   Analysis of Variance (ANOVA)\n\n-   Testing equality of variances\n\n-   Model comparison in regression analysis\n\nEach of these sampling distributions plays a crucial role in statistical\ninference:\n\n-   Chi-squared: Used for goodness-of-fit tests and tests of\n    independence\n\n-   t-distribution: Used for inference about means when population\n    variance is unknown\n\n-   F-distribution: Used for comparing variances and in ANOVA procedures\n",
    "supporting": [
      "Distributions_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}