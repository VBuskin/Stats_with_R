{
  "hash": "54ea886c9982ecc8c98e5d387f2c93f0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hypothesis testing\"\nauthor: Vladimir Buskin\nformat:\n  html:\n    self-contained: true\n    code-fold: false\n    theme: default\n    toc: true\n    toc-depth: 4\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 3\n    scrollable: true\neditor: visual\nbibliography: R.bib\n---\n\n\n\n## Preparation\n\n\n\n\n\n\n\n-   Load packages:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\nlibrary(tidyverse)\n```\n:::\n\n\n\n-   Load the data sets:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Clause order data\ndata <- read_xlsx(\"Paquot_Larsson_2020_data.xlsx\")\n\n# Data on vowel frequencies\ndata_vowels <- read.csv(\"Vowels_Apache.csv\", sep = \"\\t\")\n```\n:::\n\n\n\n## Hypothesis testing {#sec-hyp}\n\nThe first step is to define the **null hypothesis** $H_0$ and the\n**alternative hypothesis** $H_1$ (or $H_a$).\n\nGiven two categorical variables $X$ and $Y$, we assume under $H_0$ that\nboth variables are independent from each other. This hypothesis\ndescribes the \"default state of the world\" [@james_introduction_2021:\n555], i.e., what we would usually expect to see. By contrast, the\nalternative hypothesis $H_1$ states that $X$ and $Y$ are **not**\nindependent, i.e., that $H_0$ does not hold.\n\nIn the subsequent sections, we will consider two scenarios:\n\n1.  We are interested in finding out whether English clause `ORDER`\n    ('sc-mc' or 'mc-sc') depends on the type of the subordinate clause\n    (`SUBORDTYPE`), which can be either temporal ('temp') or causal\n    ('caus').\n\nOur hypotheses are:\n\n-   $H_0:$ The variables `ORDER` and `SUBORDTYPE` are independent.\n\n-   $H_1:$ The variables `ORDER` and `SUBORDTYPE` are **not**\n    independent.\n\n2.  As part of a phonetic study, we compare the base frequencies of the\n    F1 formants for male and female speakers of Apache. We forward the\n    following hypotheses:\n\n-   $H_0:$ mean `F1 frequency` of men $=$ mean `F1 frequency` of women.\n\n-   $H_1:$ mean `F1 frequency` of men $\\ne$ mean `F1 frequency` of\n    women.\n\nBased on our data, we can decide to either **accept** or **reject**\n$H_0$. Rejecting $H_0$ can be viewed as evidence in favour of $H_1$ and\nthus marks a potential 'discovery' in the data. However, there is always\na chance that we accept or reject the wrong hypothesis; the four\npossible constellations are summarised in the table below [cf.\n@heumann_introduction_2022: 223]:\n\n|                           | $H_0$ is true                                       | $H_0$ is not true                                   |\n|------------------|---------------------------|---------------------------|\n| $H_0$ **is not rejected** | $\\color{green}{\\text{Correct decision}}$            | $\\color{red}{\\text{Type II } (\\beta)\\text{-error}}$ |\n| $H_0$ **is rejected**     | $\\color{red}{\\text{Type I } (\\alpha)\\text{-error}}$ | $\\color{green}{\\text{Correct decision}}$            |\n\nThe probability of a Type I error, which refers to the rejection of\n$H_0$ although it is true, is called the **significance level**\n$\\alpha$, which has a conventional value of $0.05$ (i.e., a 5% chance of\ncommitting a Type I error).\n\n## Constructing the critical region\n\nAn important question remains: How great should the difference be for us\nto reject $H_0$? The $p$-value measures **the probability of\nencountering a specific value of a test statistic** under the assumption\nthat $H_0$ holds. For example, a $p$-value of $0.02$ means that we would\nsee a particular $\\chi^2$-score (or $T$, $F$ etc.) only 2% of the time\nif $X$ and $Y$ were unrelated (or if there was no difference between\n$\\bar{x}$ and $\\bar{y}$, respectively). Since our significance level\n$\\alpha$ is set to $0.05$, we only reject the null hypothesis if this\nprobability is lower than 5%.\n\nWe obtain $p$-values by consulting the probability density functions of\nthe underlying distributions:\n\n-   Probability density function for the $\\chi^2$-distribution with\n    $df = 1$\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Generate random samples from a chi-squared distribution with 1 degree of freedom\nx <- rchisq(100000, df = 1)\n\n# Create histogram\nhist(x,\n     breaks = \"Scott\",\n     freq = FALSE,\n     xlim = c(0, 20),\n     ylim = c(0, 0.2),\n     ylab = \"Probability density of observing a specific score\",\n     xlab = \"Chi-squared score\",\n     main = \"Histogram for a chi-squared distribution with 1 degree of freedom (df)\",\n     cex.main = 0.9)\n\n# Overlay PDF\ncurve(dchisq(x, df = 1), from = 0, to = 150, n = 5000, col = \"orange\", lwd = 2, add = TRUE)\n```\n\n::: {.cell-output-display}\n![](Hypothesis_testing_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n-   Probability density function for the $t$-distribution with\n    $df = 112.19$\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Given t-statistic and degrees of freedom\nt_statistic <- 2.4416\ndf <- 112.19\n\n# Generate random samples from a t-distribution with the given degrees of freedom\nx <- rt(100000, df = df)\n\n# Create histogram\nhist(x,\n     breaks = \"Scott\",\n     freq = FALSE,\n     xlim = c(-5, 5),\n     ylim = c(0, 0.4),\n     ylab = \"Probability density of observing a specific score\",\n     xlab = \"t-score\",\n     main = \"Histogram for a t-distribution with 112.19 degrees of freedom\",\n     cex.main = 0.9)\n\n# Overlay PDF\ncurve(dt(x, df = df), from = -5, to = 5, n = 5000, col = \"orange\", lwd = 2, add = TRUE)\n```\n\n::: {.cell-output-display}\n![](Hypothesis_testing_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n",
    "supporting": [
      "Hypothesis_testing_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}