{
  "hash": "90d3f6768834629faa88ceb39e62e360",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hypothesis testing\"\nauthor: Vladimir Buskin\nformat:\n  html:\n    self-contained: true\n    code-fold: false\n    theme: default\n    toc: true\n    toc-depth: 4\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 3\n    scrollable: true\neditor: visual\nbibliography: R.bib\n---\n\n\n## Preparation\n\n\n\n\n\n-   Load packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\n```\n:::\n\n\n-   Load the data sets:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read_xlsx(\"Paquot_Larsson_2020_data.xlsx\")\n\ndata_vowels <- read.csv(\"Vowels_Apache.csv\", sep = \"\\t\")\n```\n:::\n\n\n## Hypothesis testing\n\nThe first step is to define the **null hypothesis** $H_0$ and the\n**alternative hypothesis** $H_1$ (or $H_a$).\n\nGiven two categorical variables $X$ and $Y$, we assume under $H_0$ that\nboth variables are independent from each other. This hypothesis\ndescribes the \"default state of the world\" [@james_introduction_2021:\n555], i.e., what we would usually expect to see. By contrast, the\nalternative hypothesis $H_1$ states that $X$ and $Y$ are **not**\nindependent, i.e., that $H_0$ does not hold.\n\nIn this unit, we will consider two scenarios:\n\n1.  We are interested in finding out whether English clause `ORDER`\n    ('sc-mc' or 'mc-sc') depends on the type of the subordinate clause\n    (`SUBORDTYPE`), which can be either temporal ('temp') or causal\n    ('caus').\n\nOur hypotheses are:\n\n-   $H_0:$ The variables `ORDER` and `SUBORDTYPE` are independent.\n\n-   $H_1:$ The variables `ORDER` and `SUBORDTYPE` are **not**\n    independent.\n\n2.  As part of a phonetic study, we compare the base frequencies of the\n    F1 formants for male and female speakers of Apache. We forward the\n    following hypotheses:\n\n-   $H_0:$ mean `F1 frequency` of men $=$ mean `F1 frequency` of women.\n\n-   $H_1:$ mean `F1 frequency` of men $\\ne$ mean `F1 frequency` of\n    women.\n\nBased on our data, we can decide to either **accept** or **reject**\n$H_0$. Rejecting $H_0$ can be viewed as evidence in favour of $H_1$ and\nthus marks a potential 'discovery' in the data. However, there is always\na chance that we accept or reject the wrong hypothesis; the four\npossible constellations are summarised in the table below [cf.\n@heumann_introduction_2022: 223]:\n\n|                           | $H_0$ is true                                       | $H_0$ is not true                                   |\n|---------------------------|-----------------------------------------------------|-----------------------------------------------------|\n| $H_0$ **is not rejected** | $\\color{green}{\\text{Correct decision}}$            | $\\color{red}{\\text{Type II } (\\beta)\\text{-error}}$ |\n| $H_0$ **is rejected**     | $\\color{red}{\\text{Type I } (\\alpha)\\text{-error}}$ | $\\color{green}{\\text{Correct decision}}$            |\n\nThe probability of a Type I error, which refers to the rejection of\n$H_0$ although it is true, is called the **significance level**\n$\\alpha$, which has a conventional value of $0.05$ (i.e., a 5% chance of\ncommitting a Type I error).\n\n### The $\\chi^2$-test\n\nThe next step is to compute a test statistic that indicates how strongly\nour data conforms to $H_0$. For instance, the Pearson $\\chi^2$ statistic\nis commonly used for categorical variables. It requires two types of\nvalues: the **observed frequencies** $n_{ij}$ in our data set and the\n**expected frequencies** $m_{ij}$, which we would expect to see if $H_0$\nwas true.\n\nThis table represents a generic contingency table where $X$ and $Y$ are\ncategorical variables. Each $x_i$ represents a category of $X$ and each\n$y_j$ represents a category of $Y$. In the table, each cell indicates\nthe count of observation $n_{ij}$ corresponding to the $i$-th row and\n$j$-th column.\n\n|     |       |          | $Y$      |     |          |     |\n|-----|-------|----------|----------|-----|----------|-----|\n|     |       | $y_1$    | $y_2$    | ... | $y_J$    |     |\n|     | $x_1$ | $n_{11}$ | $n_{12}$ | ... | $n_{1J}$ |     |\n|     | $x_2$ | $n_{21}$ | $n_{22}$ | ... | $n_{2J}$ |     |\n| $X$ | ...   | ...      | ...      | ... | ...      |     |\n|     | $x_I$ | $n_{I1}$ | $n_{I2}$ | ... | $n_{3J}$ |     |\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cross-tabulate the frequencies for the variables of interest\n\nfreqs <- table(data$ORDER, data$SUBORDTYPE); freqs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n```\n\n\n:::\n:::\n\n\nWe calculate the expected frequencies by using the formula\n\n$$\nm_{ij} = \\frac{i\\textrm{th row sum} \\times j \\textrm{th column sum}}{\\textrm{number of observations}}.\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute expected frequencies\n\n## Calculate row totals\nrow_totals <- rowSums(freqs)\n\n## Calculate column totals\ncol_totals <- colSums(freqs)\n\n## Total number of observations\ntotal_obs <- sum(freqs)\n\n## Calculate expected frequencies\nexpected_table <- outer(row_totals, col_totals) / total_obs\n\nexpected_table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           caus      temp\nmc-sc 135.79404 139.20596\nsc-mc  63.20596  64.79404\n```\n\n\n:::\n:::\n\n\n::: {.callout-note collapse=\"true\" title=\"Definition of the $\\\\chi^2$-test\"}\nGiven a sample with $n$ observations and $k$ degrees of freedom\n($df$)[^1], the $\\chi^2$-statistic measures how much the observed\nfrequencies **deviate** from the expected frequencies **for each cell**\nin a contingency table [cf. @heumann_introduction_2022: 249-251]:\n\n$$\n\\chi^2 = \\sum_{i=1}^{I}\\sum_{i=j}^{J}{\\frac{(n_{ij} - m_{ij})^2}{m_{ij}}}.\n$$ The test stipulates that ...\n\n1.  all observations are independent of each other,\n2.  80% of the expected frequencies are $\\geq$ 5, and\n3.  all observed frequencies are $\\geq$ 1.\n:::\n\n[^1]: The degrees of freedom are calculated by\n    $(\\textrm{number of rows} -1) \\times (\\textrm{number of columns} - 1)$.\n\n::: {.callout-tip collapse=\"true\" title=\"Implementation in R: Manual vs. automatic\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute chi-squared scores for all cells\n## Create empty chi_squared_table for later storage\nchi_squared_table <- matrix(NA, nrow = 2, ncol = 2,\n                            dimnames = list(c(\"mc-sc\", \"sc-mc\"), c(\"caus\", \"temp\")))\n\n# Loop: Repeat the following commands ...\nfor (i in 1:2) { # for each of the 2 rows and\n  for (j in 1:2) { # for each of the 2 columns:\n    observed_freq <- freqs[i, j] # 1. Get the observed frequencies\n    expected_freq <- expected_table[i, j] # 2. Get the expected frequencies\n    chi_squared_score <- ((observed_freq - expected_freq)^2) / expected_freq # 3. Compute chi-squared scores\n    chi_squared_table[i, j] <- chi_squared_score # 4. Store output in the chi_squared_table\n  }\n}\n\nchi_squared_table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          caus     temp\nmc-sc 17.11278 16.69335\nsc-mc 36.76575 35.86463\n```\n\n\n:::\n:::\n\n\nOr, more elegantly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreqs_test <- chisq.test(freqs)\n\n# Expected frequencies\nfreqs_test$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n             caus      temp\n  mc-sc 135.79404 139.20596\n  sc-mc  63.20596  64.79404\n```\n\n\n:::\n\n```{.r .cell-code}\n# Chi-squared scores\n(freqs_test$residuals)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n            caus     temp\n  mc-sc 17.11278 16.69335\n  sc-mc 36.76575 35.86463\n```\n\n\n:::\n\n```{.r .cell-code}\n# Test statistics\nfreqs_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  freqs\nX-squared = 104.24, df = 1, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n:::\n\n::: callout-important\nIf the data does not meet the (expected) frequency requirements for the\n$\\chi^2$-test, the Fisher's Exact Test is a viable alternative (see\n`?fisher.test()` for details).\n:::\n\n### The $t$-test\n\nSince the $\\chi^2$ measure exclusively works with categorical variables,\na separate test statistic is required if one of them is a continuous\nvariable. The $t$ statistic is often used for research questions\ninvolving differences between sample means. The way $t$ is calculated\ndepends on the sources of $X$ and $Y$: Are they from the same sample or\nfrom two (in-)dependent ones?\n\nFirst, we consider two **independent samples** from a population:\n\n-   Sample $X$ with the observations $x_1, x_2, ..., {x_n}_1$, sample\n    size $n_1$, sample mean $\\bar{x}$ and sample variance $s^2_x$.\n\n-   Sample $Y$ with the observations $y_1, y_2, ..., {y_n}_2$, sample\n    size $n_2$, sample mean $\\bar{y}$ and sample variance $s^2_y$.\n\n::: {.callout-note collapse=\"true\" title=\"Definition of the $t$-test\"}\nThe $t$-statistic after Welch is given by:\n\n$$\nt(x, y) = \\frac{|\\bar{x} - \\bar{y}|}{\\sqrt{\\frac{s^2_x}{n_1} + \\frac{s^2_y}{n_2}}} \n$$\n\n-   If there is more than one observation for a given subject (e.g,\n    before and after an experiment), the samples are called\n    **dependent** or **paired**. The paired $t$-test assumes two\n    continuous variables $X$ and $Y$.\n\n-   In the paired test, the variable $d$ denotes the difference between\n    them, i.e., $x - y$. The corresponding test statistic is obtained\n    via\n\n$$\nt(x, y) = t(d) = \\frac{\\bar{d}}{s_d} \\sqrt{n}.\n$$\n\nNote the difference $\\bar{d} = \\frac{1}{n}\\sum_{i=1}^n{d_i}$ and the\nvariance\n\n$$\ns^2_d = \\frac{\\sum_{i=1}^n({d_i} - \\bar{d})^2}{n-1}.\n$$\n\nTraditionally, the $t$-test is based on the assumptions of ...\n\n1.  **Normality** and\n2.  **Variance homogeneity** (i.e., equal sample variances). Note that\n    this does not apply to the $t$-test after Welch, which can handle\n    unequal variances.\n:::\n\n::: {.callout-tip collapse=\"true\" title=\"Implementation in R: Manual vs. automatic\"}\nBy hand:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Subset the data by sex\ndata_m <- data_vowels[data_vowels$SEX == \"M\", ]\ndata_f <- data_vowels[data_vowels$SEX == \"F\", ]\n\n# Compute sample means\nmean_m <- mean(data_m$HZ_F1)\nmean_f <- mean(data_f$HZ_F1)\n\n# Compute sample variances\nvar_m <- var(data_m$HZ_F1)\nvar_f <- var(data_f$HZ_F1)\n\n# Determine sample sizes\nn_m <- length(data_m$HZ_F1)\nn_f <- length(data_f$HZ_F1)\n\n# Compute t-statistic\nt_statistic <- abs(mean_m - mean_f) / sqrt((var_m / n_m) + (var_f / n_f))\n\n# Compute degrees of freedom (simple version)\ndf <- n_m + n_f - 2\n\n# Find the p-value using the cumulative distribution function (CDF) of the t-distribution\np_value <- 2 * pt(-t_statistic, df)\n```\n:::\n\n\nOr, more concisely:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(data_vowels$HZ_F1 ~ data_vowels$SEX, paired = FALSE) # there is a significant difference!\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  data_vowels$HZ_F1 by data_vowels$SEX\nt = 2.4416, df = 112.19, p-value = 0.01619\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n  8.403651 80.758016\nsample estimates:\nmean in group F mean in group M \n       528.8548        484.2740 \n```\n\n\n:::\n:::\n\n:::\n\n::: callout-important\nIf at least one assumption of the $t$-test has been violated, it is\nimportant to use a non-parametric test such as the\n**Wilcoxon-Mann-Whitney (WMW) U-Test** instead. In essence, this test\ncompares the probabilities of encountering a value $x$ from sample $X$\nthat is greater than a value $y$ from sample $Y$. For details, see\n`?wilcox.test()`.\n:::\n\n### Constructing the critical region\n\nAn important question remains: How great should the difference be for us\nto reject $H_0$? The $p$-value measures **the probability of\nencountering a specific value of a test statistic** under the assumption\nthat $H_0$ holds. For example, a $p$-value of $0.02$ means that we would\nsee a particular $\\chi^2$-score (or $T$, $F$ etc.) only 2% of the time\nif $X$ and $Y$ were unrelated (or if there was no difference between\n$\\bar{x}$ and $\\bar{y}$, respectively). Since our significance level\n$\\alpha$ is set to $0.05$, we only reject the null hypothesis if this\nprobability is lower than 5%.\n\nWe obtain $p$-values by consulting the probability density functions of\nthe underlying distributions:\n\n-   Probability density function for the $\\chi^2$-distribution with\n    $df = 1$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Generate random samples from a chi-squared distribution with 1 degree of freedom\nx <- rchisq(100000, df = 1)\n\n# Create histogram\nhist(x,\n     breaks = \"Scott\",\n     freq = FALSE,\n     xlim = c(0, 20),\n     ylim = c(0, 0.2),\n     ylab = \"Probability density of observing a specific score\",\n     xlab = \"Chi-squared score\",\n     main = \"Histogram for a chi-squared distribution with 1 degree of freedom (df)\",\n     cex.main = 0.9)\n\n# Overlay PDF\ncurve(dchisq(x, df = 1), from = 0, to = 150, n = 5000, col = \"orange\", lwd = 2, add = TRUE)\n```\n\n::: {.cell-output-display}\n![](Hypothesis_testing_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n-   Probability density function for the $t$-distribution with\n    $df = 112.19$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Given t-statistic and degrees of freedom\nt_statistic <- 2.4416\ndf <- 112.19\n\n# Generate random samples from a t-distribution with the given degrees of freedom\nx <- rt(100000, df = df)\n\n# Create histogram\nhist(x,\n     breaks = \"Scott\",\n     freq = FALSE,\n     xlim = c(-5, 5),\n     ylim = c(0, 0.4),\n     ylab = \"Probability density of observing a specific score\",\n     xlab = \"t-score\",\n     main = \"Histogram for a t-distribution with 112.19 degrees of freedom\",\n     cex.main = 0.9)\n\n# Overlay PDF\ncurve(dt(x, df = df), from = -5, to = 5, n = 5000, col = \"orange\", lwd = 2, add = TRUE)\n```\n\n::: {.cell-output-display}\n![](Hypothesis_testing_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n## Workflow in R\n\n### $\\chi^2$-test\n\n#### Define hypotheses\n\n-   $H_0:$ The variables `ORDER` and `SUBORDTYPE` are independent.\n\n-   $H_1:$ The variables `ORDER` and `SUBORDTYPE` are **not**\n    independent.\n\n#### Descriptive overview\n\nWe plot the distribution of clause `ORDER` depending on `SUBORDTYPE`.\nThis requires (a) selecting the desired variables, (b) computing the\ntoken frequencies and (c) computing the percentages.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Filter data so as to show only those observations that are relevant\n\ndata %>% \n  # Filter columns\n  select(ORDER, SUBORDTYPE) %>%\n    # Count observations \n    count(ORDER, SUBORDTYPE) %>%  \n    # Compute percentages\n    mutate(pct = n/sum(n) * 100) -> data_order_subord\n\nknitr::kable(data_order_subord)\n```\n\n::: {.cell-output-display}\n\n\n|ORDER |SUBORDTYPE |   n|       pct|\n|:-----|:----------|---:|---------:|\n|mc-sc |caus       | 184| 45.657568|\n|mc-sc |temp       |  91| 22.580645|\n|sc-mc |caus       |  15|  3.722084|\n|sc-mc |temp       | 113| 28.039702|\n\n\n:::\n:::\n\n\nNext, we visualise the `ORDER` distribution using a barplot with a\ncustom y-axis, requiring `geom_col()`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Plot distribution\n\ndata_order_subord %>%\n  # Map variables onto axes\n  ggplot(aes(x = ORDER, y = pct, fill = SUBORDTYPE)) +\n    # Define plot type\n    geom_col(pos = \"dodge\") +\n    # Define theme\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](Hypothesis_testing_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n#### Running the test\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cross-tabulate the frequencies for the variables of interest\n\nfreqs <- table(data$ORDER, data$SUBORDTYPE)\n\nfreqs ## Assumption met: all observed freqs => 1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n```\n\n\n:::\n\n```{.r .cell-code}\n# Run a chis-quared test on the absolute frequencies and print the results\n\ntest <- chisq.test(freqs, correct = FALSE)\n\n# Inspect expected frequencies\n\ntest$expected # Assumption met: all expected frequences => 5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n             caus      temp\n  mc-sc 135.79404 139.20596\n  sc-mc  63.20596  64.79404\n```\n\n\n:::\n:::\n\n\n#### Optional: Effect size\n\nThe sample-size independent effect size measure **Cramer's *V***\n($\\phi$) is defined as\n\n$$V = \\sqrt{\\frac{\\chi^2}{N \\times df}}.$$ The outcome varies between\n$0$ (= no correlation) and $1$ (= perfect correlation); cf. also Gries\n[-@gries_statistics_2013: 186].\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Compute Cramer's V\n\n## By hand:\n\n# Given chi-squared statistic\nchi_squared <- unname(test$statistic)\n\n# Total number of observations\ntotal_obs <- sum(freqs)\n\nsqrt(chi_squared / total_obs * (min(dim(freqs)) - 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5139168\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n## Automatically:\nlibrary(\"confintr\") # Load library\n\ncramersv(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5139168\n```\n\n\n:::\n:::\n\n\n#### Reporting the results\n\nAccording to a $\\chi^2$-test, there is a significant association between\nclause `ORDER`and `SUBORDTYPE` at $p < 0.001$\n($\\chi^2 = 106.44, df = 1$), thus justifying the rejection of $H_0$.\n\n### $t$-test\n\n#### Define hypotheses\n\n-   $H_0:$ mean `F1 frequency` of men $=$ mean `F1 frequency` of women.\n\n-   $H_1:$ mean `F1 frequency` of men $\\ne$ mean `F1 frequency` of\n    women.\n\n#### Descriptive overview\n\nWe select the variables of interest and proceed calculate the mean\n`F1 frequencies` for each level of `SEX`, requiring a grouped data\nframe.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Filter data so as to show only those observations that are relevant\n\ndata_vowels %>% \n  # Filter columns\n  select(HZ_F1, SEX) %>%\n    # Define grouping variable\n    group_by(SEX) %>% \n      # Compute mean and standard deviation for each sex\n      summarise(mean = mean(HZ_F1),\n                sd = sd(HZ_F1)) -> data_vowels_stats\n\nknitr::kable(data_vowels_stats)\n```\n\n::: {.cell-output-display}\n\n\n|SEX |     mean|        sd|\n|:---|--------:|---------:|\n|F   | 528.8548| 110.80099|\n|M   | 484.2740|  87.90112|\n\n\n:::\n:::\n\n::: {.cell ouput='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Plot distribution\n\n## Plot means\n\ndata_vowels_stats %>% \n  ggplot(aes(x = SEX, y = mean)) +\n    geom_col() +\n    geom_errorbar(aes(x = SEX,\n                    ymin = mean-sd,\n                    ymax = mean+sd), width = .2) +\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](Hypothesis_testing_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n## Plot quartiles\ndata_vowels %>% \n  ggplot(aes(x = SEX, y = HZ_F1)) +\n    geom_boxplot() +\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](Hypothesis_testing_files/figure-html/unnamed-chunk-18-2.png){width=672}\n:::\n:::\n\n\n#### Check $t$-test assumptions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Normality\n\nshapiro.test(data_vowels$HZ_F1) # H0: data points follow the normal distribution\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  data_vowels$HZ_F1\nW = 0.98996, p-value = 0.5311\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check histogram\n\nggplot(data_vowels, aes(x = HZ_F1)) +\n  geom_histogram(bins = 30) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Hypothesis_testing_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Variance homogeneity\n\nvar.test(data_vowels$HZ_F1 ~ data_vowels$SEX) # H0: variances are not too different from each other\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tF test to compare two variances\n\ndata:  data_vowels$HZ_F1 by data_vowels$SEX\nF = 1.5889, num df = 59, denom df = 59, p-value = 0.07789\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.949093 2.660040\nsample estimates:\nratio of variances \n          1.588907 \n```\n\n\n:::\n:::\n\n\n#### Running the test\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# t-test for two independent samples \n\nt.test(data_vowels$HZ_F1 ~ data_vowels$SEX, paired = FALSE) # there is a significant difference!\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  data_vowels$HZ_F1 by data_vowels$SEX\nt = 2.4416, df = 112.19, p-value = 0.01619\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n  8.403651 80.758016\nsample estimates:\nmean in group F mean in group M \n       528.8548        484.2740 \n```\n\n\n:::\n:::\n\n\n#### Optional: Effect size\n\n**Cohen's** ***d*** is a possible effect size measure for continuous\ndata and is obtained by dividing the difference of both sample means by\nthe pooled standard deviation:\n\n$$\\frac{\\bar{x} - \\bar{y}}{\\sqrt{\\frac{{(n_1 - 1)s_x^2 + (n_2 - 1)s_y^2}}{{n_1 + n_2 - 2}}}}.$$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(\"effsize\")\n\n# By hand:\n## Compute pooled standard deviation sp\nsp <- sqrt(((n_m - 1) * var_m + (n_f - 1) * var_f) / (n_m + n_f - 2))\n\n## Compute Cohen's d\nd <- abs(mean_m - mean_f) / sp\n\n# Automatically:\ncohen.d(data_vowels$HZ_F1, data_vowels$SEX) # see also ?cohen.d for more details\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCohen's d\n\nd estimate: 0.4457697 (small)\n95 percent confidence interval:\n     lower      upper \n0.07976048 0.81177897 \n```\n\n\n:::\n:::\n\n\n#### Reporting the results\n\nAccording to a two-sample $t$-test, there is a significant difference\nbetween the mean `F1 frequencies` of male and female speakers of Apache\n($t = 2.44$, $df = 112.19$, $p < 0.05$). Therefore, $H_0$ will be\nrejected.\n",
    "supporting": [
      "Hypothesis_testing_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}