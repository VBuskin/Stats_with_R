{
  "hash": "3a1bd064b1b333ed619ffef2a37eab0a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hypothesis testing\"\nauthor:\n  name: \"Vladimir Buskin\" \n  orcid: \"0009-0005-5824-1012\"\n  affiliation: \n    name: \"Catholic University of Eichstätt-Ingolstadt\"\n    department: \"English Language and Linguistics\"\nformat:\n  html:\n    self-contained: true\n    code-fold: false\n    theme: default\n    toc: true\n    toc-depth: 4\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 3\n    scrollable: true\neditor: visual\nbibliography: R.bib\n---\n\n\n## Suggested reading\n\nFor linguists:\n\n> @gries_statistics_2021: Chapter 1.3.2\n\nGeneral:\n\n> @baguleySeriousStatsGuide2012: Chapter 4\n>\n> @agrestiFoundationsStatisticsData2022: Chapter 5\n>\n> @dienesUnderstandingPsychologyScience2008\n\n## On scientific inference {#sec-hyp}\n\nScience begins and ends with theory, and statistics acts as the\n\"go-between\". Regardless of the discipline, solid research is\ncharacterised by a robust theoretical foundation that gives rise to\n**substantive hypotheses**, i.e., theory-driven predictions about a\npopulation of interest. From this rather concrete hypothesis, it should\nbe possible to derive a **statistical hypothesis** that re-states the\nprediction in more formal/mathematical terms. After checking it against\nreal-world data, researchers can either **confirm** or **reject** their\nhypothesis, after which they may decide to amend (or even abandon) their\ntheory – or keep it as is.\n\n## Null hypothesis significance testing (NHST)\n\nThe **NHST** framework offers researchers a convenient way of testing\ntheir theoretical assumptions. This chiefly involves setting up a set of\n(ideally) falsifiable statistical hypotheses, gathering evidence from\nthe observed data and computing the (in)famous '$p$-value' to determine\n\"statistical significance\" -- a notion that is frequently misinterpreted\nin scientific studies.\n\n::: callout-note\n## Is this the only way of testing hypotheses?\n\nThe answer is a resounding **no**. Despite its immense popularity, NHST\nis problematic in many respects and hence subject to heavy criticism\n(cf. @dienesUnderstandingPsychologyScience2008: 76;\n@baguleySeriousStatsGuide2012: 143-144). There are other statistical\nschools that can remedy many of its shortcomings and come with distinct\nadvantages, such as those relying on likelihood-based inference and\nBayesian principles. Although these are also becoming increasingly\ncommon in linguistics, they are still restricted to very few\nsub-disciplines and journals (mostly in the area of psycholinguistics).\n:::\n\n### $H_0$ vs. $H_1$\n\nStatistical hypotheses always come in pairs: A **null hypothesis** is\naccompanied by an **alternative hypothesis**. They are set up before (!)\nseeing the data and justified by previous research.\n\n-   The **null hypothesis** $H_0$ describes the \"default state of the world\" [@james_introduction_2021: 555]. It claims there is no noteworthy effect to be observed in the data.\n\n-   The **alternative hypothesis** $H_1$ (or $H_a$) plainly states that\n    the $H_0$ is false, suggesting that there is an effect of some kind.\n    \n::: {.callout-tip collapse=\"true\" title=\"Example: Hypotheses for categorical data\"}\nWe are interested in finding out whether\nEnglish clause `ORDER` ('sc-mc' or 'mc-sc') depends on the type of the\nsubordinate clause (`SUBORDTYPE`), which can be either temporal ('temp')\nor causal ('caus').\n\nOur hypotheses are:\n\n-   $H_0:$ The variables `ORDER` and `SUBORDTYPE` are independent.\n\n-   $H_1:$ The variables `ORDER` and `SUBORDTYPE` are **not**\n    independent.\n:::\n\n::: {.callout-tip collapse=\"true\" title=\"Example: Hypotheses for continuous data\"}\nAs part of a phonetic study, we compare the\nbase frequencies of the F1 formants of vowels (in Hz) for male and\nfemale speakers of Apache. We forward the following hypotheses:\n\n-   $H_0:$ mean `F1 frequency` of men $=$ mean `F1 frequency` of women.\n\n-   $H_1:$ mean `F1 frequency` of men $\\ne$ mean `F1 frequency` of\n    women.\n:::\n\n::: {.callout-note collapse=\"true\" title=\"In formal terms\"}\n\nTo be precise, we use the hypotheses to make statements about a **population parameter** $\\theta$, which can be a mean $\\mu$ or a proportion $\\pi$, among other things. Mathematically, the null and alternative hypotheses can be restated as in @eq-hypotheses. \n\n$$\n\\begin{align}\nH_0: \\theta = 0  \\\\\nH_1: \\theta \\neq 0\n\\end{align}\n$$ {#eq-hypotheses}\n\n\n:::\n\nIn the NHST world, we're dealing with a \"This town ain't big enough for the both of us\" situation: While we have to state both $H_0$ and $H_1$, only one of them can remain at the end of the day. But how do we decide between these two?\n\n### Test statistics\n\nTo facilitate the decision-making process, we proceed to gather statistical evidence from the observed data. Since NHST primarily revolves around $H_0$ (and not $H_1$!), we need to review the evidence the data provides against or in favour $H_0$. This is done via a **test statistic** $T$ that characterises the sample at hand. There are many possible test statistics out there:\n\n-   For instance, if the data are discrete, the $\\chi^2$ measure is used to compute differences between observed and expected frequencies in the entire sample.\n-   In the case of continuos data, it is common to rely on $t$ for quantifying differences between sample means.\n-   Other possible test statistics include the correlation coefficient $r$, $z$-scores, the $F$-statistic, and many others.\n\n### Statistical significance\n\nThe final rejection of $H_0$ is determined by the **significance\n    probability** $p$. Due to the frequency and ferocity with which statistical significance is misinterpreted in the research literature, we will begin by reviewing its technical definition:\n    \n> \"The $p$-value is the probabilty, presuming that $H_0$ is true, that the test statistic equals the observed value or a value even more extreme in\n    the direction predicted by $H_a$\"  [@agrestiFoundationsStatisticsData2022: 163].\n\nIn compact notation, it is equivalent to the conditional probability \n\n$$\nP(T \\geq \\text{some value} \\mid H_0 \\text{ is true}).\n$$\nIf $p$ is lower than a pre-defined threshold (typically $0.05$), also known as the **significance level** $\\alpha$, we can reject $H_0$. However, if $p \\geq$ 0.05, this **neither justifies rejecting nor accepting** the null hypothesis     [@baguleySeriousStatsGuide2012: 121].\n\nFor example, a $p$-value of $0.02$ means that we would see a test statistic $T$ only 2% of the time if $H_0$ were true. Since $0.02$ lies below our significance level $\\alpha$ = $0.05$, this would suggest a statistically significant relationship in the data, and we could therefore reject $H_0$.\n\n\n### What could go wrong?\n\n#### Type I and Type II errors\n\nThere is always a chance that we accept or reject the wrong hypothesis;\nthe four possible constellations are summarised in the table below [cf.\n@heumann_introduction_2022: 223]:\n\n|                           | $H_0$ is true                                       | $H_0$ is not true                                   |\n|-------------------|---------------------------|---------------------------|\n| $H_0$ **is not rejected** | $\\color{green}{\\text{Correct decision}}$            | $\\color{red}{\\text{Type II } (\\beta)\\text{-error}}$ |\n| $H_0$ **is rejected**     | $\\color{red}{\\text{Type I } (\\alpha)\\text{-error}}$ | $\\color{green}{\\text{Correct decision}}$            |\n\n#### Common pitfalls\n\n### The mathematics of the $p$-value\n\nLet's say that the statistical analysis of clause `ORDER` and\n`SUBORDTYPE` has returned a test statistic of $\\chi^2 = 6.5$ for 2 $df$.\nIn order to compute the probability $P(\\chi^2 \\geq 5)$, we need to\nconsult the **sampling distribution** of this test statistic. The\nsampling distribution is a probability distribution that assigns\nprobabilities to the values of a test statistic.\n\nThe probability density function $f(x)$ of the $\\chi^2$-distribution for\n2 degrees of freedom (abbreviated $df$ and affect the function's shape)\nis visualised below. The $p$-value corresponds to the green area under\ncurve ranging from $x = 6.5$ up to $\\infty$.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Hypothesis_testing_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nThe probability $P(\\chi^2 \\geq 6.5)$ can be obtained by integrating over\nall $\\chi^2$-values in the range $0 \\leq X < 6.5$ and subtracting them\nfrom 1, which represents the total area under the curve\n\n$$\nP(X\\geq 6.5) = 1 - \\int_{0}^{6.5} f(x) dx\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For a chi-squared test statistic of 6.5 with 2 degrees of freedom\np_value <- 1 - pchisq(6.5, df = 2)\n\nprint(p_value) # significant!\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03877421\n```\n\n\n:::\n:::\n",
    "supporting": [
      "Hypothesis_testing_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}