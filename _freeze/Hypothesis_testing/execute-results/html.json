{
  "hash": "899a29899db0771a4c0dd022b24bdba9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hypothesis testing\"\nauthor:\n  name: \"Vladimir Buskin\" \n  orcid: \"0009-0005-5824-1012\"\n  affiliation: \n    name: \"Catholic University of Eichstätt-Ingolstadt\"\n    department: \"English Language and Linguistics\"\nformat:\n  html:\n    self-contained: true\n    code-fold: false\n    theme: default\n    toc: true\n    toc-depth: 4\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 3\n    scrollable: true\neditor: visual\nbibliography: R.bib\n---\n\n\n## Suggested reading\n\nFor linguists:\n\n> @gries_statistics_2021: Chapter 1.3.2\n\nGeneral:\n\n> @baguleySeriousStatsGuide2012: Chapter 4\n>\n> @agrestiFoundationsStatisticsData2022: Chapter 5\n>\n> @dienesUnderstandingPsychologyScience2008\n\n## On scientific inference {#sec-hyp}\n\nScience begins and ends with theory, and statistics acts as the\n\"go-between\". Regardless of the discipline, solid research is\ncharacterised by a robust theoretical foundation that gives rise to\n**substantive hypotheses**, i.e., theory-driven predictions about a\npopulation of interest. From this rather concrete hypothesis, it should\nbe possible to derive a **statistical hypothesis** that re-states the\nprediction in more formal/mathematical terms. After checking it against\nreal-world data, researchers can either **confirm** or **reject** their\nhypothesis, after which they may decide to amend (or even abandon) their\ntheory – or keep it as is.\n\n## Null hypothesis significance testing (NHST)\n\nThe **NHST** framework offers researchers a convenient way of testing\ntheir theoretical assumptions. This chiefly involves setting up a set of\n(ideally) falsifiable statistical hypotheses, gathering evidence from\nthe observed data and computing the (in)famous '$p$-value' to determine\n\"statistical significance\" -- a notion that is frequently misinterpreted\nin scientific studies.\n\n::: callout-note\n## Is this the only way of testing hypotheses?\n\nThe answer is a resounding **no**. Despite its immense popularity, NHST\nis problematic in many respects and hence subject to heavy criticism\n(cf. @dienesUnderstandingPsychologyScience2008: 76;\n@baguleySeriousStatsGuide2012: 143-144). There are other statistical\nschools that can remedy many of its shortcomings and come with distinct\nadvantages, such as those relying on likelihood-based inference and\nBayesian principles. Although these are also becoming increasingly\ncommon in linguistics, they are still restricted to very few\nsub-disciplines and journals (mostly in the area of psycholinguistics).\n:::\n\n### $H_0$ vs. $H_1$\n\nStatistical hypotheses always come in pairs: A **null hypothesis** is\naccompanied by an **alternative hypothesis**. They are set up before (!)\nseeing the data and justified by previous research.\n\n-   The **null hypothesis** $H_0$ describes the \"default state of the\n    world\" [@james_introduction_2021: 555]. It claims there is no\n    noteworthy effect to be observed in the data.\n\n-   The **alternative hypothesis** $H_1$ (or $H_a$) plainly states that\n    the $H_0$ is false, suggesting that there is an effect of some kind.\n\n::: {.callout-tip collapse=\"true\" title=\"Example: Hypotheses for categorical data\"}\nWe are interested in finding out whether English clause `ORDER` ('sc-mc'\nor 'mc-sc') depends on the type of the subordinate clause\n(`SUBORDTYPE`), which can be either temporal ('temp') or causal\n('caus').\n\nOur hypotheses are:\n\n-   $H_0:$ The variables `ORDER` and `SUBORDTYPE` are independent.\n\n-   $H_1:$ The variables `ORDER` and `SUBORDTYPE` are **not**\n    independent.\n:::\n\n::: {.callout-tip collapse=\"true\" title=\"Example: Hypotheses for continuous data\"}\nAs part of a phonetic study, we compare the base frequencies of the F1\nformants of vowels (in Hz) for male and female speakers of Apache. We\nforward the following hypotheses:\n\n-   $H_0:$ mean `F1 frequency` of men $=$ mean `F1 frequency` of women.\n\n-   $H_1:$ mean `F1 frequency` of men $\\ne$ mean `F1 frequency` of\n    women.\n:::\n\n::: {.callout-note collapse=\"true\" title=\"In formal terms\"}\nTo be precise, we use the hypotheses to make statements about a\n**population parameter** $\\theta$, which can be a mean $\\mu$ for\ncontinuous data or a proportion $\\pi$ for categorical data, among other\nthings. Mathematically, the null and alternative hypotheses can be\nrestated as in @eq-hypotheses.\n\n$$\n\\begin{align}\nH_0: \\theta = 0  \\\\\nH_1: \\theta \\neq 0\n\\end{align}\n$$ {#eq-hypotheses}\n:::\n\nIn the NHST world, we're dealing with a \"This town ain't big enough for\nthe both of us\" situation: While we have to state both $H_0$ and $H_1$,\nonly one of them can remain at the end of the day. But how do we decide\nbetween these two?\n\n### Test statistics\n\nTo facilitate the decision-making process, we proceed to gather\nstatistical evidence from the observed data. Since NHST primarily\nrevolves around $H_0$ (and not $H_1$!), we need to review the evidence\nthe data provides against or in favour $H_0$. This is done via a **test\nstatistic** $T$ that characterises the sample at hand. Essentially, you\ncan think of $T$ as one-value summary of your data.\n\nThere are many possible test statistics out there:\n\n-   For instance, if the data are **discrete**, the $\\chi^2$ measure is\n    used to compute differences between observed and expected\n    frequencies in the entire sample.\n-   In the case of **continuos** data, it is common to rely on $t$ for\n    quantifying differences between sample means.\n-   Other possible test statistics include the correlation coefficient\n    $r$, $z$-scores, the $F$-statistic, and many others.\n\n### Statistical significance\n\nThe final rejection of $H_0$ is determined by the **significance\nprobability** $p$. Due to the frequency and ferocity with which\nstatistical significance is misinterpreted in the research literature,\nwe will begin by reviewing its technical definition:\n\n> \"The $p$-value is the probabilty, presuming that $H_0$ is true, that\n> the test statistic equals the observed value or a value even more\n> extreme in the direction predicted by $H_a$\"\n> [@agrestiFoundationsStatisticsData2022: 163].\n\nIn compact notation, it is equivalent to the conditional probability\n\n$$\nP(T \\geq \\text{observed value} \\mid H_0 \\text{ is true}).\n$$ If $p$ is lower than a pre-defined threshold (typically $0.05$), also\nknown as the **significance level** $\\alpha$, we can reject $H_0$.\nHowever, if $p \\geq$ 0.05, this **neither justifies rejecting nor\naccepting** the null hypothesis [@baguleySeriousStatsGuide2012: 121].\n\nFor example, a $p$-value of $0.02$ means that we would see a test\nstatistic $T$ only 2% of the time if $H_0$ were true. Since $0.02$ lies\nbelow our significance level $\\alpha$ = $0.05$, this would suggest a\nstatistically significant relationship in the data, and we could\ntherefore reject $H_0$.\n\n### What could go wrong? Type I and Type II errors\n\nThere is always a chance that we accept or reject the wrong hypothesis;\nthe four possible constellations are summarised in the table below [cf.\n@heumann_introduction_2022: 223]:\n\n|                           | $H_0$ is true                                       | $H_0$ is not true                                   |\n|-------------------|---------------------------|---------------------------|\n| $H_0$ **is not rejected** | $\\color{green}{\\text{Correct decision}}$            | $\\color{red}{\\text{Type II } (\\beta)\\text{-error}}$ |\n| $H_0$ **is rejected**     | $\\color{red}{\\text{Type I } (\\alpha)\\text{-error}}$ | $\\color{green}{\\text{Correct decision}}$            |\n\n### The mathematics of the $p$-value\n\nLet's say that the statistical analysis of clause `ORDER` and\n`SUBORDTYPE` has returned a test statistic of $\\chi^2 = 6.5$ for 2 $df$.\nIn order to compute the corresponding $p$-value we need to consult the\n**sampling distribution** of this test statistic.\n\nA sampling distribution is a probability distribution that assigns\nprobabilities to the values of a test statistic. Because most (if not\nall) of them are continuous, they have characteristic **probability\ndensity functions** (PDFs). Some of them are illustrated below:\n\n::: {.callout-note collapse=\"true\" title=\"$\\\\chi^2$ distribution\"}\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n# Load ggplot2\nlibrary(ggplot2)\n\n# Define the degrees of freedom\ndf <- 2\n\n# Create a sequence of x values\nx <- seq(0, 30, length.out = 1000)\n\n# Compute the chi-squared density\ny <- dchisq(x, df = df)\n\n# Create a data frame\nchi_squared_data <- data.frame(x = x, y = y)\n\n# Generate the plot\nggplot(chi_squared_data, aes(x = x, y = y)) +\n  geom_line(color = \"steelblue\", size = 1) + # Line for the density curve\n  labs(\n    title = \"Chi-Squared Distribution\",\n    subtitle = \"Probability density function with 2 degrees of freedom\",\n    x = \"Chi-squared value\",\n    y = \"Probability density\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(ylim = c(0, 0.05), xlim = c(0, 30)) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.title = element_text(size = 12)\n  )\n```\n\n::: {.cell-output-display}\n![](Hypothesis_testing_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n:::\n\n::: {.callout-note collapse=\"true\" title=\"$t$ distribution\"}\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n# Define the degrees of freedom\ndf_t <- 10\n\n# Create a sequence of x values\nx_t <- seq(-4, 4, length.out = 1000)\n\n# Compute the t-distribution density\ny_t <- dt(x_t, df = df_t)\n\n# Create a data frame\nt_distribution_data <- data.frame(x = x_t, y = y_t)\n\n# Generate the plot\nggplot(t_distribution_data, aes(x = x, y = y)) +\n  geom_line(color = \"steelblue\", size = 1) + # Line for the density curve\n  labs(\n    title = \"t-Distribution\",\n    subtitle = \"Probability density function with 10 degrees of freedom\",\n    x = \"t value\",\n    y = \"Probability density\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(ylim = c(0, 0.4), xlim = c(-4, 4)) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.title = element_text(size = 12)\n  )\n```\n\n::: {.cell-output-display}\n![](Hypothesis_testing_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n:::\n\n::: {.callout-note collapse=\"true\" title=\"$F$ distribution\"}\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n# Define the degrees of freedom\ndf1 <- 5\ndf2 <- 10\n\n# Create a sequence of x values\nx_f <- seq(0, 5, length.out = 1000)\n\n# Compute the F-distribution density\ny_f <- df(x_f, df1 = df1, df2 = df2)\n\n# Create a data frame\nf_distribution_data <- data.frame(x = x_f, y = y_f)\n\n# Generate the plot\nggplot(f_distribution_data, aes(x = x, y = y)) +\n  geom_line(color = \"steelblue\", size = 1) + # Line for the density curve\n  labs(\n    title = \"F-Distribution\",\n    subtitle = \"Probability density function with 5 and 10 degrees of freedom\",\n    x = \"F value\",\n    y = \"Probability density\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(ylim = c(0, 1), xlim = c(0, 5)) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.title = element_text(size = 12)\n  )\n```\n\n::: {.cell-output-display}\n![](Hypothesis_testing_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n:::\n\nBecause continuous functions have an infinite number of $x$-values, the\nprobability of any single value must be 0.[^hypothesis_testing-1]\nTherefore, if we are interested in obtaining actual probabilities from\nthe PDF, we can only do so for intervals of values. The probability that\na value $X$ falls into the interval $a < X < b$ is in fact equivalent to\nthe area under the curve between $a$ and $b$ (cf. @eq-pdf).\n\n[^hypothesis_testing-1]: The proof for the underlying theorem is given\n    in Heumann et al. [-@heumann_introduction_2022: 544].\n\n$$\nP(a < X < b) = \\int_a^b f(x)dx.\n$$ {#eq-pdf}\n\nRecall the PDF $f(x)$ of the $\\chi^2$-distribution with 2 degrees of\nfreedom. The $p$-value corresponds to the green area under the curve\nranging from $x = 6.5$ up to $\\infty$, which can be restated formally in\n@eq-pdf-chisq. This brings us back to the definition of the $p$-value:\nIt is the probability that the $\\chi^2$ score is equal to 6.5 or\n**higher**, i.e., $P(\\chi^2 \\geq 6.5)$.\n\n$$\nP(6.5 < X < \\infty) = \\int_{6.5}^\\infty f(x)dx.\n$$ {#eq-pdf-chisq}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Hypothesis_testing_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Practical considerations\n\n::: {.callout-important title=\"Common pitfalls [cf. @agrestiFoundationsStatisticsData2022: 189-190]\"}\n-   Statistical significance is **NOT** an indication of a causal\n    relationship between the variables of interest (**correlation**\n    $\\neq$ **causation**).\n\n-   $p$-values do **NOT** signify the strength of an effect ($\\neq$\n    effect size). It only helps identify whether there is an effect to\n    begin with.\n\n-   $p$-values are **NOT** the probability of the null hypothesis being\n    true.\n\n-   Statistical significance is only a starting point for further\n    scientific inquiry, and by no means the end of it.\n:::\n\n## Exercises\n\n::: {#exr-hyp-1}\nSchröter & Kortmann [-@schroter_pronoun_2016] investigate the\nrelationship between subject realisation (`overt` vs. `null`) and the\ngrammatical category Person (`1.p`. vs. `2.p.` vs. `3.p.`) in three\nvarieties of English (`Great Britain` vs. `Hong Kong` vs. `Singapore`).\nThey report the following test results [-@schroter_pronoun_2016: 235]:\n\n> Chi-square test scores: $$\n> \\begin{align}\n> \\text{Singapore: \\quad} & \\chi^2 = 3.3245, df = 2, p = 0.1897 \\\\\n> \\text{Hong Kong: \\quad} & \\chi^2 = 40.799, df = 2, p < 0.01 \\\\\n> \\text{Great Britain: \\quad} & \\chi^2 = 3.6183, df = 2, p = 0.1638 \\\\\n> \\end{align}\n> $$\n\n-   What hypotheses are the authors testing?\n-   Assuming a significance level $\\alpha = 0.05$, what statistical conclusions can\n    be drawn from the test results?\n-   What could be the theoretical implications of these results?\n:::\n\n::: {#exr-hyp-2}\nTry to develop statistical hypotheses for a research project you are\ncurrently working on!\n:::\n",
    "supporting": [
      "Hypothesis_testing_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}