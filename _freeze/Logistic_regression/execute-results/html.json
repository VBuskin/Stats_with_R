{
  "hash": "2a43e7c59b2feec8eb115af442c88591",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Logistic Regression\"\nauthor: Vladimir Buskin\nformat:\n  html:\n    self-contained: true\n    logo: logo.png\n    footer: \"Regression\"\n    theme: Reference\n    toc: true\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 4\n    scrollable: true\neditor: visual\nbibliography: R.bib\n---\n\n\nThis unit is based on James et al. [-@james_introduction_2021: Chapter\n4] and Levshina [-@levshina_how_2015: Chapter 12]. For an in-depth\nintroduction, see Hosmer & Lemeshow [-@hosmer_applied_2008].\n\n## Preparation {.smaller}\n\n\n::: {.cell}\n\n:::\n\n\nConsider the data from Buskin's [-@buskinDefiniteNullInstantiationfc]\ncorpus-study on subject pronoun realisation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(\"tidyverse\")\nlibrary(\"rms\") # For regression modelling\n\n# Load data\ndata_pro <- read_csv(\"data/Buskin_pronoun_data.csv\", sep = \",\", header = TRUE)\n```\n:::\n\n\n-   **Target variable**:\n\n    -   `Reference` ('overt', 'null')\n\n-   **Explanatory variables**:\n\n    -   `Person` ('1.p.', '2.p', '3.p' as well as the dummy pronouns\n        'it' and 'there')\n\n    -   `Register` (the text category in the International Corpus of\n        English; 'S1A' are informal conversations, whereas 'S1B'\n        comprises formal class lessons)\n\n    -   `Variety` (British English 'GB', Singapore English 'SING' and\n        Hong Kong English 'HK') and\n\n    -   `Referentiality` ('referential' with an identifiable referent or\n        'non-referential' with no/generic reference)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(data_pro)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Reference Person Register Variety Referentiality\n1     overt      3      S1A      GB    referential\n2     overt      3      S1A      GB    referential\n3     overt      3      S1A      GB    referential\n4     overt      3      S1A      GB    referential\n5     overt      3      S1A      GB    referential\n6     overt      3      S1A      GB    referential\n```\n\n\n:::\n\n```{.r .cell-code}\ntable(data_pro$Reference)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n null overt \n  174  4664 \n```\n\n\n:::\n:::\n\n\n### Descriptive overview\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\n# Raw data for Ref x Reg x Var\ndata_pro %>% \n  count(Reference, Register, Variety) %>% \n  filter(Reference == \"null\") %>% \n  mutate(pct = n/sum(n) * 100) -> pro_stats1\n\n# Raw data for Ref x Per x Var\ndata_pro %>% \n  count(Reference, Person, Variety) %>% \n  filter(Reference == \"null\") %>% \n  mutate(pct = n/sum(n) * 100) -> pro_stats2\n\n# Raw data for Ref x Referent. x Var\ndata_pro %>% \n  count(Reference, Referentiality, Variety) %>% \n  filter(Reference == \"null\") %>% \n  mutate(pct = n/sum(n) * 100) -> pro_stats3\n\n# Plot 1\n\npro_stats1 %>%\n  ggplot(aes(x = Variety, y = pct, color = Register)) +\n  geom_point(size = 3) +\n  theme_minimal() +\n  labs(\n    title = \"Null subjects by Variety and Register\",\n    y = \"Proportion of null subjects\"\n  )\n```\n\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\n# Plot 2\n\npro_stats2 %>%\n  ggplot(aes(x = Variety, y = pct, color = Person)) +\n  geom_point(size = 3) +\n  theme_minimal() +\n  labs(\n    title = \"Null subjects by Variety and Person\",\n    y = \"Proportion of null subjects\"\n  )\n```\n\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\n# Plot 3\n\npro_stats3 %>%\n  ggplot(aes(x = Variety, y = pct, color = Referentiality)) +\n  geom_point(size = 3) +\n  theme_minimal() +\n  labs(\n    title = \"Null subjects by Variety and Referentiality\",\n    y = \"Proportion of null subjects\"\n  )\n```\n\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n:::\n\n\n## Logistic regression\n\n-   In contrast to linear regression, logistic regression models a\n    **qualitative response variable** $Y$ (here: `Reference`) with two\n    outcomes as a function of the independent variables $X_p$\n    (`Register`, Variety etc.). The goal is to predict a value for $Y$.\n\n-   For a given observation, the model should predict either a specific\n    category of $Y$ (e.g., `Reference = overt` or `Reference = null`) or\n    provide a probability estimation of a particular outcome (e.g., the\n    probability of `Reference = null`).\n\n-   The probability that `Reference = null` given a predictor (e.g.,\n    `Register`) can be written as\n    $P(\\text{Reference} = \\text{null} \\mid \\text{Register})$ or more\n    simply as $p(\\text{Register})$.\n\n-   A core component of logistic regression is the **logistic\n    function**. The rationale for using it is that the output of the\n    function will always lie between $0$ and $1$, and it will always\n    denote a **probability**.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n### The logistic model\n\nAssuming a binary response variable $Y$ with the values 1 and 0 and a\nsingle predictor $X$, we can model the probability\n$P(Y = 1\\vert X) = p(X)$ as\n\n$$\np(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}.\n$$\n\nNote that $e \\approx 2.71828$. This expression is equivalent to\n\n$$\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0 + \\beta_1X.\n$$\n\nThe fraction $\\frac{p(X)}{1-p(X)}$ represents the **odds**, which stand\nfor to the probability of one outcome (e.g., `Reference = null`)\ncompared to the other (e.g., `Reference = overt`). Their logarithmic\ntransformation are the **log odds** (or **logits**) of a model. When\ninterpreting the output of a logistic model, note that\n\n-   positive log odds indicate an **increase** in $p(X)$, whereas\n\n-   negative log odds indicate a **decrease** in $p(X)$.\n\nIf $X = \\text{Register}$, then our model has the form:\n\n$$\n\\log\\left(\\frac{P(\\text{Reference} = \\text{null} \\mid \\text{Register})}{1- P(\\text{Reference} = \\text{null} \\mid \\text{Register})}\\right) = \\beta_0 + \\beta_1\\text{Register}\n$$\n\n### Multiple logistic regression\n\nIf more than one predictor is included, the above equations can be\nexpanded so as to take into account $p$ slopes $\\beta_p$ for $p$\nindependent variables $X_p$.\n\n$$\np(X) = \\frac{e^{\\beta_0 + \\beta_1X_1 + ... +  \\beta_pX_p}}{1 + e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}.\n$$ Thus, the log odds correspond to the sum of $\\beta_pX_p$,\n\n$$\n\\begin{aligned}\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right) &= \\beta_0 + \\beta_1X_1 + ... +  \\beta_pX_p \\\\\n& = \\beta_0 + \\sum_{i=1}^{p} \\beta_i X_i\n\\end{aligned}\n$$ respectively.\n\nReplacing the variables with the predictors in the data set, the\nequation has the final form\n\n$$\n\\log\\left(\\frac{P(\\text{Reference} = \\text{null} \\mid \\text{Register}, \\text{Variety}, \\text{Person}, \\text{Referentiality})}{1- P(\\text{Reference} = \\text{null} \\mid \\text{Register}, \\text{Variety}, \\text{Person}, \\text{Referentiality})}\\right) = \\beta_0 + \\beta_1\\text{Register} + \\beta_2\\text{Variety} + \\beta_3\\text{Person} + \\beta_4\\text{Referentiality}.\n$$\n\n### Odds ratios\n\nTo assess the strength of an effect, it is instructive to examine the\n**odds ratios** that correspond to the model coefficients. Odds ratios\n(OR) are defined as\n\n$$\nOR(X_1) = e^{\\beta_1}.\n$$\n\nEssentially, the OR describes the ratio between two odds with respect to\nanother independent variable. This is illustrated for `Reference` given\n`Register` below:\n\n$$\n\\text{OR}(\\text{Reference} \\mid \\text{Register}) = \\frac{\\frac{P(\\text{Reference} = \\text{null} \\mid \\text{Register} = \\text{S1A})}{P(\\text{Reference} = \\text{overt} \\mid \\text{Register} = \\text{S1A})}}{\\frac{P(\\text{Reference} = \\text{null} \\mid \\text{Register} = \\text{S1B})}{P(\\text{Reference} = \\text{overt} \\mid \\text{Register} = \\text{S1B})}}\n$$ Read as: '**The ratio between** the probability of a null vs. overt\nobject in S1A **and** the probability of a null vs. overt object in\nS1B'.\n\n## Workflow in R\n\n### Step 1: Research question and hypotheses\n\nHow do the intra- and extra-linguistic variables suggested in the\nliterature affect subject pronoun realisation (Definite Null\nInstantiation) in British English, Singapore English and Hong Kong\nEnglish?\n\nGiven a significance level $\\alpha = 0.05$, the hypotheses are: $$ \n\\begin{aligned}\nH_0: & \\quad \\text{None of the predictor coefficients deviate from 0}.\\\\\nH_1: & \\quad \\text{At least one predictor coefficient deviates from 0}.\n\\end{aligned}\n$$\n\nThese can be restated mathematically as:\n\n$$ \n\\begin{aligned}\nH_0: & \\quad \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0 \\\\\nH_1: & \\quad \\text{At least one } \\beta_i \\neq 0 \\text{ for } i \\in \\{1, 2, \\ldots, p\\}\n\\end{aligned} $$\n\n### Step 2: Convert to factors and specify reference levels\n\nThe next step involves specifying **reference levels** for all\ncategorical variables. This step is very important because it will\ndirectly impact the parameter estimation and, consequently, influence\nour interpretation of the model output.\n\n-   The reference level of the response is usually chosen such that it\n    corresponds to the **unmarked or most frequent case**. Since overt\n    pronouns are much more common in the data, the reference level of\n    the `Reference` variable will be set to `Reference = overt`. This\n    way, the model coefficients will directly represent the probability\n    of the **null** **subject** variant (i.e., the special case) given\n    certain predictor configurations.\n\n-   The **predictor levels** need to be specified as well. Among other\n    things, we are interested in how the Asian Englishes pattern\n    relative to British English. Therefore, we will define British\n    English as the baseline for comparison.\n\nWe will use the following specifications:\n\n| **Variable**   | **Factor Levels**            | **Preferred Reference level** |\n|-------------------|--------------------------|---------------------------|\n| Register       | S1A, S1B                     | S1A                           |\n| Variety        | GB, SING, HK                 | GB                            |\n| Person         | 1, 2, 3, *it*, *there*       | 3                             |\n| Referentiality | referential, non-referential | referential                   |\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Store \"Reference\" as factor\ndata_pro$Reference <- as.factor(data_pro$Reference)\n\n## Specify reference level (the 'unmarked' case)\ndata_pro$Reference <- relevel(data_pro$Reference, \"overt\")\n\n## Print levels\nlevels(data_pro$Reference)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"overt\" \"null\" \n```\n\n\n:::\n:::\n\n\nRepeat the procedure for the remaining categorical variables.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\n# Store \"Register\" as factor\ndata_pro$Register <- as.factor(data_pro$Register)\n\n## Specify reference level\ndata_pro$Register <- relevel(data_pro$Register, \"S1A\")\n\n# Store \"Variety\" as factor\ndata_pro$Variety <- as.factor(data_pro$Variety)\n\n## Specify reference level\ndata_pro$Variety <- relevel(data_pro$Variety, \"GB\")\n\n# Store \"Person\" as factor\ndata_pro$Person <- as.factor(data_pro$Person)\n\n## Specify reference level\ndata_pro$Person <- relevel(data_pro$Person, \"3\")\n\n# Store \"Referentiality\" as factor\ndata_pro$Referentiality <- as.factor(data_pro$Referentiality)\n\n## Specify reference level\ndata_pro$Referentiality <- relevel(data_pro$Referentiality, \"referential\")\n```\n:::\n\n\n### Step 3: Fit the model\n\nThere are two functions that can fit logistic models in R: `lrm()` and\n`glm()`.\n\n::: callout-note\nThe model formula below does not include `Referentiality` because\nseveral intermediary steps revealed it to be almost completely\nirrelevant for predicting `Reference`. In addition, the existing (and\nsignificant) interaction `Variety:Person` has been excluded to improve\nthe interpretability of the model.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# With lrm(); requires library(\"rms\")\n\n# Fit interaction model\nReference.lrm <- lrm(Reference ~ Register + Variety + Register:Variety + Person, data = data_pro)\n\n# View model statistics\nReference.lrm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLogistic Regression Model\n\nlrm(formula = Reference ~ Register + Variety + Register:Variety + \n    Person, data = data_pro)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs          4838    LR chi2     120.43      R2       0.092    C       0.729    \n overt       4664    d.f.             9     R2(9,4838)0.023    Dxy     0.458    \n null         174    Pr(> chi2) <0.0001    R2(9,503.2)0.199    gamma   0.488    \nmax |deriv| 4e-10                            Brier    0.034    tau-a   0.032    \n\n                            Coef    S.E.   Wald Z Pr(>|Z|)\nIntercept                   -3.4132 0.2746 -12.43 <0.0001 \nRegister=S1B                 0.0269 0.3807   0.07 0.9437  \nVariety=HK                   0.6712 0.3174   2.11 0.0345  \nVariety=SING                 1.1193 0.2959   3.78 0.0002  \nPerson=1                    -0.8807 0.1811  -4.86 <0.0001 \nPerson=2                    -1.6441 0.2695  -6.10 <0.0001 \nPerson=it                    0.7897 0.2978   2.65 0.0080  \nPerson=there                -2.5641 1.0095  -2.54 0.0111  \nRegister=S1B * Variety=HK    0.6035 0.4521   1.34 0.1819  \nRegister=S1B * Variety=SING -0.4753 0.4688  -1.01 0.3107  \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# With (glm); available in base R\n# Note the additional \"family\" argument!\nReference.glm <- glm(Reference ~ Register + Variety + Register:Variety + Person, data = data_pro, family = \"binomial\")\n\n# View model statistics\nsummary(Reference.glm)\n```\n:::\n\n\n::: callout-tip\n## Stepwise variable selection\n\nWith the function `drop1()`, it is possible to successively remove\nvariables from the complex model to ascertain which ones improve the\nmodel significantly (i.e., decrease the deviance and AIC scores).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(Reference.glm, test = \"Chisq\")\n```\n:::\n\n:::\n\n### Step 4: Confidence intervals and odds ratios\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tidy the model output\ntidy_model <- tidy(Reference.glm, conf.int = TRUE)\n\n# Remove intercept, compute odds ratios and their CIs\ntidy_model <- tidy_model %>% \n  filter(term != \"(Intercept)\") %>% \n  mutate(\n    odds_ratio = exp(estimate),\n    odds.conf.low = exp(conf.low),\n    odds.conf.high = exp(conf.high)\n  )\n```\n:::\n\n\n### Step 5: Visualise the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the coefficient plot\nggplot(tidy_model, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient Estimate (log-odds)\",\n    y = \"Predictor\",\n    title = \"Coefficient Estimates with Confidence Intervals\",\n    caption = \"*Note that the CIs of singificant predictors do not include 0.\"\n  )\n```\n\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot odds ratios\nggplot(tidy_model, aes(x = exp(estimate), y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = odds.conf.low, xmax = odds.conf.high), height = 0.2) +\n  geom_vline(xintercept = 1, linetype = \"dashed\", color = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient Estimate (odds ratios)\",\n    y = \"Predictor\",\n    title = \"Odds ratios with Confidence Intervals\",\n    caption = \"*Note that the CIs of singificant predictors do not include 1.\"\n  )\n```\n\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-12-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot marginal effects; y-axis = log odds of a null vs. overt subject\nplot(Effect(\"Register\", mod = Reference.glm)) \n```\n\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-12-3.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(Effect(\"Variety\", mod = Reference.glm))\n```\n\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-12-4.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(Effect(\"Person\", mod = Reference.glm))\n```\n\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-12-5.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot interactions\nplot(Effect(focal.predictors = c(\"Register\", \"Variety\"), mod = Reference.glm))\n```\n\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-12-6.png){width=672}\n:::\n:::\n\n\n### Step 6: Interpret the model\n\nThe logistic regression model is statistically significant at\n$p < 0.001$ ($\\chi^2 = 120.43$, $df = 9$) and has acceptable fit\n(Nagelkerke's-$R^2$ = $0.09$, $C = 0.73$).\n\nThe model coefficients indicate that null subjects are significantly\nmore likely in Singapore English compared to British English (Estimate =\n1.12, 95% CI \\[0.56, 1.73\\], $p < 0.001$). This effect is moderate with\nan $OR$ of 3.06 (95% CI \\[1.75, 5.64\\]), suggesting that the probability\nof subject omission is elevated by a factor of approximately 3 in the\nSingaporean variety.\n\n...\n\n### Step 7: Further model diagnostics\n\n-   **Cross-validation**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Refit the model with additional settings\nReference.val <- lrm(Reference ~ Register + Variety + Register:Variety + Person, data = data_pro, x = T, y = T)\n\n# Perform 200-fold cross-validation\nmodel.validated <- validate(Reference.val, B = 200); model.validated \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          index.orig training    test optimism index.corrected   n\nDxy           0.4592   0.4713  0.4455   0.0258          0.4335 200\nR2            0.0923   0.0999  0.0844   0.0156          0.0767 200\nIntercept     0.0000   0.0000 -0.2484   0.2484         -0.2484 200\nSlope         1.0000   1.0000  0.9136   0.0864          0.9136 200\nEmax          0.0000   0.0000  0.0717   0.0717          0.0717 200\nD             0.0247   0.0267  0.0225   0.0042          0.0205 200\nU            -0.0004  -0.0004  0.0003  -0.0007          0.0003 200\nQ             0.0251   0.0271  0.0222   0.0049          0.0202 200\nB             0.0336   0.0334  0.0337  -0.0003          0.0340 200\ng             1.0081   1.2305  1.1051   0.1254          0.8827 200\ngp            0.0319   0.0326  0.0304   0.0023          0.0296 200\n```\n\n\n:::\n\n```{.r .cell-code}\n# Slope optimism should be as low possible!\n```\n:::\n\n\n-   **Multicollinearity**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Variable inflation factors further reveal severe multicollinearity\nvif(Reference.lrm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Register=S1B                  Variety=HK \n                   5.818111                    4.006084 \n               Variety=SING                    Person=1 \n                   3.421687                    1.140407 \n                   Person=2                   Person=it \n                   1.089502                    1.102908 \n               Person=there   Register=S1B * Variety=HK \n                   1.007148                    6.218803 \nRegister=S1B * Variety=SING \n                   3.685075 \n```\n\n\n:::\n:::\n",
    "supporting": [
      "Logistic_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}