{
  "hash": "9692b482ad73a99fdafbb930325ab064",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Logistic Regression\"\nauthor: Vladimir Buskin\nformat:\n  html:\n    self-contained: true\n    logo: logo.png\n    footer: \"Regression\"\n    theme: Reference\n    toc: true\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 4\n    scrollable: true\neditor: visual\nbibliography: R.bib\n---\n\n\n## Recommended reading\n\nFor linguists:\n\n> Levshina [-@levshina_how_2015: Chapter 12]\n>\n> Winter [-@winter_statistics_2020: Chapter 12]\n\nFull theoretical treatment:\n\n> James et al. [-@james_introduction_2021: Chapter 4]\n>\n> Hosmer & Lemeshow [-@hosmer_applied_2008]\n\n## Preparation {.smaller}\n\nConsider the data from Buskin's\n[-@buskinDefiniteNullInstantiationfc][^logistic_regression-1]\ncorpus-study on subject pronoun realisation:\n\n[^logistic_regression-1]: The input data is can be downloaded from this\n    OSF repository: <https://osf.io/qgnms>.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(tidyverse)\nlibrary(rms)\nlibrary(broom)\nlibrary(sjPlot)\nlibrary(ggeffects)\nlibrary(ggpubr)\n\n# Load data\ndata_pro <- read.csv(\"INPUT_pronouns.csv\", sep = \",\", header = TRUE)\n\n# Inspect data\nstr(data_pro)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t4838 obs. of  5 variables:\n $ Reference     : chr  \"overt\" \"overt\" \"overt\" \"overt\" ...\n $ Person        : chr  \"3\" \"3\" \"3\" \"3\" ...\n $ Register      : chr  \"S1A\" \"S1A\" \"S1A\" \"S1A\" ...\n $ Variety       : chr  \"GB\" \"GB\" \"GB\" \"GB\" ...\n $ Referentiality: chr  \"referential\" \"referential\" \"referential\" \"referential\" ...\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(data_pro)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Reference Person Register Variety Referentiality\n1     overt      3      S1A      GB    referential\n2     overt      3      S1A      GB    referential\n3     overt      3      S1A      GB    referential\n4     overt      3      S1A      GB    referential\n5     overt      3      S1A      GB    referential\n6     overt      3      S1A      GB    referential\n```\n\n\n:::\n:::\n\n\n-   **Target variable**:\n\n    -   `Reference` ('overt', 'null')\n\n-   **Explanatory variables**:\n\n    -   `Person` ('1.p.', '2.p', '3.p' as well as the dummy pronouns\n        'it' and 'there')\n\n    -   `Register` (the text category in the International Corpus of\n        English; 'S1A' are informal conversations, whereas 'S1B'\n        comprises formal class lessons)\n\n    -   `Variety` (British English 'GB', Singapore English 'SING' and\n        Hong Kong English 'HK') and\n\n    -   `Referentiality` ('referential' with an identifiable referent or\n        'non-referential' with no/generic reference)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(data_pro)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Reference Person Register Variety Referentiality\n1     overt      3      S1A      GB    referential\n2     overt      3      S1A      GB    referential\n3     overt      3      S1A      GB    referential\n4     overt      3      S1A      GB    referential\n5     overt      3      S1A      GB    referential\n6     overt      3      S1A      GB    referential\n```\n\n\n:::\n\n```{.r .cell-code}\ntable(data_pro$Reference)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n null overt \n  174  4664 \n```\n\n\n:::\n:::\n\n\n### Descriptive overview\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Logistic regression\n\nIn contrast to linear regression, logistic regression models a\n**qualitative response variable** $Y$ with two\noutcomes[^logistic_regression-2]. In the present study, $Y$ is\npronominal `Reference` and has the outcomes `Reference = null` and\n`Reference = overt`, which represent null and overt subjects,\nrespectively. Dichotomous variables of this kind are also often coded as\n`yes`/`no` or `1`/`0`.\n\n[^logistic_regression-2]: Logistic regression can also be used for\n    $\\geq 3$ classes by breaking down the response variable into a\n    series of dichotomous variables. This is also known as **multinomial\n    logistic regression** or **softmax regression.**\n\nAnother difference from linear regression is the **output** of the\nmodel. In linear regression, we obtain a predicted value for the\ncontinuous response variable we're interested in. For instance, if we're\nmodelling reaction times, the model will return an estimated reaction\ntime (given the predictors).\n\nIn logistic regression, however, we either get a **class label** or a\n**probability**. When modelling pronominal reference, the model will\nthus either tell us\n\n1.  whether a speaker would use an overt or a null subject in a given\n    observation (class prediction).\n\n2.  what the probability of using one variant vs. the other would be\n    (probability prediction).\n\nA core component of logistic regression is the **logistic function**.\nThe rationale for using it is that the output of the function will\nalways lie between $0$ and $1$, and it will always denote a\n**probability**.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n### The simple logistic model\n\nAssuming a binary response variable $Y$ with the values 1 and 0 and a\nsingle predictor $X$, we can model the probability $P(Y = 1 \\mid X)$ as\n\n$$\nP(Y = 1 \\mid  X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}.\n$$\n\nThe above expression is equivalent to\n\n$$\n\\log\\left(\\frac{P(Y = 1 \\mid  X)}{1 - P(Y = 1 \\mid  X)}\\right) = \\beta_0 + \\beta_1X.\n$$\n\nThe fraction $\\frac{P(Y = 1 \\mid X)}{1-P(Y = 1 \\mid X)}$ represents the\n**odds**, which stand for to the probability of one outcome (e.g.,\n`Reference = null`) compared to the other (e.g., `Reference = overt`).\n\nTheir logarithmic transformation are the **log odds** (or **logits**) of\none outcome versus the other.\n\n::: {.callout-tip title=\"Understanding log odds\"}\nWhen interpreting the output of a logistic model, note that\n\n-   positive log odds indicate an **increase** in\n    $\\frac{P(Y = 1 \\mid X)}{1-P(Y = 1 \\mid X)}$, whereas\n\n-   negative log odds indicate a **decrease** in\n    $\\frac{P(Y = 1 \\mid X)}{1-P(Y = 1 \\mid X)}$.\n:::\n\nIn more concrete terms: If $X = \\text{Register}$, then our model would\nhave the form:\n\n$$\n\\log\\left(\\frac{P(\\text{Reference} = \\text{null} \\mid \\text{Register})}{1- P(\\text{Reference} = \\text{null} \\mid \\text{Register})}\\right) = \\beta_0 + \\beta_1\\text{Register}\n$$\n\nGiven `Register`, the log odds of a null subject versus an overt one is\nessentially equivalent to the linear model equation. The model\nparameters $\\beta_0$ and $\\beta_1$ are typically estimated using\n**Maximum Likelihood Estimation**[^logistic_regression-3] (MLE).\n\n[^logistic_regression-3]: The goal of the fitting procedure is to\n    maximise the likelihood function of the parameter matrix\n    $\\ell(\\mathbf{\\beta})$. Via partial differentiation, an intermediary\n    result can be attained which can only be solved using an iterative\n    algorithm (e.g. Newton-Ralphson). For further technical details, see\n    Wood [-@woodGeneralizedAdditiveModels2006: 63-66] or Agresti &\n    Kateri [-@agrestiFoundationsStatisticsData2022: 291-294].\n\n### Multiple logistic regression\n\nIf more than one predictor is included, the above equations can be\nexpanded so as to take into account $p$ slopes $\\beta_p$ for $p$\nindependent variables $X_p$.\n\n$$\n\\log\\left(\\frac{P(Y = 1 \\mid X_1 + ... + X_p)}{1 - P(Y = 1 \\mid X_1 + ... + X_p)}\\right) = \\frac{e^{\\beta_0 + \\beta_1X_1 + ... +  \\beta_pX_p}}{1 + e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}.\n$$ Thus, the log odds correspond to the sum of $\\beta_pX_p$,\n\n$$\n\\begin{aligned}\n\\log\\left(\\frac{P(Y = 1 \\mid X_p)}{1 - P(Y = 1 \\mid X_p)}\\right) & = \\beta_0 + \\beta_1X_1 + ... +  \\beta_pX_p \\\\\n& = \\beta_0 + \\sum_{i=1}^{p} \\beta_i X_i\n\\end{aligned}\n$$ respectively.\n\n### Odds ratios\n\nTo assess the strength of an effect, it is instructive to examine the\n**odds ratios** that correspond to the model coefficients. Odds ratios\n(OR) are defined as\n\n$$\nOR(X_1) = e^{\\beta_1}.\n$$\n\n::: {.callout-tip collapse=\"true\" title=\"Understanding odds ratios\"}\nEssentially, the OR describes the ratio between two odds with respect to\nanother independent variable. This is illustrated for `Reference` given\n`Register` below:\n\n$$\n\\text{OR}(\\text{Reference} \\mid \\text{Register}) = \\frac{\\frac{P(\\text{Reference} = \\text{null} \\mid \\text{Register} = \\text{S1A})}{P(\\text{Reference} = \\text{overt} \\mid \\text{Register} = \\text{S1A})}}{\\frac{P(\\text{Reference} = \\text{null} \\mid \\text{Register} = \\text{S1B})}{P(\\text{Reference} = \\text{overt} \\mid \\text{Register} = \\text{S1B})}}\n$$\n\nRead as: '**The ratio between** the probability of a null vs. overt\nobject in S1A **and** the probability of a null vs. overt object in\nS1B'.\n:::\n\n## Workflow in R\n\n### Step 1: Research question and hypotheses\n\nHow do the intra- and extra-linguistic variables suggested in the\nliterature affect subject pronoun realisation (Definite Null\nInstantiation) in British English, Singapore English and Hong Kong\nEnglish?\n\nGiven a significance level $\\alpha = 0.05$, the hypotheses are: $$ \n\\begin{aligned}\nH_0: & \\quad \\text{None of the predictor coefficients deviate from 0}.\\\\\nH_1: & \\quad \\text{At least one predictor coefficient deviates from 0}.\n\\end{aligned}\n$$\n\nThese can be restated mathematically as:\n\n$$ \n\\begin{aligned}\nH_0: & \\quad \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0 \\\\\nH_1: & \\quad \\text{At least one } \\beta_i \\neq 0 \\text{ for } i \\in \\{1, 2, \\ldots, p\\}\n\\end{aligned} $$\n\n### Step 2: Convert to factors and specify reference levels\n\nThe next step involves specifying **reference levels** for all\ncategorical variables. This step is very important because it will\ndirectly impact the parameter estimation and, consequently, influence\nour interpretation of the model output.\n\n-   The reference level of the response is usually chosen such that it\n    corresponds to the **unmarked or most frequent case**. Since overt\n    pronouns are much more common in the data, the reference level of\n    the `Reference` variable will be set to `Reference = overt`. This\n    way, the model coefficients will directly represent the probability\n    of the **null** **subject** variant (i.e., the special case) given\n    certain predictor configurations.\n\n-   The **predictor levels** need to be specified as well. Among other\n    things, we are interested in how the Asian Englishes pattern\n    relative to British English. Therefore, we will define British\n    English as the baseline for comparison.\n\nWe will use the following specifications:\n\n| **Variable**   | **Factor Levels**            | **Preferred Reference level** |\n|------------------|---------------------------|----------------------------|\n| Register       | S1A, S1B                     | S1A                           |\n| Variety        | GB, SING, HK                 | GB                            |\n| Person         | 1, 2, 3, *it*, *there*       | 3                             |\n| Referentiality | referential, non-referential | referential                   |\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Store \"Reference\" as factor\ndata_pro$Reference <- as.factor(data_pro$Reference)\n\n## Specify reference level (the 'unmarked' case)\ndata_pro$Reference <- relevel(data_pro$Reference, \"overt\")\n\n## Print levels\nlevels(data_pro$Reference)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"overt\" \"null\" \n```\n\n\n:::\n:::\n\n\nRepeat the procedure for the remaining categorical variables.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\n# Store \"Register\" as factor\ndata_pro$Register <- as.factor(data_pro$Register)\n\n## Specify reference level\ndata_pro$Register <- relevel(data_pro$Register, \"S1A\")\n\n# Store \"Variety\" as factor\ndata_pro$Variety <- as.factor(data_pro$Variety)\n\n## Specify reference level\ndata_pro$Variety <- relevel(data_pro$Variety, \"GB\")\n\n# Store \"Person\" as factor\ndata_pro$Person <- as.factor(data_pro$Person)\n\n## Specify reference level\ndata_pro$Person <- relevel(data_pro$Person, \"3\")\n\n# Store \"Referentiality\" as factor\ndata_pro$Referentiality <- as.factor(data_pro$Referentiality)\n\n## Specify reference level\ndata_pro$Referentiality <- relevel(data_pro$Referentiality, \"referential\")\n```\n:::\n\n\n### Step 3: Fit the model\n\nThere are two functions that can fit logistic models in R: `lrm()` and\n`glm()`.\n\n::: callout-note\nThe model formula below does not include `Referentiality` because\nseveral intermediary steps revealed it to be almost completely\nirrelevant for predicting `Reference`. In addition, the existing (and\nsignificant) interaction `Variety:Person` has been excluded to improve\nthe interpretability of the model.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# With lrm(); requires library(\"rms\")\n\n# Fit interaction model\nReference.lrm <- lrm(Reference ~ Register + Variety + Register:Variety + Person, data = data_pro)\n\n# View model statistics\nReference.lrm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLogistic Regression Model\n\nlrm(formula = Reference ~ Register + Variety + Register:Variety + \n    Person, data = data_pro)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs          4838    LR chi2     120.43      R2       0.092    C       0.729    \n overt       4664    d.f.             9     R2(9,4838)0.023    Dxy     0.458    \n null         174    Pr(> chi2) <0.0001    R2(9,503.2)0.199    gamma   0.488    \nmax |deriv| 4e-10                            Brier    0.034    tau-a   0.032    \n\n                            Coef    S.E.   Wald Z Pr(>|Z|)\nIntercept                   -3.4132 0.2746 -12.43 <0.0001 \nRegister=S1B                 0.0269 0.3807   0.07 0.9437  \nVariety=HK                   0.6712 0.3174   2.11 0.0345  \nVariety=SING                 1.1193 0.2959   3.78 0.0002  \nPerson=1                    -0.8807 0.1811  -4.86 <0.0001 \nPerson=2                    -1.6441 0.2695  -6.10 <0.0001 \nPerson=it                    0.7897 0.2978   2.65 0.0080  \nPerson=there                -2.5641 1.0095  -2.54 0.0111  \nRegister=S1B * Variety=HK    0.6035 0.4521   1.34 0.1819  \nRegister=S1B * Variety=SING -0.4753 0.4688  -1.01 0.3107  \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# With (glm); available in base R\n# Note the additional \"family\" argument!\nReference.glm <- glm(Reference ~ Register + Variety + Register:Variety + Person, data = data_pro, family = \"binomial\")\n\n# View model statistics\nsummary(Reference.glm)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntab_model(Reference.glm, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, show.fstat = TRUE, transform = NULL)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"4\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">Reference</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Log-Odds</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">std. Error</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;3.41</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.27</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;3.99&nbsp;&ndash;&nbsp;-2.91</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Register [S1B]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.03</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.38</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.74&nbsp;&ndash;&nbsp;0.77</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.944</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Variety [HK]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.67</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.32</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.06&nbsp;&ndash;&nbsp;1.32</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>0.034</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Variety [SING]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.12</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.30</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.56&nbsp;&ndash;&nbsp;1.73</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Person [1]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.88</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.18</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;1.24&nbsp;&ndash;&nbsp;-0.53</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Person [2]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;1.64</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.27</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;2.21&nbsp;&ndash;&nbsp;-1.14</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Person [it]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.79</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.30</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.18&nbsp;&ndash;&nbsp;1.35</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>0.008</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Person [there]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;2.56</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.01</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;5.44&nbsp;&ndash;&nbsp;-1.05</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>0.011</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Register [S1B] × Variety<br>[HK]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.60</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.45</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.28&nbsp;&ndash;&nbsp;1.50</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.182</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Register [S1B] × Variety<br>[SING]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.48</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.47</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;1.40&nbsp;&ndash;&nbsp;0.45</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.311</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"4\">4838</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> Tjur</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"4\">0.030</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">Deviance</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"4\">1378.406</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">AIC</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"4\">1398.406</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n::: callout-tip\n## Stepwise variable selection\n\nWith the function `drop1()`, it is possible to successively remove\nvariables from the complex model to ascertain which ones improve the\nmodel significantly (i.e., decrease the deviance and AIC scores).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(Reference.glm, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nReference ~ Register + Variety + Register:Variety + Person\n                 Df Deviance    AIC    LRT Pr(>Chi)    \n<none>                1378.4 1398.4                    \nPerson            4   1460.2 1472.2 81.828  < 2e-16 ***\nRegister:Variety  2   1387.5 1403.5  9.100  0.01057 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\n### Step 4: Confidence intervals and odds ratios\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tidy the model output\ntidy_model <- tidy(Reference.glm, conf.int = TRUE)\n\n# Remove intercept, compute odds ratios and their CIs\ntidy_model <- tidy_model %>% \n  filter(term != \"(Intercept)\") %>% \n  mutate(\n    odds_ratio = exp(estimate),\n    odds.conf.low = exp(conf.low),\n    odds.conf.high = exp(conf.high)\n  )\n```\n:::\n\n\n### Step 5: Visualise the model\n\nPlot model coefficients:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\n# Create the coefficient plot\nggplot(tidy_model, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient Estimate (log-odds)\",\n    y = \"Predictor\",\n    title = \"Coefficient Estimates with Confidence Intervals\",\n    caption = \"*Note that the CIs of singificant predictors do not include 0.\"\n  )\n```\n\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\n# Plot odds ratios\nggplot(tidy_model, aes(x = exp(estimate), y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = odds.conf.low, xmax = odds.conf.high), height = 0.2) +\n  geom_vline(xintercept = 1, linetype = \"dashed\", color = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient Estimate (odds ratios)\",\n    y = \"Predictor\",\n    title = \"Odds ratios with Confidence Intervals\",\n    caption = \"*Note that the CIs of singificant predictors do not include 1.\"\n  )\n```\n\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-12-2.png){width=672}\n:::\n:::\n\n\n\n\nPlot predicted probabilities:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\n# Use ggeffect() from the ggeffects package\nplot(ggeffect(Reference.glm, terms = c(\"Register\"))) + geom_line(col = \"steelblue\")\n```\n\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\nplot(ggeffect(Reference.glm, terms = c(\"Variety\"))) + geom_line(col = \"steelblue\")\n```\n\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\nplot(ggeffect(Reference.glm, terms = c(\"Person\"))) + geom_line(col = \"steelblue\")\n```\n\n::: {.cell-output-display}\n![](Logistic_regression_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n:::\n\n\n### Step 6: Interpret the model\n\nThe logistic regression model is statistically significant at\n$p < 0.001$ ($\\chi^2 = 120.43$, $df = 9$) and has acceptable fit\n(Nagelkerke's-$R^2$ = $0.09$, $C = 0.73$).\n\nThe model coefficients indicate that null subjects are significantly\nmore likely in Singapore English compared to British English (Estimate =\n1.12, 95% CI \\[0.56, 1.73\\], $p < 0.001$). This effect is moderate with\nan $OR$ of 3.06 (95% CI \\[1.75, 5.64\\]), suggesting that the probability\nof subject omission is elevated by a factor of approximately 3 in the\nSingaporean variety.\n\n...\n\n### Step 7: Further model diagnostics\n\n-   **Cross-validation**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\n# Set seed for reproducibility\nset.seed(123)\n\n# Refit the model with additional settings\nReference.val <- lrm(Reference ~ Register + Variety + Register:Variety + Person, data = data_pro, x = T, y = T)\n\n# Perform 200-fold cross-validation\nmodel.validated <- validate(Reference.val, B = 200)\n\n# Slope optimism should be as low possible!\nmodel.validated\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          index.orig training    test optimism index.corrected   n\nDxy           0.4592   0.4655  0.4456   0.0200          0.4393 200\nR2            0.0923   0.0981  0.0843   0.0138          0.0785 200\nIntercept     0.0000   0.0000 -0.2177   0.2177         -0.2177 200\nSlope         1.0000   1.0000  0.9262   0.0738          0.9262 200\nEmax          0.0000   0.0000  0.0622   0.0622          0.0622 200\nD             0.0247   0.0263  0.0225   0.0038          0.0208 200\nU            -0.0004  -0.0004  0.0002  -0.0006          0.0002 200\nQ             0.0251   0.0268  0.0223   0.0045          0.0206 200\nB             0.0336   0.0336  0.0337  -0.0001          0.0337 200\ng             1.0081   1.2487  1.1361   0.1127          0.8954 200\ngp            0.0319   0.0326  0.0303   0.0022          0.0297 200\n```\n\n\n:::\n:::\n\n\n-   **Multicollinearity**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code\"}\n# Variable inflation factors further reveal severe multicollinearity\nvif(Reference.lrm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Register=S1B                  Variety=HK \n                   5.818111                    4.006084 \n               Variety=SING                    Person=1 \n                   3.421687                    1.140407 \n                   Person=2                   Person=it \n                   1.089502                    1.102908 \n               Person=there   Register=S1B * Variety=HK \n                   1.007148                    6.218803 \nRegister=S1B * Variety=SING \n                   3.685075 \n```\n\n\n:::\n:::\n",
    "supporting": [
      "Logistic_regression_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}