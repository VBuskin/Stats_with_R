{
  "hash": "1b6deb080335a1fc34dafed1aa2d733d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Gradient boosting\"\nauthor:\n  name: \"Vladimir Buskin\" \n  orcid: \"0009-0005-5824-1012\"\n  affiliation: \n    name: \"Catholic University of EichstÃ¤tt-Ingolstadt\"\n    department: \"English Language and Linguistics\"\nabstract: > \n  Gradient boosting constitutes a powerful extension of tree-based methods and is generally appreciated for its high predictive performance. Nevertheless, this family of methods, which includes implementations such as AdaBoost, XGBoost, and CatBoost, among many others, is not yet established in corpus-linguistic statistics. A practical scenario is presented to introduce the core ideas of gradient boosting, demonstrate its application to linguistic data as well as point out its advantages and drawbacks.\nkeywords: \"Machine learning, gradient descent, loss function, regularization\"\nformat:\n  html:\n    self-contained: true\n    theme: default\n    toc: true\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 3\n    scrollable: true\neditor: visual\nbibliography: R.bib\n---\n\n\n## Recommended reading\n\n> @james_introduction_2021: Chapter 8.2\n>\n> @hastie2017: Chapters 9 & 10\n\n## Preparation\n\n## Boosting\n\nThe core idea of **boosting** is as simple as it is intuitive: By\naggregating the insights of multiple weak models, a much more powerful\ncomplex model can be formed. The new model ensemble is generally\nsuperior in terms of predictive performance. While there are various\ncomputational implementations of boosting, we will restrict our scope to\ndecision trees as introduced in the [previous\nunit](Decision_trees_and_random_forests.qmd).\n\n### Trees revisited\n\nA single tree $T$ splits up the feature space into prediction regions\n$R_j$ for $j = 1, \\dots, J$. Each observation $x$ from the training data\nis assigned to a prediction region $R_j$ and receives a constant value\n$\\gamma_j$. The final prediction $\\hat{y}$ is a function of the\nconstants returned from all regions:\n\n$$\n\\hat{y} = f(x) = \\sum_{j= 1}^{J}\\gamma_jI(x \\in R_j)\n$$ {#eq-tree}\n\nThus, the tree $T$ is determined by the input $x$ and the parameter\nspace $\\Theta = \\{R_j, \\gamma_j\\}_{1}^{J}$ which contains said regions\nand constant terms. A tree ensemble can the be described as the sum of\n$M$ trees $T(x; \\Theta)$, as summarised in @eq-ensemble.\n\n$$\nf_M(x) = \\sum_{m=1}^{M}T(x;\\Theta_m)\n$$ {#eq-ensemble}\n\nIn contrast to random forests, where the goal is to attain maximum\nvariance among trees, boosted trees actually learn from each other. Each\nsubsequent tree should improve on the errors made by the current one. It\ndoes so by finding the parameter values that minimise the **loss\nfunction** $L(f)$.[^gradient_boosting-1]\n\n[^gradient_boosting-1]: There is no analytical solution to this which\n    can be computed via a single formula. Instead, it poses an\n    optimisation problem that requires specialised numerical techniques.\n    In the end, these can only provide approximate solutions.\n\n### Loss functions and gradient descent\n\nGiven $N$ observations, let $y_i$ represent the labels of the response\nvariable and $f(x_i)$ the prediction function for a data point $x_i$.\nThere are numerous metrics that can be used to quantify the \"loss\"\n$L(y_i, f(x_i))$, such as squared errors for regression or deviance for\nclassification. The full loss function $L(f)$ is given by\n@eq-boost-loss.\n\n$$\nL(f) = \\sum_{i=1}^N L(y_i, f(x_i))\n$$ {#eq-boost-loss}\n\nThe multi-dimensional space can be conceptualised as a mountain range\nwhere we are looking for paths that lead us back to level ground. The\nloss function quantifies the amount of energy we have to expend during\nthis endeavor. For this reason, it would be a good idea to keep track of\nall the paths we could take. As fearless as we are, we want to opt for\nthe steepest paths that lead us back down as quickly as possible.\n\nMathematically, this can be done via the **gradient** $\\mathbf{g}_m$,\nwhich is a vector of partial derivatives. It captures how a function\nwith multiple variables changes in different directions. Essentially, it\nis akin to a map that indicates all paths and how steep they are. One\npossible path, a gradient component $g_{mi}$, would have the general\nform in @eq-boost-gradient.\n\n$$\ng_{mi} = \\frac{\\partial L(y, f(x_i))}{\\partial f(x_i)}\n$$ {#eq-boost-gradient}\n\nThe steepest path, and thus the most rapid decrease in the loss\nfunction, is the **negative gradient** $-\\mathbf{g}_m$, determining the\npredictions of the next boosted tree. It is, therefore, no surprise that\nthis procedure is also known as **gradient descent**.\n\n## Implementation in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(gbm)\nlibrary(xgboost)\n\n# Load data\n```\n:::\n\n\n\n::: callout-warning\nThis page is still under construction. More content will be added soon!\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}