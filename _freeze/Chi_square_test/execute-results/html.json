{
  "hash": "c8cb7b8001c5fd0de2561538e5259eb9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chi-squared test\"\nauthor: Vladimir Buskin\nabstract: > \n  The chi-squared ($\\chi^2$) test helps determine if there is a statistically significant association between two categorical variables. It compares the observed frequencies of categories with those expected under the null hypothesis. The $\\chi^2$ (chi-squared) score quantifies the difference between observed and expected frequencies for every cell in a contingency table. The greater the difference between observed and expected, the higher the $\\chi^2$ score and the lower the $p$-value, given the degrees of freedom. It is recommended to compute effect size measures and inspect the residuals to assess the nature of the association.\nformat:\n  html:\n    self-contained: true\n    code-fold: false\n    theme: default\n    toc: true\n    toc-depth: 4\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 3\n    scrollable: true\neditor: visual\nbibliography: R.bib\nmetadata-files:\n  - _quarto.yml\ngoogle-scholar: false\n---\n\n\n## Suggested reading\n\nFor linguists:\n\n> @gries_statistics_2021: Chapter 4.1.2.1\n\nGeneral:\n\n> @baguleySeriousStatsGuide2012: Chapter 4\n>\n> @agrestiFoundationsStatisticsData2022: Chapter 5\n\n## Preparation\n\n::: callout-tip\n## Script\n\nYou can find the full R script associated with this unit [here](...).\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#  Load libraries\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(confintr) # for effect size calculation\n\n# Load data\ncl.order <- read_xlsx(\"Paquot_Larsson_2020_data.xlsx\")\n```\n:::\n\n\n## The $\\chi^2$-test\n\nRecall our statistical hypotheses stated in the previous unit:\n\n-   $H_0:$ The variables `ORDER` and `SUBORDTYPE` are independent.\n\n-   $H_1:$ The variables `ORDER` and `SUBORDTYPE` are **not**\n    independent.\n\nThe next step is to compute a test statistic that indicates how strongly\nour data conforms to $H_0$, such as Pearson's $\\chi^2$. In order to\ncompute it, we need two types of values: the **observed frequencies**\n$n_{ij}$ in our data set and the **expected frequencies** $m_{ij}$,\nwhich we would expect to see if $H_0$ were true, with\n$n, m \\in \\mathbb{N}$. The indices $i$ and $j$ uniquely identify the\nfrequencies found in all column-row combinations of a contingency table.\n\n::: {.callout-note collapse=\"true\" title=\"The structure of a contingency table\"}\nThe table below represents a generic contingency table where $X$ and $Y$\nare categorical variables. Each $x_i$ represents a category of $X$ and\neach $y_j$ represents a category of $Y$. In the table, each cell\nindicates the count of observation $n_{ij}$ corresponding to the $i$-th\nrow and $j$-th column.\n\n|     |       |          | $Y$      |     |          |     |\n|-----|-------|----------|----------|-----|----------|-----|\n|     |       | $y_1$    | $y_2$    | ... | $y_J$    |     |\n|     | $x_1$ | $n_{11}$ | $n_{12}$ | ... | $n_{1J}$ |     |\n|     | $x_2$ | $n_{21}$ | $n_{22}$ | ... | $n_{2J}$ |     |\n| $X$ | ...   | ...      | ...      | ... | ...      |     |\n|     | $x_I$ | $n_{I1}$ | $n_{I2}$ | ... | $n_{3J}$ |     |\n:::\n\nIn the `cl.order` data, the observed frequencies correspond to how often\neach `ORDER` value (i.e., `mc-sc` and `sc-mc`) is attested for a given\n`SUBORDTYPE` (i.e., `temp` and `caus`). This can be done in a very\nstraightforward fashion using R's `table()` function on the variables of\ninterest.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobserved_freqs <- table(cl.order$ORDER, cl.order$SUBORDTYPE)\n\nprint(observed_freqs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n```\n\n\n:::\n:::\n\n\nThe expected frequencies require a few additional steps. Usually, these\nsteps are performed automatically when conducting the chi-squared test\nin R, so you don't have to worry about calculating them by hand. We will\ndo it anyway to drive home the rationale of the test.\n\nThe expected frequencies $m_{ij}$ are given by the formula in\n@eq-exp-freq. In concrete terms, we go through each cell in the\ncross-table and multiply the corresponding row sums with the column\nsums, dividing the result by the total number of occurrences in the\nsample. For example, there are $184$ occurrences of mc-sc clause orders\nwhere the subordinate clause is causal. The row sum is $184 + 91 = 275$\nand the column sum is $184 + 15 = 199$. Next, we take their product\n$275 \\times 199$ and divide it by the total number of observations,\nwhich is $184 + 91 + 15 + 113 = 403$. Thus we obtain an expected\nfrequency of $\\frac{275 \\times 199}{403} = 135.79$ under the null\nhypothesis.\n\n$$\nm_{ij} = \\frac{i\\textrm{th row sum} \\times j \\textrm{th column sum}}{\\textrm{number of observations}}.\n$$ {#eq-exp-freq}\n\nThe expected frequencies for our combination of variables is shown\nbelow. In which cells can you see the greatest deviations between\nobserved and expected frequencies?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n## Calculate row totals\nrow_totals <- rowSums(observed_freqs)\n\n## Calculate column totals\ncol_totals <- colSums(observed_freqs)\n\n## Total number of observations\ntotal_obs <- sum(observed_freqs)\n\n## Calculate expected frequencies\nexpected_freqs <- outer(row_totals, col_totals) / total_obs\n\nprint(expected_freqs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           caus      temp\nmc-sc 135.79404 139.20596\nsc-mc  63.20596  64.79404\n```\n\n\n:::\n:::\n\n\nThe $\\chi^2$-test now offers a convenient way of quantifying the\ndifferences between the two tables above. It measures how much the\nobserved frequencies **deviate** from the expected frequencies **for\neach cell** in a contingency table [cf. @heumann_introduction_2022:\n249-251]. The gist of this procedure is summarised in @eq-chisq-simple.\n\n$$\n\\text{Chi-squared } \\chi^2 =\\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\n$$ {#eq-chisq-simple}\n\n::: {.callout-note collapse=\"true\" title=\"Formal definition of the chi-squared test\"}\nGiven $n$ observations and $k$ degrees of freedom $df$, the joint\ndeviations between $n_{ij}$ $m_{ij}$ contribute to the final\n$\\chi^2$-score, which is defined as\n\n$$\n\\chi^2 = \\sum_{i=1}^{I}\\sum_{i=j}^{J}{\\frac{(n_{ij} - m_{ij})^2}{m_{ij}}}\n$$ {#eq-chisq}\n\nfor $i = 1, ..., I$ and $j = 1, ..., J$ and\n$df = (\\textrm{number of rows} -1) \\times (\\textrm{number of columns} - 1)$.\n:::\n\nThe implementation in R is a simple one-liner. Keep in mind that we have\nto supply absolute frequencies to `chisq.test()` rather than\npercentages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreqs_test <- chisq.test(observed_freqs)\n\nprint(freqs_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  observed_freqs\nX-squared = 104.24, df = 1, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\nQuite conveniently, the test object `freqs_test` stores the expected\nfrequencies, which can be easily accessed via subsetting. Luckily, they\nare identical to what we calculated above!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreqs_test$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n             caus      temp\n  mc-sc 135.79404 139.20596\n  sc-mc  63.20596  64.79404\n```\n\n\n:::\n:::\n\n\n::: callout-important\n### Assumptions of the chi-squared test\n\nThis $\\chi^2$-test comes with certain **statistical assumptions**.\nViolations of these assumptions decrease the validity of the result and\ncould, therefore, lead to wrong conclusions about relationships in the\ndata. In this case, other tests should be consulted.\n\n1.  All observations are independent of each other.\n2.  80% of the expected frequencies are $\\geq$ 5.\n3.  All observed frequencies are $\\geq$ 1.\n\nIf assumptions 2 and 3 are violated, it is recommended to use a more\nrobust test such as the Fisher's Exact Test (see `?fisher.test()` for\ndetails) or the log-likelihood test ($G$-test).\n\nIn case of dependent observations (e.g., multiple measurements per\nparticipant), the default approach is to fit a multilevel model that can\ncontrol for grouping factors (see mixed-effects regression in @sec-mer.)\n:::\n\n### How do I make sense of the test results?\n\nThe test output has three 'ingredients':\n\n-   the **chi-squared score** (X-squared)\n-   the **degrees of freedom** (df)\n-   the **p-value**.\n\nIt is absolutely essential to report all three of those as they\ndetermine each other. Here is an example:\n\n> According to a $\\chi^2$-test, there is a highly significant\n> association between clause ORDER and SUBORDTYPE at $p < 0.001$\n> ($\\chi^2 = 106.44, df = 1$), thus justifying the rejection of $H_0$.\n\nThe test results provide evidence that the dependent variable `ORDER`\nand the explanatory variable `SUBORDTYPE` are not independent of each\nother. The probability of **randomly** observing usage patterns such as\nthose found in the `cl.order` data is lower than 0.001 $\\approx$ 0.1%,\nwhich is enough to reject the null hypothesis at $\\alpha = 0.05$.\n\nWe can infer that a speaker's choice of clause `ORDER` is very likely\ninfluenced by the semantic type of subordinate clause; in other words,\nthese two variables are **correlated**. However, there are still several\nthings the test does **not** tell us:\n\n-   Are there certain variable combinations where the $\\chi^2$-scores\n    are particularly high?\n\n-   How strongly do `ORDER` and `SUBORDTYPE` influence each other?\n\n-   Does a causal subordinate clause make the `mc-sc` clause order more\n    likely?\n\n## Pearson residuals\n\nIf we're interested in what cells show the greatest difference between\nobserved and expected frequencies, an option would be to inspect the\nPearson residuals (cf. @eq-pear-res).\n\n$$ \\text{residuals} = \\frac{\\text{observed} - \\text{expected}}{\\sqrt{\\text{expected}}} \n$$ {#eq-pear-res}\n\nThese can be accessed via the test results stored `freqs_test`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreqs_test$residuals\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n             caus      temp\n  mc-sc  4.136760 -4.085750\n  sc-mc -6.063476  5.988708\n```\n\n\n:::\n:::\n\n\nThe function `assocplot()` can automatically compute the pearson\nresiduals for any given contingency table and create a plot that\nhighlights their contributions. If the bar is above the dashed line, it\nis black and indicates that a category is observed more frequently than\nexpected (e.g., causal subordinate clauses in the `mc-sc` order).\nConversely, bars are coloured grey if a category is considerably less\nfrequent than expected, such as `caus` in `sc-mc`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nassocplot(t(observed_freqs), col = c(\"black\", \"lightgrey\"))\n```\n\n::: {.cell-output-display}\n![](Chi_square_test_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n::: {.callout-tip collapse=\"true\" title=\"Testing the residuals: Configural Frequency Analysis\"}\nThe chi-squared test only provides a $p$-value for the entire\ncontingency table. But what if we wanted to test the residuals for their\nsignificance as well? Configural Frequency Analysis\n[@krauth_lienert_kfa] allows us to do exactly that: It performs a\nsignificance test for **all combinations of variable values** in a\ncross-table. Moreover, CFA is not limited to two variables only.\nTechnically, users can test for associations between arbitrary numbers\nof variables, but should be aware of the increasing complexity of\ninterpretation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cfa) # install library beforehand\n\n# Get the observed counts and convert them to a data frame\nconfig_df <- as.data.frame(observed_freqs)\n\n# Convert to matrix\nconfigs <- as.matrix(config_df[, 1:2])  # First two columns contain the configurations\ncounts <- config_df$Freq                # The Freq column contains the counts\n\n# Perform CFA\ncfa_output <- cfa(configs, counts, bonferroni = TRUE)\n\n# Print output\nprint(cfa_output)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n*** Analysis of configuration frequencies (CFA) ***\n\n       label   n  expected         Q    chisq      p.chisq sig.chisq         z\n1 sc-mc caus  15  63.20596 0.1418682 36.76575 1.332103e-09      TRUE -6.671872\n2 sc-mc temp 113  64.79404 0.1425343 35.86463 2.115143e-09      TRUE  6.469444\n3 mc-sc caus 184 135.79404 0.1804075 17.11278 3.522441e-05      TRUE  5.027611\n4 mc-sc temp  91 139.20596 0.1827409 16.69335 4.393467e-05      TRUE -5.102385\n           p.z sig.z\n1 1.000000e+00  TRUE\n2 4.918210e-11  TRUE\n3 2.483137e-07  TRUE\n4 9.999998e-01  TRUE\n\n\nSummary statistics:\n\nTotal Chi squared         =  106.4365 \nTotal degrees of freedom  =  1 \np                         =  0 \nSum of counts             =  403 \n\nLevels:\n\nVar1 Var2 \n   2    2 \n```\n\n\n:::\n:::\n\n:::\n\n## Effect size\n\nThe $p$-value only indicates the **presence of correlation**, but not\nits strength – regardless of how low it may be. It does not convey how\nmuch two variables determine each other. To this end, it is recommended\nto report an **effect size measure** alongside the $p$-value. One such\nmeasure is Cramér's $V$, which takes values in the interval $[0, 1]$:\n\n$$\nV = \\sqrt{\\frac{\\chi^2}{n \\times (min(nrow, ncol) - 1)}}.\n$$ {#eq-effsize-v}\n\nThe package `confintr` implements this in its `cramersv()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncramersv(freqs_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5085863\n```\n\n\n:::\n:::\n\n\nThe association between two categorical variables is stronger, the closer $V$ approximates 1. Conversely, if $V = 0$, then the variables are completely independent. There are various guidelines in the literature that provide thresholds for \"small\", \"moderate\" and \"large\" effects, yet these are rarely justified on theoretical grounds and could be viewed as arbitrary.\n\n## Likelihood-based inference\n\nSince causal subordinate clauses typically follow the main clause in the\nsample, it may seem tempting to draw conclusions about the\n**probability** of a certain outcome of clause `ORDER`. Unfortunately,\nthe $\\chi^2$-test is **not designed** for this kind of statistical\ninference. An approach that would actually allow the estimation of\nprobabilities is [Logistic regression](Logistic_regression.qmd).\n\n::: {.callout-note collapse=\"true\" title=\"What would that look like?\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert to factors and define reference levels\ncl.order$ORDER <- factor(cl.order$ORDER, levels = c(\"sc-mc\", \"mc-sc\"))\ncl.order$SUBORDTYPE <- factor(cl.order$SUBORDTYPE, levels = c(\"temp\", \"caus\")) \n\n# Fit logistic regression model\norder.glm1 <- glm(ORDER ~ SUBORDTYPE, data = cl.order, family = \"binomial\")\n\n# Store model parameters\nintercept <- order.glm1$coefficients[1]\nslope <- order.glm1$coefficients[2]\n\n# Convert to probabilities\nunname(plogis(intercept + slope * 0)) # if temporal\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4460784\n```\n\n\n:::\n\n```{.r .cell-code}\nunname(plogis(intercept + slope * 1)) # if causal\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9246231\n```\n\n\n:::\n:::\n\n\nIf the subordinate clause is causal, there is a 92.5% chance that\nspeakers will use the `mc-sc` ordering, but only a 44.6% chance of using\n`sc-mc`.\n:::\n",
    "supporting": [
      "Chi_square_test_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}