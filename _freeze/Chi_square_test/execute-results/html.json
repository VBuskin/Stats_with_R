{
  "hash": "47737b3eeac908af58678e6fc4959165",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chi square test\"\nauthor: Vladimir Buskin\nformat:\n  html:\n    self-contained: true\n    code-fold: false\n    theme: default\n    toc: true\n    toc-depth: 4\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 3\n    scrollable: true\neditor: visual\nbibliography: R.bib\n---\n\n\n## Preparation\n\n\n\n\n\n-   Load packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\n```\n:::\n\n\n-   Load the data sets:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncl.order <- read_xlsx(\"Paquot_Larsson_2020_data.xlsx\")\n```\n:::\n\n\n## The $\\chi^2$-test\n\n\n::: {.callout-note collapse=\"false\" title=\"This unit in a nutshell\"}\n\nThe chi-square test helps determine if there is a statistically significant association between two categorical variables. It compares observed frequencies (actual data) with expected frequencies (if the null hypothesis were true). The greater the difference, the higher the $\\chi^2$ score, providing evidence against $H_0$\n\n-   $p$-value: Indicates significance level.\n-   $\\chi^2$ score: Shows the deviation between observed and expected frequencies.\n-   Degrees of freedom: Affects the shape of the chi-square distribution.\n\nIf the $p$-value is below 0.05, we reject $H_0$, indicating a significant association between the variables.\n\nExample:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Hypotheses:\n# H0: ORDER and SUBORDTYPE are independent.\n# H1: ORDER and SUBORDTYPE are not independent.\n\nchisq.test(cl.order$ORDER, cl.order$SUBORDTYPE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  cl.order$ORDER and cl.order$SUBORDTYPE\nX-squared = 104.24, df = 1, p-value < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# We can reject H0 and accept H1.\n```\n:::\n\n\n:::\n\nWe calculate the Pearson $\\chi^2$ statistic, requiring the observed frequencies ($n_{ij}$) from the data set and the expected frequencies ($m_{ij}$), assuming $H_0$ is true.\n\nThe table below represents a generic contingency table where $X$ and $Y$ are\ncategorical variables. Each $x_i$ represents a category of $X$ and each\n$y_j$ represents a category of $Y$. In the table, each cell indicates\nthe count of observation $n_{ij}$ corresponding to the $i$-th row and\n$j$-th column.\n\n|     |       |          | $Y$      |     |          |     |\n|-----|-------|----------|----------|-----|----------|-----|\n|     |       | $y_1$    | $y_2$    | ... | $y_J$    |     |\n|     | $x_1$ | $n_{11}$ | $n_{12}$ | ... | $n_{1J}$ |     |\n|     | $x_2$ | $n_{21}$ | $n_{22}$ | ... | $n_{2J}$ |     |\n| $X$ | ...   | ...      | ...      | ... | ...      |     |\n|     | $x_I$ | $n_{I1}$ | $n_{I2}$ | ... | $n_{3J}$ |     |\n\nThis is really just an abstraction of the cross-tables we've computed so many times before. These are our observed frequencies:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n```\n\n\n:::\n:::\n\n\nThe expected frequencies are calculated as:\n\n$$\nm_{ij} = \\frac{i\\textrm{th row sum} \\times j \\textrm{th column sum}}{\\textrm{number of observations}}.\n$$\nThe expected frequencies for our combination of variables is shown below. In which cells can you see the greatest deviations between observed and expected frequencies?\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n           caus      temp\nmc-sc 135.79404 139.20596\nsc-mc  63.20596  64.79404\n```\n\n\n:::\n:::\n\n\nThe $\\chi^2$-test now offers a convenient way of quantifying the differences between the two tables above. Given $n$ observations and $k$ degrees of freedom ($df$)[^chi_square_test-1], the $\\chi^2$-statistic measures how much the observed frequencies **deviate** from the expected frequencies **for each cell** in a contingency table [cf. @heumann_introduction_2022:\n249-251]. The joint deviations make up the final $\\chi^2$-score, which is defined as\n\n[^chi_square_test-1]: The degrees of freedom are calculated by\n    $(\\textrm{number of rows} -1) \\times (\\textrm{number of columns} - 1)$.\n\n$$\n\\chi^2 = \\sum_{i=1}^{I}\\sum_{i=j}^{J}{\\frac{(n_{ij} - m_{ij})^2}{m_{ij}}}.\n$$\n\nNote, however, that this test comes with certain assumptions:\n\n1.  All observations are independent of each other.\n2.  80% of the expected frequencies are $\\geq$ 5.\n3.  All observed frequencies are $\\geq$ 1.\n\nIn R, conducting this test is a simple one-liner:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreqs_test <- chisq.test(freqs)\n\nprint(freqs_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  freqs\nX-squared = 104.24, df = 1, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe test object `freqs_test` also stores the expected frequencies, which we can also access. They appear to be all right:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Expected frequencies\nfreqs_test$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n             caus      temp\n  mc-sc 135.79404 139.20596\n  sc-mc  63.20596  64.79404\n```\n\n\n:::\n:::\n\n\n::: callout-important\nIf the data does not meet the (expected) frequency requirements for the\n$\\chi^2$-test, the Fisher's Exact Test is a viable alternative (see\n`?fisher.test()` for details).\n:::\n\n## Workflow in R\n\n### $\\chi^2$-test\n\n#### Define hypotheses\n\n-   $H_0:$ The variables `ORDER` and `SUBORDTYPE` are independent.\n\n-   $H_1:$ The variables `ORDER` and `SUBORDTYPE` are **not**\n    independent.\n\n#### Running the test\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cross-tabulate the frequencies for the variables of interest\n\nfreqs <- table(cl.order$ORDER, cl.order$SUBORDTYPE)\n\nfreqs ## Assumption met: all observed freqs => 1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n```\n\n\n:::\n\n```{.r .cell-code}\n# Run a chis-quared test on the absolute frequencies and print the results\n\ntest <- chisq.test(freqs, correct = FALSE)\n\n# Inspect expected frequencies\n\ntest$expected # Assumption met: all expected frequences => 5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n             caus      temp\n  mc-sc 135.79404 139.20596\n  sc-mc  63.20596  64.79404\n```\n\n\n:::\n:::\n\n\n#### Optional: Effect size\n\nThe sample-size independent effect size measure **Cramer's *V***\n($\\phi$) is defined as\n\n$$V = \\sqrt{\\frac{\\chi^2}{N \\times df}}.$$ The outcome varies between\n$0$ (= no correlation) and $1$ (= perfect correlation); cf. also Gries\n[-@gries_statistics_2013: 186].\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Compute Cramer's V\n\n## By hand:\n\n# Given chi-squared statistic\nchi_squared <- unname(test$statistic)\n\n# Total number of observations\ntotal_obs <- sum(freqs)\n\nsqrt(chi_squared / total_obs * (min(dim(freqs)) - 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5139168\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n## Automatically:\nlibrary(\"confintr\") # Load library\n\ncramersv(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5139168\n```\n\n\n:::\n:::\n\n\n#### Reporting the results\n\nAccording to a $\\chi^2$-test, there is a significant association between\nclause `ORDER`and `SUBORDTYPE` at $p < 0.001$\n($\\chi^2 = 106.44, df = 1$), thus justifying the rejection of $H_0$.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}