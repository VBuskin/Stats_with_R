{
  "hash": "74997fa6e17ea698476540ed50ccb142",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chi-squared test\"\nauthor: Vladimir Buskin\nabstract: > \n  The chi-squared ($\\chi^2$) test helps determine if there is a statistically significant association between two categorical variables. It compares the observed frequencies of categories with those expected under the null hypothesis. The $\\chi^2$ (chi-squared) score quantifies the difference between observed and expected frequencies for every cell in a contingency table. The greater the difference between observed and expected, the higher the $\\chi^2$ score and the lower the $p$-value, given the degrees of freedom. It is recommended to compute effect size measures and inspect the residuals to assess the nature of the association.\nformat:\n  html:\n    self-contained: true\n    code-fold: false\n    theme: default\n    toc: true\n    toc-depth: 4\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 3\n    scrollable: true\neditor: visual\nbibliography: R.bib\nmetadata-files:\n  - _quarto.yml\ngoogle-scholar: false\n---\n\n\n## Preparation\n\n::: callout-tip\n## Script\n\nYou can find the full R script associated with this unit\n[here](...).\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#  Load libraries\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(confintr) # for effect size calculation\n\n# Load data\ncl.order <- read_xlsx(\"Paquot_Larsson_2020_data.xlsx\")\n```\n:::\n\n\n## The $\\chi^2$-test\n\nRecall our statistical hypotheses stated in the previous unit:\n\n-   $H_0:$ The variables `ORDER` and `SUBORDTYPE` are independent.\n\n-   $H_1:$ The variables `ORDER` and `SUBORDTYPE` are **not**\n    independent.\n\nThe next step is to compute a test statistic that indicates how strongly our data\nconforms to $H_0$. For instance, the Pearson $\\chi^2$ statistic is\nsuitable for categorical data.\n\nIt requires two types of values: the **observed frequencies** $n_{ij}$ in our data set and the **expected frequencies** $m_{ij}$, which we would expect to see if $H_0$ were true, with $n, m \\in \\mathbb{N}$. The indices $i$ and $j$ uniquely identify the frequencies found in\nall column-row combinations of a cross-table.\n\n::: {.callout-note collapse=\"false\" title=\"Mathematical details: The structure of a contingency table\"}\nThe table below represents a generic contingency table where $X$ and $Y$\nare categorical variables. Each $x_i$ represents a category of $X$ and\neach $y_j$ represents a category of $Y$. In the table, each cell\nindicates the count of observation $n_{ij}$ corresponding to the $i$-th\nrow and $j$-th column.\n\n|     |       |          | $Y$      |     |          |     |\n|-----|-------|----------|----------|-----|----------|-----|\n|     |       | $y_1$    | $y_2$    | ... | $y_J$    |     |\n|     | $x_1$ | $n_{11}$ | $n_{12}$ | ... | $n_{1J}$ |     |\n|     | $x_2$ | $n_{21}$ | $n_{22}$ | ... | $n_{2J}$ |     |\n| $X$ | ...   | ...      | ...      | ... | ...      |     |\n|     | $x_I$ | $n_{I1}$ | $n_{I2}$ | ... | $n_{3J}$ |     |\n:::\n\nDrawing on the `cl.order` data, we can cross-tabulate the frequencies of\n`ORDER` by `SUBORDTYPE`. These correspond to our observed frequencies\n$n_{ij}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cross-tabulate the frequencies for the variables of interest\nfreqs <- table(cl.order$ORDER, cl.order$SUBORDTYPE)\n\nprint(freqs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n```\n\n\n:::\n:::\n\n\nThe expected frequencies $m_{ij}$ are calculated by\n\n$$\nm_{ij} = \\frac{i\\textrm{th row sum} \\times j \\textrm{th column sum}}{\\textrm{number of observations}}.\n$$ {#eq-exp-freq}\n\nThe expected frequencies for our combination of variables is shown\nbelow. In which cells can you see the greatest deviations between\nobserved and expected frequencies?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n# Compute expected frequencies\n\n## Calculate row totals\nrow_totals <- rowSums(freqs)\n\n## Calculate column totals\ncol_totals <- colSums(freqs)\n\n## Total number of observations\ntotal_obs <- sum(freqs)\n\n## Calculate expected frequencies\nexpected_table <- outer(row_totals, col_totals) / total_obs\n\nexpected_table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           caus      temp\nmc-sc 135.79404 139.20596\nsc-mc  63.20596  64.79404\n```\n\n\n:::\n:::\n\n\nThe $\\chi^2$-test now offers a convenient way of quantifying the\ndifferences between the two tables above. Given $n$ observations and $k$\ndegrees of freedom ($df$)[^chi_square_test-1], the $\\chi^2$-statistic\nmeasures how much the observed frequencies **deviate** from the expected\nfrequencies **for each cell** in a contingency table [cf.\n@heumann_introduction_2022: 249-251].\n\n[^chi_square_test-1]: The degrees of freedom are calculated by\n    $(\\textrm{number of rows} -1) \\times (\\textrm{number of columns} - 1)$.\n    \n    \n\n$$\n\\text{chi-squared =}\\frac{(observed - expected)^2}{expected}\n$$ {#eq-chisq-simple}\n\n\n::: {.callout-note collapse=\"false\" title=\"Mathematical details: Formal definition of the chi-square test\"}\nThe joint deviations make up the final $\\chi^2$-score, which is defined\nas\n\n$$\n\\chi^2 = \\sum_{i=1}^{I}\\sum_{i=j}^{J}{\\frac{(n_{ij} - m_{ij})^2}{m_{ij}}}\n$$ {#eq-chisq}\n\nfor $i = 1, ..., I$ and $j = 1, ..., J$.\n\n:::\n\n### Assumptions\n\nNote, however, that this test comes with certain **statistical\nassumptions**. Violations of these assumptions decrease the validity of\nthe result and could, therefore, lead to wrong conclusions about\nrelationships in the data. In this case, other tests should be\nconsulted.\n\n::: callout-important\n### Assumptions of the chi-squared test\n\n1.  All observations are independent of each other.\n2.  80% of the expected frequencies are $\\geq$ 5.\n3.  All observed frequencies are $\\geq$ 1.\n\nIf assumptions 2 and 3 are violated, it is recommended to use a more\nrobust test such as the Fisher's Exact Test (see `?fisher.test()` for\ndetails) or the log-likelihood test ($G$-test).\n\nIn case of dependent observations (e.g., multiple measurements per\nparticipant), the default approach is to fit a multilevel model that can\ncontrol for grouping factors (see mixed-effects regression in @sec-mer.)\n:::\n\n### Implementation in R\n\nIn R, conducting this test is a simple one-liner:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreqs_test <- chisq.test(freqs)\n\nprint(freqs_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  freqs\nX-squared = 104.24, df = 1, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe test object `freqs_test` also stores the expected frequencies, which\nwe can also access. They appear to be all right:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Expected frequencies\nfreqs_test$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n             caus      temp\n  mc-sc 135.79404 139.20596\n  sc-mc  63.20596  64.79404\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}