{
  "hash": "4b639333d829f480ac513cdc3699f8a5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chi-squared test\"\nauthor:\n  name: \"Vladimir Buskin\" \n  orcid: \"0009-0005-5824-1012\"\n  affiliation: \n    name: \"Catholic University of Eichstätt-Ingolstadt\"\n    department: \"English Language and Linguistics\"\nabstract: > \n  The chi-squared ($\\chi^2$) test helps determine if there is a statistically significant association between two categorical variables. It compares the observed frequencies of categories with those expected under the null hypothesis. The $\\chi^2$ (chi-squared) score quantifies the difference between observed and expected frequencies for every cell in a contingency table. The greater the difference between observed and expected, the higher the $\\chi^2$ score and the lower the $p$-value, given the degrees of freedom. It is recommended to compute effect size measures and inspect the residuals to assess the nature of the association.\nformat:\n  html:\n    self-contained: true\n    code-fold: false\n    theme: default\n    toc: true\n    toc-depth: 4\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 3\n    scrollable: true\neditor: visual\nbibliography: R.bib\nmetadata-files:\n  - _quarto.yml\ngoogle-scholar: false\n---\n\n\n## Suggested reading\n\nFor linguists:\n\n> @gries_statistics_2021: Chapter 4.1.2.1\n\nGeneral:\n\n> @baguleySeriousStatsGuide2012: Chapter 4\n>\n> @agrestiFoundationsStatisticsData2022: Chapter 5\n\n## Preparation\n\n::: callout-tip\n## Script\n\nYou can find the full R script associated with this unit\n[here](https://osf.io/u7f8w).\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#  Load libraries\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(confintr) # for effect size calculation\n\n# Load data\ncl.order <- read_xlsx(\"Paquot_Larsson_2020_data.xlsx\")\n```\n:::\n\n\n## The Pearson $\\chi^2$-test of independence\n\nThe first step of any significance test involves setting up the null and\nalternative hypothesis. In this unit, we will perform a sample analysis\non the relationship between clause `ORDER` and the type of subordinate\nclause (`SUBORDTYPE`). Specifically, we will focus on the\n**independence** of these two discrete variables (i.e, the presence or\nabsence of correlation).\n\n-   $H_0:$ The variables `ORDER` and `SUBORDTYPE` are independent.\n\n-   $H_1:$ The variables `ORDER` and `SUBORDTYPE` are **not**\n    independent.\n\n::: {.callout-note title=\"What does independence really mean?\" collapse=\"true\"}\nThe core idea is \"that the probability distribution of the response\nvariable is the same for each group\"\n[@agrestiFoundationsStatisticsData2022: 177]. If clause `ORDER` is the\nresponse variable and `SUBORDTYPE` the explanatory variable,\nindependence would entail that the probabilities of the outcomes of the\nresponse variable `ORDER = \"mc-sc\"` and `ORDER = \"sc-mc\"` are not\ninfluenced by whether they occur in the groups `SUBORDTYPE = \"temp\"` or\n`SUBORDTYPE = \"caus\"`.\n\nThe term **probability distribution** refers to a mathematical function\nthat assigns probabilities to the outcomes of a variable. If we consider\ntwo variables at the same time, such as $X$ and $Y$, they are said to\nhave **marginal probability functions** $f_1(x)$ and $f_2(y)$. If we\ncondition the outcomes of all values on each other, the following\nequivalence will hold:\n\n$$\nf(x \\mid y) = f_1(x) \\text{ and } f(y \\mid x) = f_2(y).\n$$ {#eq-indep}\n\nThus, the null hypothesis assumes that the probabilities of each\ncombination of values (such as `ORDER` and `SUBORDTYPE`), denoted by\n$\\pi_{ij}$, have the relationship in @eq-indep. This can be stated\nsuccinctly as\n\n$$\nH_0 : \\pi_{ij} = P(X = j)P(Y = i).\n$$ {#eq-nullhyp-math}\n:::\n\nNext, we compute a test statistic that indicates how strongly our data\nconforms to $H_0$, such as Pearson's $\\chi^2$. To this end, we will need\ntwo types of values:\n\n-   the **observed frequencies** $f_{ij}$ present in our data set\n\n-   the **expected frequencies** $e_{ij}$, which we would expect to see\n    if $H_0$ were true,\n\nwhere $f, e \\in \\mathbb{N}$. The indices $i$ and $j$ uniquely identify\nthe cell counts in all column-row combinations of a contingency table.\n\n### Observerd frequencies\n\nThe table below represents a generic contingency table where $Y$ and $X$\nare categorical variables and have the values\n$Y = \\{y_1, y_2, \\dots, y_i \\}$ and $X = \\{x_1, x_2, \\dots, x_j\\}$. In\nthe table, each cell indicates the count of observation $f_{ij}$\ncorresponding to the $i$-th row and $j$-th column.\n\n|     |       |          | $X$      |     |          |     |\n|-----|-------|----------|----------|-----|----------|-----|\n|     |       | $x_1$    | $x_2$    | ... | $x_j$    |     |\n|     | $y_1$ | $f_{11}$ | $f_{12}$ | ... | $f_{1j}$ |     |\n|     | $y_2$ | $f_{21}$ | $f_{22}$ | ... | $f_{2j}$ |     |\n| $Y$ | ...   | ...      | ...      | ... | ...      |     |\n|     | $y_i$ | $f_{i1}$ | $f_{i2}$ | ... | $f_{ij}$ |     |\n\n\n```{=html}\n<div style=\"margin-top: 20px;\"></div>\n```\n\nIn the `cl.order` data, the **observed frequencies** correspond to how\noften each `ORDER` value (i.e., `mc-sc` and `sc-mc`) is attested for a\ngiven `SUBORDTYPE` (i.e., `temp` and `caus`). This can be done in a very\nstraightforward fashion using R's `table()` function on the variables of\ninterest.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobserved_freqs <- table(cl.order$ORDER, cl.order$SUBORDTYPE)\n\nprint(observed_freqs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n```\n\n\n:::\n:::\n\n\n### Expected frequencies\n\nThe **expected frequencies** require a few additional steps. Usually,\nthese steps are performed automatically when conducting the chi-squared\ntest in R, so you don't have to worry about calculating them by hand. We\nwill do it anyway to drive home the rationale of the test.\n\n|     |       |          | $X$      |     |          |     |\n|-----|-------|----------|----------|-----|----------|-----|\n|     |       | $x_1$    | $x_2$    | ... | $x_j$    |     |\n|     | $y_1$ | $e_{11}$ | $e_{12}$ | ... | $e_{1j}$ |     |\n|     | $y_2$ | $e_{21}$ | $e_{22}$ | ... | $e_{2j}$ |     |\n| $Y$ | ...   | ...      | ...      | ... | ...      |     |\n|     | $y_i$ | $e_{i1}$ | $e_{i2}$ | ... | $e_{ij}$ |     |\n\n\n```{=html}\n<div style=\"margin-top: 20px;\"></div>\n```\n\nThe expected frequencies $e_{ij}$ are given by the formula in\n@eq-exp-freq. In concrete terms, we go through each cell in the\ncross-table and multiply the corresponding row sums with the column\nsums, dividing the result by the total number of occurrences in the\nsample. For example, there are $184$ occurrences of `mc-sc` clause\norders where the subordinate clause is causal. The row sum is\n$184 + 91 = 275$ and the column sum is $184 + 15 = 199$. Next, we take\ntheir product $275 \\times 199$ and divide it by the total number of\nobservations, which is $184 + 91 + 15 + 113 = 403$. Thus we obtain an\nexpected frequency of $\\frac{275 \\times 199}{403} = 135.79$ under the\nnull hypothesis.\n\n$$\ne_{ij} = \\frac{i\\textrm{th row sum} \\times j \\textrm{th column sum}}{\\textrm{number of observations}}\n$$ {#eq-exp-freq}\n\nThe expected frequencies for our combination of variables is shown\nbelow. In which cells can you see the greatest deviations between\nobserved and expected frequencies?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n## Calculate row totals\nrow_totals <- rowSums(observed_freqs)\n\n## Calculate column totals\ncol_totals <- colSums(observed_freqs)\n\n## Total number of observations\ntotal_obs <- sum(observed_freqs)\n\n## Calculate expected frequencies\nexpected_freqs <- outer(row_totals, col_totals) / total_obs\n\nprint(expected_freqs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           caus      temp\nmc-sc 135.79404 139.20596\nsc-mc  63.20596  64.79404\n```\n\n\n:::\n:::\n\n\n### Conducting the test\n\nThe $\\chi^2$-test now offers a convenient way of quantifying the\ndifferences between the two tables above. It measures how much the\nobserved frequencies **deviate** from the expected frequencies **for\neach cell** in a contingency table [cf. @heumann_introduction_2022:\n249-251]. The gist of this procedure is summarised in @eq-chisq-simple.\n\n$$\n\\text{Chi-squared } \\chi^2 =\\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\n$$ {#eq-chisq-simple}\n\n::: {.callout-note collapse=\"true\" title=\"Formal definition of the $\\\\chi^2$-test\"}\nGiven $n$ observations and $k$ degrees of freedom $df$, the joint\nsquared deviations between $f_{ij}$ and $e_{ij}$ contribute to the final\n$\\chi^2$-score, which is defined as\n\n$$\n\\chi^2 = \\sum_{i=1}^{I}\\sum_{j=1}^{J}{\\frac{(f_{ij} - e_{ij})^2}{e_{ij}}}\n$$ {#eq-chisq}\n\nfor $i = 1, ..., I$ and $j = 1, ..., J$ and\n$df = (\\textrm{number of rows} -1) \\times (\\textrm{number of columns} - 1)$.\n\nApplying the formula, the updated contingency would have the following\nform:\n\n|     |       |                                      | $X$                                  |     |                                      |     |\n|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n|     |       | $x_1$                                | $x_2$                                | ... | $x_j$                                |     |\n|     | $y_1$ | $\\frac{(f_{11} - e_{11})^2}{e_{11}}$ | $\\frac{(f_{12} - e_{12})^2}{e_{12}}$ | ... | $\\frac{(f_{1j} - e_{1j})^2}{e_{1j}}$ |     |\n|     | $y_2$ | $\\frac{(f_{21} - e_{21})^2}{e_{21}}$ | $\\frac{(f_{22} - e_{22})^2}{e_{22}}$ | ... | $\\frac{(f_{2j} - e_{2j})^2}{e_{2j}}$ |     |\n| $Y$ | ...   | ...                                  | ...                                  | ... | ...                                  |     |\n|     | $y_i$ | $\\frac{(f_{i1} - e_{i1})^2}{e_{i1}}$ | $\\frac{(f_{i2} - e_{i1})^2}{e_{i2}}$ | ... | $\\frac{(f_{ij} - e_{ij})^2}{e_{ij}}$ |     |\n:::\n\nThe implementation in R is a simple one-liner. Keep in mind that we have\nto supply **absolute frequencies** to `chisq.test()` rather than\npercentages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreqs_test <- chisq.test(observed_freqs)\n\nprint(freqs_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  observed_freqs\nX-squared = 104.24, df = 1, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\nQuite conveniently, the test object `freqs_test` stores the expected\nfrequencies, which can be easily accessed via subsetting. Luckily, they\nare identical to what we calculated above!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreqs_test$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n             caus      temp\n  mc-sc 135.79404 139.20596\n  sc-mc  63.20596  64.79404\n```\n\n\n:::\n:::\n\n\n### Assumptions of the chi-squared test\n\nThis $\\chi^2$-test comes with certain **statistical assumptions**.\nViolations of these assumptions decrease the validity of the result and\ncould, therefore, lead to wrong conclusions about relationships in the\ndata. In this case, other tests should be consulted.\n\n::: callout-important\n1.  All observations are independent of each other.\n2.  80% of the expected frequencies are $\\geq$ 5.\n3.  All observed frequencies are $\\geq$ 1.\n:::\n\nIn case of dependent observations (e.g., multiple measurements per\nparticipant), the default approach is to fit a multilevel model that can\ncontrol for grouping factors (see mixed-effects regression in @sec-mer.)\n\nIf assumptions 2 and 3 are violated, it is recommended to use a more\nrobust test such as the **Fisher's Exact Test** or the **log-likelihood\nratio test** ($G$-test).\n\n::: {.callout-tip collapse=\"true\" title=\"Fisher's Exact Test\"}\nWhile the $\\chi^2$-test can only approximate the $p$-value, Fisher's\nExact Test can provide an exact solution. Note that for anything more\ncomplex than a $2 \\times 2$ table, it becomes considerably more\ncomputationally expensive; if it takes too long, set\n`simulate.p.value = TRUE`.\n\nDrawing on the hypergeometric distribution (see `?dhyper()`), it\ncomputes the probability of all frequency tables that are equal or more\nextreme than the one observed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfisher.test(observed_freqs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  observed_freqs\np-value < 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  8.216959 29.548120\nsample estimates:\nodds ratio \n  15.11768 \n```\n\n\n:::\n:::\n\n:::\n\n::: {.callout-tip collapse=\"true\" title=\"$G$-test\"}\nThe $G^2$-statistic is analogous to $\\chi^2$, but it tends to be more\nrobust for lower observed counts. It is defined as\n\n$$\nG^2 = 2\\sum_{i=1}^{I}\\sum_{j=1}^{J} f_{ij} \\ln\\left({\\frac{f_{ij}}{e_{ij}}}\\right)\n$$ {#eq-g2}\n\nand implemented in R via the `DescTools` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load library (install if necessary)\nlibrary(DescTools)\n\n# Perform G-test (preferably for tables with more than 2 rows/columns)\nGTest(observed_freqs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLog likelihood ratio (G-test) test of independence without correction\n\ndata:  observed_freqs\nG = 116.97, X-squared df = 1, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n:::\n\n### How do I make sense of the test results?\n\nThe test output has three 'ingredients':\n\n-   the **chi-squared score** (X-squared)\n-   the **degrees of freedom** (df)\n-   the **p-value**.\n\nIt is absolutely essential to report all three of those as they\ndetermine each other. Here a few possible wordings that could be used:\n\n> According to a $\\chi^2$-test, there is a highly significant\n> association between clause ORDER and SUBORDTYPE at $p < 0.001$\n> ($\\chi^2 = 106.44, df = 1$), thus justifying the rejection of $H_0$.\n\n> A $\\chi^2$-test revealed a highly significant association between\n> clause ORDER and SUBORDTYPE ($\\chi^2(1) = 106.44$, $p < 0.001$),\n> supporting the rejection of $H_0$.\n\n> The $\\chi^2$-test results ($\\chi^2 = 106.44$, $df = 1$, $p < 0.001$)\n> provide strong evidence against the null hypothesis, demonstrating a\n> significant association between clause ORDER and SUBORDTYPE.\n\nThe test results suggest that the dependent variable `ORDER` and the\nexplanatory variable `SUBORDTYPE` are not independent of each other. The\nprobability of **randomly** observing usage patterns such as those found\nin the `cl.order` data is lower than 0.001 $\\approx$ 0.1%, which is\nenough to reject the null hypothesis at $\\alpha = 0.05$.\n\nWe can infer that a speaker's choice of clause `ORDER` is very likely\ninfluenced by the semantic type of subordinate clause; in other words,\nthese two variables are **correlated**. However, there are still several\nthings the test does **not** tell us:\n\n-   Are there certain variable combinations where the $\\chi^2$-scores\n    are particularly high?\n\n-   How strongly do `ORDER` and `SUBORDTYPE` influence each other?\n\n## Pearson residuals\n\nIf we're interested in what cells show the greatest difference between\nobserved and expected frequencies, an option would be to inspect the\nPearson residuals (cf. @eq-pear-res).\n\n$$ \\text{residuals} = \\frac{\\text{observed} - \\text{expected}}{\\sqrt{\\text{expected}}} \n$$ {#eq-pear-res}\n\nThese can be accessed via the test results stored `freqs_test`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreqs_test$residuals\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n             caus      temp\n  mc-sc  4.136760 -4.085750\n  sc-mc -6.063476  5.988708\n```\n\n\n:::\n:::\n\n\nThe function `assocplot()` can automatically compute the pearson\nresiduals for any given contingency table and create a plot that\nhighlights their contributions. If the bar is above the dashed line, it\nis black and indicates that a category is observed more frequently than\nexpected (e.g., causal subordinate clauses in the `mc-sc` order).\nConversely, bars are coloured grey if a category is considerably less\nfrequent than expected, such as `caus` in `sc-mc`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nassocplot(t(observed_freqs), col = c(\"black\", \"lightgrey\"))\n```\n\n::: {.cell-output-display}\n![](Chi_square_test_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n::: {.callout-tip collapse=\"true\" title=\"Testing the residuals: Configural Frequency Analysis\"}\nThe chi-squared test only provides a $p$-value for the entire\ncontingency table. But what if we wanted to test the residuals for their\nsignificance as well? Configural Frequency Analysis\n[@krauth_lienert_kfa] allows us to do exactly that: It performs a\nsignificance test for **all combinations of variable values** in a\ncross-table. Moreover, CFA is not limited to two variables only.\nTechnically, users can test for associations between arbitrary numbers\nof variables, but should be aware of the increasing complexity of\ninterpretation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cfa) # install library beforehand\n\n# Get the observed counts and convert them to a data frame\nconfig_df <- as.data.frame(observed_freqs)\n\n# Convert to matrix\nconfigs <- as.matrix(config_df[, 1:2])  # first two columns contain the configurations (= combinations of variable values)\ncounts <- config_df$Freq # Freq column contains the corresponding counts\n\n# Perform CFA on configuarations and counts; apply Bonferroni correction for multiple testing\ncfa_output <- cfa(configs, counts, bonferroni = TRUE)\n\n# Print output\nprint(cfa_output)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n*** Analysis of configuration frequencies (CFA) ***\n\n       label   n  expected         Q    chisq      p.chisq sig.chisq         z\n1 sc-mc caus  15  63.20596 0.1418682 36.76575 1.332103e-09      TRUE -6.671872\n2 sc-mc temp 113  64.79404 0.1425343 35.86463 2.115143e-09      TRUE  6.469444\n3 mc-sc caus 184 135.79404 0.1804075 17.11278 3.522441e-05      TRUE  5.027611\n4 mc-sc temp  91 139.20596 0.1827409 16.69335 4.393467e-05      TRUE -5.102385\n           p.z sig.z\n1 1.000000e+00  TRUE\n2 4.918210e-11  TRUE\n3 2.483137e-07  TRUE\n4 9.999998e-01  TRUE\n\n\nSummary statistics:\n\nTotal Chi squared         =  106.4365 \nTotal degrees of freedom  =  1 \np                         =  0 \nSum of counts             =  403 \n\nLevels:\n\nVar1 Var2 \n   2    2 \n```\n\n\n:::\n:::\n\n:::\n\n## Effect size\n\nThe $p$-value only indicates the **presence of correlation**, but not\nits strength – regardless of how low it may be. It does not convey how\nmuch two variables determine each other. For this reason, it is highly\nrecommended to report an **effect size measure** alongside the\n$p$-value. One such measure is Cramér's $V$, which takes values in the\ninterval $[0, 1]$:\n\n$$\nV = \\sqrt{\\frac{\\chi^2}{n \\times (min(nrow, ncol) - 1)}}.\n$$ {#eq-effsize-v}\n\nThe package `confintr` implements this in its `cramersv()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncramersv(freqs_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5085863\n```\n\n\n:::\n:::\n\n\nThe association between two categorical variables is stronger, the\ncloser $V$ approximates 1. Conversely, if $V = 0$, then the variables\nare completely independent. There are various guidelines in the\nliterature that provide thresholds for \"small\", \"moderate\" and \"large\"\neffects, yet these are rarely justified on theoretical grounds and could\nbe viewed as arbitrary.\n\n## Only one variable? The Pearson $\\chi^2$ goodness-of-fit test\n\nThe $\\chi^2$-statistic can also be utilised in simpler scenarios that involve only one single categorical variable. Consider the small [`eat_obj_aspect.xlsx`](https://osf.io/y5scq) dataset, where the `object_realisation` column indicates whether the object was realised or dropped in a given observation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data\neat <- read_xlsx(\"eat_obj_aspect.xlsx\")\n\n# Show object realisation pattern\neat_observed <- table(eat$object_realisation)\n\nprint(eat_observed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n no yes \n 57  45 \n```\n\n\n:::\n:::\n\n\nWe can use the $\\chi^2$ **goddness-of-fit test** \"to compare an observed frequency distribution against its expected probability of occurrence\" [@baguleySeriousStatsGuide2012: 132]. In other words, we can check if the observed object data matches the frequencies we'd expect to see if both outcomes of `object realisation` were equally likely and hence randomly distributed.\n\nOur hypotheses are thus \n\n-   $H_0:$ Observed frequencies of `object realisation` $=$ expected frequencies of `object realisation`\n\n-   $H_1:$ Observed frequencies of `object realisation` $\\neq$ expected frequencies of `object realisation`\n\nThe expected frequencies are given by \n\n$$\ne_i = \\frac{\\text{number of observations}}{\\text{number of cells}},\n$$ {#eq-exp-gof}\n\n\nand the test statistic simplifies to \n\n$$\n\\chi^2 = \\sum_{i=1}^{I}{\\frac{(f_{i} - e_{i})^2}{e_{i}}}.\n$$ {#eq-chisq-gof}\n\n\nIn R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform the chi-squared goodness-of-fit test\neat_gof <-  chisq.test(eat_observed)\n\nprint(eat_gof)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tChi-squared test for given probabilities\n\ndata:  eat_observed\nX-squared = 1.4118, df = 1, p-value = 0.2348\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check expected frequencies (should be greater or equal to 5)\nprint(eat_gof$expected)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n no yes \n 51  51 \n```\n\n\n:::\n:::\n\n\n> A chi-squared goodness-of-fit test indicates that the distribution of object realisation for *eat* does not significantly differ from an equal distribution ($\\chi^2$ = 1.41, df = 1, $p$ = 0.23). In the sample at hand, the verb does not seem to prefer one variant or the other.\n\n\n## Exercises\n\n::: callout-tip\n## Solutions\n\nYou can find the solutions to the exercises\n[here](https://osf.io/m8et2).\n:::\n\n::: {#exr-chisq-1}\nLoad the file [`eat_obj_aspect.xlsx`](https://osf.io/y5scq) into R. It\ncontains hits for the verb lemma *eat* which were annotated for the\npresence or absence of a direct object (`object_realisation` column) and\nfor the aspect of the verb (`verb_aspect` column).\n\n-   Create a frequency table that cross-classifies `object_realisation`\n    and `verb_aspect`.\n\n-   Forward a set of statistical hypotheses.\n\n-   Perform a $\\chi^2$-test of independence and interpret the result.\n:::\n\n::: {#exr-chisq-2}\nThe `chisq.test()` returns a warning message: \"Chi-squared approximation\nmay be incorrect\". Examine the frequency table for potential violations\nof the test assumptions. How could you control for them?\n:::\n",
    "supporting": [
      "Chi_square_test_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}