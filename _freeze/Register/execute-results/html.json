{
  "hash": "3537ee0395f46264ab50a7c9c9351637",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"ICE: Extract register data\"\nauthor:\n  name: \"Vladimir Buskin\" \n  orcid: \"0009-0005-5824-1012\"\n  affiliation: \n    name: \"Catholic University of Eichstätt-Ingolstadt\"\n    department: \"English Language and Linguistics\"\nformat:\n  html:\n    self-contained: true\n    theme: default\n    toc: true\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 3\n    scrollable: true\neditor: visual\nbibliography: R.bib\n---\n\n\n::: callout-tip\n## Script\n\nYou can find the full R script associated with this unit\n[here](https://osf.io/ny2g3).\n:::\n\n## Preparation\n\nThis supplementary unit illustrates how to extract register information\nfrom `kwic()` queries performed on data from the International Corpus of\nEnglish (cf. [Concordancing](Concordancing.qmd)). We begin by loading\nthe relevant libraries:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(readxl)\n```\n:::\n\n\nNext we load some sample data to work with. If you have data of your\nown, you may skip this step.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data\ndata_eat <- read_xlsx(\"eat_obj_aspect.xlsx\")\n```\n:::\n\n\nThe dataset follows the default `kwic()` structure with the document\nname and the immediate context before and after the keyword. The columns\n`object_realisation` (dependent variable) and `verb_aspect` (indepdent\nvariable) are the result of manual annotation in Microsoft Excel.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First few lines\nhead(data_eat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n  docname  from    to pre   keyword post  pattern object_realisation verb_aspect\n  <chr>   <dbl> <dbl> <chr> <chr>   <chr> <chr>   <chr>              <chr>      \n1 ICE_GB…   458   458 had … eaten   anyw… \"\\\\b(e… no                 perfective \n2 ICE_GB…   478   478 : 1 … eating  will… \"\\\\b(e… no                 progressive\n3 ICE_GB…   785   785 > Ye… eat     befo… \"\\\\b(e… no                 neutral    \n4 ICE_GB…  1198  1198 the … eat     them… \"\\\\b(e… yes                neutral    \n5 ICE_GB…  4529  4529 > Ye… ate     in t… \"\\\\b(e… no                 neutral    \n6 ICE_GB…   958   958 know… eat     it f… \"\\\\b(e… yes                neutral    \n```\n\n\n:::\n:::\n\n\n## Fine-grained: Text categories\n\nAfter querying the ICE corpora, information on the texts (and,\ntherefore, the register) is stored in the `docname` column.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(data_eat[,\"docname\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 1\n  docname           \n  <chr>             \n1 ICE_GB/S1A-006.txt\n2 ICE_GB/S1A-006.txt\n3 ICE_GB/S1A-006.txt\n4 ICE_GB/S1A-009.txt\n5 ICE_GB/S1A-009.txt\n6 ICE_GB/S1A-010.txt\n```\n\n\n:::\n:::\n\n\nWe want to split up this column into two separate ones. One should\ncontain the text category labels (S1A, S1B, S2A etc.) and one the file\nnumbers (001, 002, 003 etc.). The tidyverse function\n`seperate_wider_delim()` offers a method to handle that. Let's call the\nnew columns `text_category` and `file_number`, respectively.\n\nIf you apply the function to your own data, simply exchange the `data =`\nargument with your search results, and leave the rest as is.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_eat_reg <- separate_wider_delim(\n                                # Original data frame\n                                data = data_eat,\n                                # Which column to split\n                                cols = docname,\n                                # Where to split it (at the hyphen)\n                                delim = \"-\", # where to split exactly\n                                # Names of the new columns\n                                names = c(\"text_category\", \"file_number\")\n                                ) \n```\n:::\n\n\nThe data frame now have the desired format, as the first two columns\nindicate:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(data_eat_reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 10\n  text_category file_number  from    to pre                keyword post  pattern\n  <chr>         <chr>       <dbl> <dbl> <chr>              <chr>   <chr> <chr>  \n1 ICE_GB/S1A    006.txt       458   458 had lunch < ICE-G… eaten   anyw… \"\\\\b(e…\n2 ICE_GB/S1A    006.txt       478   478 : 1 : A > Well I … eating  will… \"\\\\b(e…\n3 ICE_GB/S1A    006.txt       785   785 > Yeah < ICE-GB:S… eat     befo… \"\\\\b(e…\n4 ICE_GB/S1A    009.txt      1198  1198 the summer < ICE-… eat     them… \"\\\\b(e…\n5 ICE_GB/S1A    009.txt      4529  4529 > Yeah < , > < IC… ate     in t… \"\\\\b(e…\n6 ICE_GB/S1A    010.txt       958   958 know I mean it wo… eat     it f… \"\\\\b(e…\n# ℹ 2 more variables: object_realisation <chr>, verb_aspect <chr>\n```\n\n\n:::\n:::\n\n\n## More general: Spoken vs. written\n\nIf there is no need for the fine-grained register distinctions\nillustrated above, we might as well group them into macro-categories\ninstead, such as **spoken** data and **written** data.\n\n::: {.callout-note collapse=\"true\" title=\"Summary of text categories in the ICE corpora\"}\n| Category           | Subcategory                        | Type                     | Code Range         |\n|------------------|------------------|------------------|------------------|\n| **SPOKEN (S)**     | **DIALOGUE (S1)**                  | **PRIVATE (S1A)**        |                    |\n|                    |                                    | Direct Conversations     | S1A-001 to S1A-090 |\n|                    |                                    | Telephone Calls          | S1A-091 to S1A-100 |\n|                    | **PUBLIC (S1B)**                   |                          |                    |\n|                    |                                    | Class Lessons            | S1B-001 to S1B-020 |\n|                    |                                    | Broadcast Discussions    | S1B-021 to S1B-040 |\n|                    |                                    | Broadcast Interviews     | S1B-041 to S1B-050 |\n|                    |                                    | Parliamentary Debates    | S1B-051 to S1B-060 |\n|                    |                                    | Legal Cross-examinations | S1B-061 to S1B-070 |\n|                    |                                    | Business Transactions    | S1B-071 to S1B-080 |\n| **MONOLOGUE (S2)** | **UNSCRIPTED (S2A)**               |                          |                    |\n|                    |                                    | Spontaneous Commentaries | S2A-001 to S2A-020 |\n|                    |                                    | Unscripted Speeches      | S2A-021 to S2A-050 |\n|                    |                                    | Demonstrations           | S2A-051 to S2A-060 |\n|                    |                                    | Legal Presentations      | S2A-061 to S2A-070 |\n|                    | **SCRIPTED (S2B)**                 |                          |                    |\n|                    |                                    | Broadcast News           | S2B-001 to S2B-020 |\n|                    |                                    | Broadcast Talks          | S2B-021 to S2B-040 |\n|                    |                                    | Non-broadcast Talks      | S2B-041 to S2B-050 |\n| **WRITTEN (W)**    | **NON-PRINTED (W1)**               |                          |                    |\n|                    | **NON-PROFESSIONAL WRITING (W1A)** |                          |                    |\n|                    |                                    | Student Essays           | W1A-001 to W1A-010 |\n|                    |                                    | Examination Scripts      | W1A-011 to W1A-020 |\n|                    | **CORRESPONDENCE (W1B)**           |                          |                    |\n|                    |                                    | Social Letters           | W1B-001 to W1B-015 |\n|                    |                                    | Business Letters         | W1B-016 to W1B-030 |\n| **PRINTED (W2)**   | **ACADEMIC WRITING (W2A)**         |                          |                    |\n|                    |                                    | Humanities               | W2A-001 to W2A-010 |\n|                    |                                    | Social Sciences          | W2A-011 to W2A-020 |\n|                    |                                    | Natural Sciences         | W2A-021 to W2A-030 |\n|                    |                                    | Technology               | W2A-031 to W2A-040 |\n|                    | **NON-ACADEMIC WRITING (W2B)**     |                          |                    |\n|                    |                                    | Humanities               | W2B-001 to W2B-010 |\n|                    |                                    | Social Sciences          | W2B-011 to W2B-020 |\n|                    |                                    | Natural Sciences         | W2B-021 to W2B-030 |\n|                    |                                    | Technology               | W2B-031 to W2B-040 |\n|                    | **REPORTAGE (W2C)**                | Press News Reports       | W2C-001 to W2C-020 |\n|                    | **INSTRUCTIONAL WRITING (W2D)**    |                          |                    |\n|                    |                                    | Administrative Writing   | W2D-001 to W2D-010 |\n|                    |                                    | Skills & Hobbies         | W2D-011 to W2D-020 |\n|                    | **PERSUASIVE WRITING (W2E)**       | Press Editorials         | W2E-001 to W2E-010 |\n|                    | **CREATIVE WRITING (W2F)**         | Novels & Stories         | W2F-001 to W2F-020 |\n:::\n\nWhat all spoken files have in common is that they begin with the\nupper-case letter `\"S\"`, and written files with `\"W\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Classify text files as either \"spoken\" or \"written\"\ndata_eat_reg %>% # <1>\n  mutate(medium = case_when( # <2>\n    grepl(\"ICE_GB/S\", text_category) ~ \"spoken\", # <3>\n    grepl(\"ICE_GB/W\", text_category) ~ \"written\" # <4>\n  )) -> data_sw # <5>\n```\n:::\n\n\n1.  Take the data frame `data_eat_reg` and pass it on to the next\n    function via the pipe operator `%>%`. Make sure `library(tidyverse)`\n    is loaded.\n\n2.  Create a new column with `mutate` and call it `medium`. The column\n    values are assigned conditionally with `case_when()`:\n\n3.  If `text_category` contains the string `\"ICE_GB/S\"`, classify\n    `medium` as `\"spoken\"`.\n\n4.  If `text_category` contains the string `\"ICE_GB/W\"`, classify\n    `medium` as `\"written\"`.\n\n5.  Store the new data frame in the variable `data_sw`.\n\n## What next? A few sample analyses\n\nIt is now possible to investigate associations between register (i.e.,\n`text_category` or `medium`) and other variables more comfortably.\n\n-   `object_realisation` and `text_category`\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Contingency table\n    obj_reg_freq <- xtabs(~ object_realisation + text_category, data_eat_reg)\n    \n    # Percentage table\n    object_reg_prop <- prop.table(obj_reg_freq, margin = 2) * 100\n    \n    print(object_reg_prop)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n                      text_category\n    object_realisation ICE_GB/S1A ICE_GB/S1B ICE_GB/S2A ICE_GB/S2B ICE_GB/W1B\n                   no    61.22449   60.00000   50.00000   66.66667   90.00000\n                   yes   38.77551   40.00000   50.00000   33.33333   10.00000\n                      text_category\n    object_realisation ICE_GB/W2B ICE_GB/W2C ICE_GB/W2D ICE_GB/W2E ICE_GB/W2F\n                   no     0.00000  100.00000   50.00000  100.00000   50.00000\n                   yes  100.00000    0.00000   50.00000    0.00000   50.00000\n    ```\n    \n    \n    :::\n    \n    ```{.r .cell-code}\n    # Simple barplot\n    barplot(object_reg_prop, \n            beside = TRUE, \n            legend = TRUE,\n            cex.names = 0.8)\n    ```\n    \n    ::: {.cell-output-display}\n    ![](Register_files/figure-html/unnamed-chunk-9-1.png){width=672}\n    :::\n    \n    ```{.r .cell-code}\n    # Statistical analysis\n    fisher.test(obj_reg_freq)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    \n    \tFisher's Exact Test for Count Data\n    \n    data:  obj_reg_freq\n    p-value = 5.15e-05\n    alternative hypothesis: two.sided\n    ```\n    \n    \n    :::\n    :::\n\n\n-   `object_realisation` and `medium`\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Contingency table\n    obj_sw_freq <- xtabs(~ object_realisation + medium, data_sw)\n    \n    # Percentage table\n    obj_sw_prop <- prop.table(obj_sw_freq, margin = 2) * 100\n    \n    print(obj_sw_prop)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n                      medium\n    object_realisation   spoken  written\n                   no  60.29412 47.05882\n                   yes 39.70588 52.94118\n    ```\n    \n    \n    :::\n    \n    ```{.r .cell-code}\n    # Simple barplot\n    barplot(obj_sw_prop, \n            beside = TRUE, \n            legend = TRUE,\n            cex.names = 0.8)\n    ```\n    \n    ::: {.cell-output-display}\n    ![](Register_files/figure-html/unnamed-chunk-10-1.png){width=672}\n    :::\n    \n    ```{.r .cell-code}\n    # Statistical analysis\n    chisq.test(obj_sw_freq)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    \n    \tPearson's Chi-squared test with Yates' continuity correction\n    \n    data:  obj_sw_freq\n    X-squared = 1.1184, df = 1, p-value = 0.2903\n    ```\n    \n    \n    :::\n    :::\n",
    "supporting": [
      "Register_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}