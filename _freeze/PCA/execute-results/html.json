{
  "hash": "af50a47e8e185326675a16f4b73e9a14",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Principal Components Analysis\"\nauthor: Vladimir Buskin\nformat:\n  html:\n    self-contained: true\n    code-fold: false\n    theme: default\n    toc: true\n    toc-depth: 4\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 3\n    scrollable: true\neditor: visual\nbibliography: R.bib\nmetadata-files:\n  - _quarto.yml\ngoogle-scholar: false\n---\n\n\n## Recommended reading\n\n> Mair [-@mairModernPsychometrics2018: Chapter 6]\n\n## Preparation\n\nThis unit relies on psycholinguistic data from the [South Carolina\nPsycholinguistic\nMetabase](https://sc.edu/study/colleges_schools/artsandsciences/psychology/research_clinical_facilities/scope/)\n[@gaoSCOPESouthCarolina2022].[^pca-1] Detailed descriptions of the\nvariables can be found\n[here](https://sc.edu/study/colleges_schools/artsandsciences/psychology/research_clinical_facilities/scope/search.php).\n\n[^pca-1]: One exception is the variable\n    `Resnik_strength [`@resnikSelectionalConstraintsInformationtheoretic1996\\],\n    which was computed manually and appended to the data frame.\n\nThe data frame `scope_sem_df` contains semantic ratings for a sample of\n1,702 transitive verbs. Note that all columns have been standardised\n(cf. `?scale()` for details).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(lattice)\nlibrary(corrplot)\nlibrary(psych)\nlibrary(GPArotation)\nlibrary(Gifi)\n\n\n# Load data\nscope_sem_df <- readRDS(\"scope_sem_df.RDS\")\n\n# Overview\nglimpse(scope_sem_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,702\nColumns: 36\n$ Verb                   <chr> \"abstain\", \"abstract\", \"abuse\", \"accelerate\", \"…\n$ Conc_Brys              <dbl> -0.94444378, -1.92983639, -0.59478833, 0.221074…\n$ Nsenses_WordNet        <dbl> -0.68843996, 0.27755219, 0.00155443, -0.6884399…\n$ Nmeanings_Websters     <dbl> -0.95559835, 0.73781281, 0.73781281, -0.2782338…\n$ Visual_Lanc            <dbl> -2.2545455, 0.6103733, 1.3354358, -0.4342084, -…\n$ Auditory_Lanc          <dbl> -0.84225787, -0.35605108, 1.54797548, 0.1879565…\n$ Haptic_Lanc            <dbl> -0.75523987, -0.29089287, 1.25099360, -0.189118…\n$ Olfactory_Lanc         <dbl> -0.14444936, -0.37350419, -0.53335522, -0.37350…\n$ Gustatory_Lanc         <dbl> 0.27698988, -0.10105698, -0.36148925, -0.521109…\n$ Interoceptive_Lanc     <dbl> 1.08153427, -0.06560311, 1.64313895, 1.45452985…\n$ Head_Lanc              <dbl> 0.09711686, 1.51822273, 0.97485872, -1.30201866…\n$ Torso_Lanc             <dbl> -0.10777786, -0.63010545, 1.31967910, 0.6727322…\n$ Mouth_Throat_Lanc      <dbl> -0.4500356, 0.1664627, 1.5030949, -0.6329479, 0…\n$ Hand_Arm_Lanc          <dbl> -1.01104245, -1.22599107, 0.77468463, -0.754757…\n$ Foot_Leg_Lanc          <dbl> -0.483955477, -0.602915105, 1.222099719, 3.4122…\n$ Mink_Perceptual_Lanc   <dbl> -1.75989047, -0.06929703, 1.73296327, -0.349141…\n$ Mink_Action_Lanc       <dbl> -0.97778288, 0.27808304, 1.24132057, 0.86695389…\n$ Valence_Warr           <dbl> -0.58345737, 0.08449407, -2.83877574, 0.7524455…\n$ Valence_Extremity_Warr <dbl> -0.559736647, -1.018041177, 3.198360497, 0.0949…\n$ Valence_NRC            <dbl> -0.55165249, -0.31398847, -1.95969974, 0.479719…\n$ Arousal_Warr           <dbl> -1.216311211, -0.246556265, 2.290593302, 2.0988…\n$ Arousal_NRC            <dbl> -0.69572036, -1.21970928, 2.05381290, 2.2172073…\n$ Dominance_Warr         <dbl> 0.72011166, 0.13608080, -2.65664859, 0.79444286…\n$ Dominance_NRC          <dbl> -0.50032523, -0.39567615, -0.24178044, 1.543409…\n$ Sem_Diversity          <dbl> -0.1138539, -0.2220504, -0.2149318, 0.3821524, …\n$ Sem_N                  <dbl> -0.78192795, 0.33165527, 0.84139181, -0.7738980…\n$ Sem_N_D                <dbl> -0.98851407, 0.67208691, 0.79139050, 0.32954290…\n$ Sem_N_D_Taxonomic_N3   <dbl> -0.90022451, -0.63786127, 0.09023886, -0.461734…\n$ Sem_N_D_Taxonomic_N10  <dbl> -0.900538782, -0.663275990, 0.145287118, -0.429…\n$ Sem_N_D_Taxonomic_N25  <dbl> -0.90215340, -0.67273309, 0.20836289, -0.432637…\n$ Sem_N_D_Taxonomic_N50  <dbl> -0.9098510, -0.6805376, 0.2579465, -0.4333353, …\n$ Assoc_Freq_Token       <dbl> -0.4595315, -0.3343794, 0.2469727, -0.3908997, …\n$ Assoc_Freq_Type        <dbl> -0.5236722, -0.1066413, 0.1559338, -0.5236722, …\n$ Assoc_Freq_Token123    <dbl> -0.49485710, -0.33469345, 0.27293772, -0.445321…\n$ Assoc_Freq_Type123     <dbl> -0.5614632, -0.1818171, 0.6015796, -0.5192803, …\n$ Resnik_strength        <dbl> 0.40909889, 0.18206692, 0.12473608, -0.76972217…\n```\n\n\n:::\n:::\n\n\n## Examine correlations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove the verb labels from the first column\nscope_sem_df_cleaned <- scope_sem_df[,-1]\n\n# Generate correlation matrix\ncor_mat1 <- cor(scope_sem_df_cleaned)\n\n# Plot correlation matrix\ncorrplot(cor_mat1, col = topo.colors(200), tl.col = \"darkgrey\", number.cex = 0.5, tl.cex = 0.5)\n```\n\n::: {.cell-output-display}\n![](PCA_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Levelplot\nseq1 <- seq(-1, 1, by = 0.01)\n\nlevelplot(cor_mat1, aspect = \"fill\", col.regions = topo.colors(length(seq1)),\n          at = seq1, scales = list(x = list(rot = 45)),\n          xlab = \"\", ylab = \"\")\n```\n\n::: {.cell-output-display}\n![](PCA_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nNeedless to say, the above correlation matrices are hard to interpret –\neven more so if the number of variables were to increase further.\n\n**Principal Components Analysis** offers a technique to break down a\nhigh-dimensional dataset, into a much smaller set of \"meta-variables\", i.e., **principle components** (PCs) which capture the bulk of the variance in the data. This allows researchers to see overarching patterns in the data and re-use the output for further analysis\n(e.g., clustering or predictive modelling).\n\n## How does PCA work?\n\nPCA \"repackages\" large sets of variables by forming uncorrelated linear combinations of them, yielding $k$ principal components $Z_k$ of the dataset (for $k \\in \\{1, ..., K\\}$).\n\nEach PC comprises a set of **loadings** (or **weights**) $w_{nm}$, which are comparable to the coefficients of regression equations. For instance, the first PC has the general form shown in the equation below, where $x_m$ stand for continuous input variables in the $n \\times m$ data matrix.\n\n\\begin{equation} \\label{pca-1}\nZ_1 = w_{11}\\mathbf{x}_{1} + w_{21}\\mathbf{x}_{2} + \\dots + w_{m1}\\mathbf{x}_{m}\n\\end{equation}\n\nIf a feature positively loads on a principal component (i.e., $w > 0$), it means that as the value of this feature increases, the score for this principal component also increases. The magnitude of $w$ indicates the strength of this relationship. Conversely, negative loadings ($w < 0$) indicate that as the feature value increases, the PC score decreases as well.\n\nThe figure below offers a visual summary of PCA:\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](PCA_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n::: {.callout-tip title=\"How do you identify the principle components of a dataset?\" collapse=\"true\"}\n\nThe computation of PC loadings/weights relies on several techniques from matrix algebra, namely **singular value decomposition** (SVD) and **eigenvalue decomposition**, depending on the format of the input matrix.\n\nLet $\\mathbf{X}$ denote the $n \\times m$ matrix of input variables. Using SVD, it can be analysed into three elements:\n\n$$\n\\mathbf{X} = \\mathbf{UDV}^T\n$$\n\n\n\n\n\n\n\n:::\n\n\n",
    "supporting": [
      "PCA_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}