{
  "hash": "62ec5ab2ae883c6b7115d5915a69b145",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Principal Components Analysis\"\nauthor:\n  name: \"Vladimir Buskin\" \n  orcid: \"0009-0005-5824-1012\"\n  affiliation: \n    name: \"Catholic University of Eichstätt-Ingolstadt\"\n    department: \"English Language and Linguistics\"\nformat:\n  html:\n    self-contained: true\n    code-fold: false\n    theme: default\n    toc: true\n    toc-depth: 4\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 3\n    scrollable: true\neditor: visual\nbibliography: R.bib\nmetadata-files:\n  - _quarto.yml\ngoogle-scholar: false\n---\n\n\n## Recommended reading\n\nFor linguists:\n\n> Levshina [-@levshina_how_2015: Chapter 18]\n\nGeneral:\n\n> Mair [-@mairModernPsychometrics2018: Chapter 6]\n\n## Preparation {#sec-pca-prep}\n\nThis unit relies on psycholinguistic data from the [South Carolina\nPsycholinguistic\nMetabase](https://sc.edu/study/colleges_schools/artsandsciences/psychology/research_clinical_facilities/scope/)\n[@gaoSCOPESouthCarolina2022].[^pca-1] Detailed descriptions of the\nvariables can be found\n[here](https://sc.edu/study/colleges_schools/artsandsciences/psychology/research_clinical_facilities/scope/search.php).\n\n[^pca-1]: One exception is the variable\n    `Resnik_strength [`@resnikSelectionalConstraintsInformationtheoretic1996\\],\n    which was computed manually and appended to the data frame.\n\nThe data frame `scope_sem_df` contains semantic ratings for a sample of\n1,702 transitive verbs. Note that all columns have been standardised\n(cf. `?scale()` for details).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(lattice)\nlibrary(corrplot)\nlibrary(psych)\nlibrary(GPArotation)\nlibrary(gridExtra)\n\n# Load data\nscope_sem_df <- readRDS(\"scope_sem.RDS\")\n\n# Select subset\nscope_sem_sub <- scope_sem_df[,1:11]\n\n# Overview\nglimpse(scope_sem_sub)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,702\nColumns: 11\n$ Verb               <chr> \"abstain\", \"abstract\", \"abuse\", \"accelerate\", \"acce…\n$ Resnik_strength    <dbl> 0.40909889, 0.18206692, 0.12473608, -0.76972217, -1…\n$ Conc_Brys          <dbl> -0.94444378, -1.92983639, -0.59478833, 0.22107437, …\n$ Nsenses_WordNet    <dbl> -0.68843996, 0.27755219, 0.00155443, -0.68843996, 0…\n$ Nmeanings_Websters <dbl> -0.95559835, 0.73781281, 0.73781281, -0.27823388, 0…\n$ Visual_Lanc        <dbl> -2.2545455, 0.6103733, 1.3354358, -0.4342084, -0.34…\n$ Auditory_Lanc      <dbl> -0.84225787, -0.35605108, 1.54797548, 0.18795651, 1…\n$ Haptic_Lanc        <dbl> -0.75523987, -0.29089287, 1.25099360, -0.18911818, …\n$ Olfactory_Lanc     <dbl> -0.14444936, -0.37350419, -0.53335522, -0.37350419,…\n$ Gustatory_Lanc     <dbl> 0.27698988, -0.10105698, -0.36148925, -0.52110903, …\n$ Interoceptive_Lanc <dbl> 1.08153427, -0.06560311, 1.64313895, 1.45452985, 0.…\n```\n\n\n:::\n:::\n\n\n## Descriptive overview\n\nA popular descriptive measure for associations between continuous\nvariables $x$ and $y$ is the **Pearson product-moment correlation\ncoefficient** (or simply Pearson's $r$; cf. @eq-pearson). It varies on a\nscale from $-1$ to $1$ and indicates the extent to which two variables\nform a straight-line relationship [@heumann_introduction_2022: 153-154].\nOne of its core components is the **covariance** between $x$ and $y$\nwhich \"measures the average tendency of two variables to covary (change\ntogether)\" [@baguleySeriousStatsGuide2012: 206].\n\n$$\nr_{xy} = \\frac{Cov(x, y)}{\\sqrt{Var(x)}\\sqrt{Var(y)}}= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}.\n$$ {#eq-pearson}\n\nIn R, we can compute Pearson's $r$ by using the `cor()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check correlation between number of senses and concreteness\ncor(scope_sem_sub[,-1]$Nsenses_WordNet, scope_sem_sub[,-1]$Conc_Brys) # low\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2351554\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check correlation between haptic experience and concreteness\ncor(scope_sem_sub[,-1]$Haptic_Lanc, scope_sem_sub[,-1]$Conc_Brys) # high\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5676945\n```\n\n\n:::\n:::\n\n\nIf the data frame consists of numeric columns only (i.e., if it is a\n**matrix**), we can apply `cor()` to the full dataset and obtain the\n**correlation matrix** (also known as covariance matrix).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate correlation matrix\ncor_mat1 <- cor(scope_sem_sub[,-1])\n\nhead(cor_mat1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   Resnik_strength  Conc_Brys Nsenses_WordNet\nResnik_strength         1.00000000  0.1166670     -0.37442983\nConc_Brys               0.11666697  1.0000000      0.23515537\nNsenses_WordNet        -0.37442983  0.2351554      1.00000000\nNmeanings_Websters     -0.34225250  0.2023356      0.68509560\nVisual_Lanc             0.05471417  0.5519836      0.17154846\nAuditory_Lanc          -0.11162700 -0.2683646     -0.02960745\n                   Nmeanings_Websters Visual_Lanc Auditory_Lanc  Haptic_Lanc\nResnik_strength            -0.3422525  0.05471417   -0.11162700  0.008260683\nConc_Brys                   0.2023356  0.55198358   -0.26836458  0.567694470\nNsenses_WordNet             0.6850956  0.17154846   -0.02960745  0.239470104\nNmeanings_Websters          1.0000000  0.14597243   -0.04656650  0.193226862\nVisual_Lanc                 0.1459724  1.00000000   -0.11674896  0.404536416\nAuditory_Lanc              -0.0465665 -0.11674896    1.00000000 -0.289586292\n                   Olfactory_Lanc Gustatory_Lanc Interoceptive_Lanc\nResnik_strength        0.05131717    0.015087346      -9.459551e-02\nConc_Brys              0.21354305    0.123459754      -3.257702e-01\nNsenses_WordNet       -0.03353627   -0.018262178      -1.392934e-02\nNmeanings_Websters    -0.01898766    0.001409646      -4.460375e-05\nVisual_Lanc            0.15319007    0.055176064      -3.424087e-01\nAuditory_Lanc         -0.06123191   -0.047086877       1.799479e-01\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot correlation matrix\ncorrplot(cor_mat1, col = topo.colors(200), tl.col = \"darkgrey\", number.cex = 0.5, tl.cex = 0.5)\n```\n\n::: {.cell-output-display}\n![](PCA_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nSince the upper triangle mirrors the lower one, it is enough to only\nexamine one of them. The diagonal values are not particularly insightful\nand can be ignored.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Levelplot\nseq1 <- seq(-1, 1, by = 0.01)\n\nlevelplot(cor_mat1, aspect = \"fill\", col.regions = topo.colors(length(seq1)),\n          at = seq1, scales = list(x = list(rot = 45)),\n          xlab = \"\", ylab = \"\")\n```\n\n::: {.cell-output-display}\n![](PCA_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nNeedless to say, the above correlation matrices are hard to interpret –\neven more so if the number of variables were to increase further.\n\n**Principal Components Analysis** offers a technique to break down a\nhigh-dimensional dataset into a much smaller set of \"meta-variables\",\ni.e., **principle components** (PCs) which capture the bulk of the\nvariance in the data. This is also known as **dimension reduction**,\nwhich allows researchers to see overarching patterns in the data and\nre-use the output for further analysis (e.g., clustering or predictive\nmodelling).\n\n## Basics of PCA\n\nPCA \"repackages\" large sets of variables by forming uncorrelated linear\ncombinations of them, yielding $k$ principal components $Z_1, ..., Z_k$\n(PCs hf.) of the dataset (for $1, ..., k$). PCs are ordered such that\nthe first PC explains the most variance in the data, with each\nsubsequent PC explaining the maximum remaining variance while being\nuncorrelated with previous PCs.\n\nEach PC comprises a set of **loadings** (or **weights**) $w_{nm}$, which\nare comparable to the coefficients of regression equations. For\ninstance, the first PC has the general form shown in @eq-pca, where\n$x_m$ stand for continuous input variables in the $n \\times m$ data\nmatrix $\\mathbf{X}$.\n\n$$\nZ_{1} = w_{11}\n\\begin{pmatrix}\nx_{11} \\\\\nx_{21} \\\\\n\\vdots \\\\\nx_{n1}\n\\end{pmatrix}\n+ w_{21}\n\\begin{pmatrix}\nx_{12} \\\\\nx_{22} \\\\\n\\vdots \\\\\nx_{n2}\n\\end{pmatrix}\n+ \\dots + w_{m1}\n\\begin{pmatrix}\nx_{1m} \\\\\nx_{2m} \\\\\n\\vdots \\\\\nx_{nm}\n\\end{pmatrix}\n$$ {#eq-pca}\n\nIf a feature positively loads on a principal component (i.e., $w > 0$),\nit means that as the value of this feature increases, the score for this\nprincipal component also increases. The magnitude of $w$ indicates the\nstrength of this relationship. Conversely, negative loadings ($w < 0$)\nindicate that as the feature value increases, the PC score decreases as\nwell.\n\n::: {.callout-note collapse=\"true\" title=\"How do we find PCs?\"}\nPCs are identified using common techniques from matrix algebra, namely\n**singular value decomposition** and **eigenvalue decomposition**. By\nbreaking down the input data into products of several further matrices,\nit becomes possible to characterise the exact 'shape' of its variance\n[@mairModernPsychometrics2018: 181].\n:::\n\nThe figure below offers a visual summary of PCA:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](PCA_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Application in R\n\n### Fitting the model and identifying number of PCs\n\nFirst, we fit a PCA object with the number of PCs equivalent to the\nnumber of columns in `scope_sem_sub`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit initial PCA\npca1 <- principal(scope_sem_sub[,-1],\n                  nfactors = ncol(scope_sem_sub[,-1]),\n                  rotate = \"none\")\n\n# Print loadings\nloadings(pca1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLoadings:\n                   PC1    PC2    PC3    PC4    PC5    PC6    PC7    PC8   \nResnik_strength            0.666 -0.271 -0.100  0.250  0.627              \nConc_Brys           0.813  0.210 -0.173         0.149        -0.170 -0.260\nNsenses_WordNet     0.523 -0.696  0.124                0.241              \nNmeanings_Websters  0.493 -0.683  0.149                0.343              \nVisual_Lanc         0.691  0.168 -0.236  0.382  0.152 -0.127  0.484  0.136\nAuditory_Lanc      -0.388 -0.228  0.199  0.734  0.413        -0.208       \nHaptic_Lanc         0.728  0.113        -0.272  0.401 -0.254 -0.266  0.120\nOlfactory_Lanc      0.324  0.444  0.671  0.163 -0.200               -0.347\nGustatory_Lanc      0.256  0.377  0.759        -0.160                0.377\nInteroceptive_Lanc -0.341 -0.164  0.577 -0.366  0.543         0.265 -0.100\n                   PC9    PC10  \nResnik_strength                 \nConc_Brys          -0.378       \nNsenses_WordNet            0.405\nNmeanings_Websters        -0.373\nVisual_Lanc                     \nAuditory_Lanc                   \nHaptic_Lanc         0.266       \nOlfactory_Lanc      0.234       \nGustatory_Lanc     -0.187       \nInteroceptive_Lanc -0.120       \n\n                 PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10\nSS loadings    2.629 1.898 1.595 0.937 0.806 0.657 0.461 0.378 0.330 0.309\nProportion Var 0.263 0.190 0.160 0.094 0.081 0.066 0.046 0.038 0.033 0.031\nCumulative Var 0.263 0.453 0.612 0.706 0.787 0.852 0.898 0.936 0.969 1.000\n```\n\n\n:::\n:::\n\n\nIt is common practice to retain only those PCs with eigenvalues\n(variances) $> 1$ (cf. scree plot).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Scree plot\nbarplot(pca1$values, main = \"Scree plot\", ylab = \"Variances\", xlab = \"PC\", # first three PCs\n        names.arg = 1:length(pca1$values))\n  abline(h = 1, col = \"blue\", lty = \"dotted\")\n```\n\n::: {.cell-output-display}\n![](PCA_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nAlternatively, one can perform parallel analysis to identify\nstatistically significant PCs whose variances are \"larger than the 95%\nquantile \\[...\\] of those obtained from random or resampled data\"\n[@mairModernPsychometrics2018: 31]. The corresponding function is\n`fa.parallel()` from the `psych` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca.pa <- fa.parallel(scope_sem_sub[,-1], # raw data\n                     fa = \"pc\", # Use PCA instead of factor analysis\n                     cor = \"cor\",  # Use Pearson correlations (default for PCA)\n                     n.iter = 200, # Number of iterations (increase for more stable results)\n                     quant = 0.95, # Use 95th percentile (common choice)\n                     fm = \"minres\") # Factor method\n```\n\n::: {.cell-output-display}\n![](PCA_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParallel analysis suggests that the number of factors =  NA  and the number of components =  3 \n```\n\n\n:::\n:::\n\n\n### Accessing and visualising the loadings\n\nSince three PCs appear to be enough to explain the majority of variance\nin the data, we will refit the model with `nfactors = 3`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca2 <- principal(scope_sem_sub[,-1],\n                  nfactors = 3,\n                  rotate = \"none\")\n```\n:::\n\n\nA convenient function for printing the PCA loadings is `loadings()`.\nWeights close to $0$ are not displayed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloadings(pca2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLoadings:\n                   PC1    PC2    PC3   \nResnik_strength            0.666 -0.271\nConc_Brys           0.813  0.210 -0.173\nNsenses_WordNet     0.523 -0.696  0.124\nNmeanings_Websters  0.493 -0.683  0.149\nVisual_Lanc         0.691  0.168 -0.236\nAuditory_Lanc      -0.388 -0.228  0.199\nHaptic_Lanc         0.728  0.113       \nOlfactory_Lanc      0.324  0.444  0.671\nGustatory_Lanc      0.256  0.377  0.759\nInteroceptive_Lanc -0.341 -0.164  0.577\n\n                 PC1   PC2   PC3\nSS loadings    2.629 1.898 1.595\nProportion Var 0.263 0.190 0.160\nCumulative Var 0.263 0.453 0.612\n```\n\n\n:::\n:::\n\n\nIn order to see what features load particularly strongly on the PCs, we\ncan draw a path diagram with `diagram()`. Note that the red arrows\nindicate negative weights (i.e., negative \"regression coefficients\").\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiagram(pca2, main = NA)\n```\n\n::: {.cell-output-display}\n![](PCA_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThe generic `plot` method returns a scatterplot of the loadings:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(pca2, labels = colnames(scope_sem_sub[,-1]), main = NA)\n```\n\n::: {.cell-output-display}\n![](PCA_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nFinally, you can obtain the PC scores for each observation in the input\ndata by accessing the `$scores` element:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(pca2$scores, n = 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              PC1         PC2         PC3\n [1,] -1.45999990  0.38323657  0.61549383\n [2,] -0.32170158 -0.60027352 -0.08271852\n [3,]  0.12196548 -0.68757984  0.33959072\n [4,] -0.57929327 -0.35785887  0.26877126\n [5,] -0.34097381 -1.35963060  0.54717808\n [6,] -0.04799048 -0.34404820 -0.19668070\n [7,] -0.33873248  0.52372694 -0.28318588\n [8,] -1.11868861  0.26178424 -0.66465979\n [9,]  0.15263031  0.60489417 -0.84324699\n[10,] -1.75834143 -0.47957110  0.44313029\n[11,] -1.26440095 -1.15766536  0.46594800\n[12,]  0.10641410  0.05075197  0.48556702\n[13,] -1.26133394 -0.35022899 -0.36512925\n[14,] -0.28070472  0.60992380 -1.29547347\n[15,] -0.72805598 -0.45777808  0.56031788\n```\n\n\n:::\n:::\n\n\nBiplots offer juxtaposed visualisations of PC scores (points) and\nloadings (arrows).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# PC1 and PC2\nbiplot(pca2, choose = c(1, 2), \n       main = NA, pch = 20, col = c(\"darkgrey\", \"blue\"))\n```\n\n::: {.cell-output-display}\n![](PCA_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# PC2 and PC3\nbiplot(pca2, choose = c(2, 3), \n       main = NA, pch = 20, col = c(\"darkgrey\", \"blue\"))\n```\n\n::: {.cell-output-display}\n![](PCA_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n:::\n\n\n::: callout-note\n## Interpreting the PCA output\n\nAfter inspecting the loadings and biplots, we can see the following\npatterns:\n\n-   **External sensation**: Higher ratings in concreteness (i.e., direct\n    perception with one's senses) as well as the visual and haptic\n    dimensions of verbs are associated with an increase in PC1.\n\n-   **Senses and selection**: PC2 displays notable negative loadings in\n    features relating to the number of meanings a verb has and how much\n    information it carries about the meaning of its objects. PC2 scores\n    decrease if a verb has fewer meanings, but they increase if it\n    displays higher selectional preference strength.\n\n-   **Internal sensation**: PC3 captures variance in olfactory,\n    gustatory and interoceptive[^pca-2] ratings.\n:::\n\n[^pca-2]: Here *interoceptive* means \"\\[t\\]o what extent one experiences\n    the referent by sensations inside one's body\"\n    [@gaoSCOPESouthCarolina2022: 2859].\n",
    "supporting": [
      "PCA_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}