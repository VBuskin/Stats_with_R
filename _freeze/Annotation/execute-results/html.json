{
  "hash": "de5e9b54df6e3d9d1930f0f774ec1220",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Data annotation\"\ndate: \"November, 2024\"\nauthor:\n  name: \"Vladimir Buskin\" \n  orcid: \"0009-0005-5824-1012\"\n  affiliation: \n    name: \"Catholic University of Eichstätt-Ingolstadt\"\n    department: \"English Language and Linguistics\"\ncitation:\n  type: \"webpage\"\n  container-title: \"Statistics and Data Analysis for Corpus Linguists\"\n  url: \"https://your-book-url.com\" \n  accessed: \"2024-12-12\"\nformat:\n  html:\n    self-contained: true\n    theme: default\n    toc: true\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 3\n    scrollable: true\n    \neditor: visual\n---\n\n\n## Recommended reading\n\n> @gries_statistics_2013: Chapter 1.3.3\n\n## Sample study\n\n### Theoretical background\n\nLet's assume we are interested in the object realisation patterns of the\nverb *eat* in the British ICE component. A quick review of the\nliterature tells us that ...\n\n-   ... argument realisation may be related to the aspectual structure\n    of a verbal action [cf. @goldberg_patient_2001], but ...\n\n-   ... there is a stronger focus on situation aspect\n    (telicity/atelicity; i.e., logical endpoints of actions) than on\n    grammatical aspect (i.e., perfective/progressive).\n\nSince grammatical aspect is also concerned with the temporal construal\nof actions, it raises the question of whether or not it can also\ninfluence object realisation. To investigate the relationship between\naspect and object realisation, we will perform an exemplary analysis on\nthe verb lemma EAT.\n\n### Obtaining data\n\nWe load all necessary libraries to query the ICE-GB corpus and run a\nKWIC-search using the regular expression `\\\\b(eat(s|ing|en)?|ate)\\\\b`,\nwhich finds all inflection forms of EAT. We then store the results in a\nspreadsheet file `kwic_eat.xlsx`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load library and corpus\nlibrary(quanteda)\nlibrary(writexl)\n\nICE_GB <- readRDS(\"ICE_GB.RDS\")\n\n# Perform query\nkwic_eat <- kwic(ICE_GB,\n          phrase(\"\\\\b(eat(s|ing|en)?|ate)\\\\b\"),\n          valuetype = \"regex\",\n          window = 15)\n\n# Store results\nwrite_xlsx(kwic_eat, \"kwic_eat.xlsx\")\n```\n:::\n\n\nWhen you open `kwic_eat.xlsx` in a spreadsheet software, the file will\ncontain 7 columns by default (docname, from, to, keyword, post,\npattern). Each row corresponds to a match of your search expression in\nthe corpus, which is equal to 113 here. This is your **raw output**.\n\n![](kwic_eat.png){fig-align=\"center\"}\n\n## Data annotation\n\nWhenever you decide to work on your corpus results, it is good practice\nto duplicate your file and append the current date to the filename.\nRe-save it as, for instance, `kwic_eat_09_09_2024.xlsx` and open it\nagain. This way you're performing basic **version control**, which will\nallow you to return to previous stages of your analysis with ease.\n\nIn your spreadsheet software, you can now assign your variables of\ninterest to the empty columns next to your output data. For our specific\nexample, we will need one that captures object realisation and one the\ntype of verb aspect. Let's simply call them `object_realisation` and\n`verb_aspect`.\n\n::: callout-warning\n### Naming variables\n\nOf course, you could also opt for a different column name, as long it\nhas **no spaces** or **special characters** (e.g., !?%#). You could also\nname it `object_realisation` or, even more plainly, `object`, but\n**not** `direct object` or `object realisation` with spaces. Otherwise\nyou are bound to encounter a surge of cryptic error messages in your R\nconsole.\n:::\n\nNow, you are ready to annotate your data! An easy coding scheme would\ninvolve classifying rows where *eat* occurs with an object as `yes`.\nConversely, rows where the direct object is not realised syntactically\nare assigned the column value `no`. In the aspect column, verbal aspect\nwill be coded as either `perfective`, `progressive` or `neutral`,\nfollowing @gries2014: 118.\n\n![](kwic_eat_annotated2.png){fig-align=\"center\"}\n\n### Dealing with problematic cases\n\nHowever, things are not always that clear-cut. What if you encounter a\nfalse positive, i.e., an erroneous hit in your dataset? Further down in\nthe spreadsheet the keyword *ate* is actually part of the preceding\nword, *inappropriate*.\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:left;\"> docname </th>\n   <th style=\"text-align:right;\"> from </th>\n   <th style=\"text-align:right;\"> to </th>\n   <th style=\"text-align:left;\"> pre </th>\n   <th style=\"text-align:left;\"> keyword </th>\n   <th style=\"text-align:left;\"> post </th>\n   <th style=\"text-align:left;\"> pattern </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 113 </td>\n   <td style=\"text-align:left;\"> ICE_GB/W2F-019.txt </td>\n   <td style=\"text-align:right;\"> 696 </td>\n   <td style=\"text-align:right;\"> 696 </td>\n   <td style=\"text-align:left;\"> : 1 &gt; Too much colour on her face would be inappropri &lt; l &gt; </td>\n   <td style=\"text-align:left;\"> ate </td>\n   <td style=\"text-align:left;\"> , she feels , but she wears a light foundation . &lt; ICE-GB:W2F-019 #28 : </td>\n   <td style=\"text-align:left;\"> \\b(eat(s|ing|en)?|ate)\\b </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n::: callout-important\n### What do I do with false hits?\n\nShort answer: Do **not** delete irrelevant rows or columns. Essentially,\nfrom the moment you've obtained your corpus output, you should withstand\nthe temptation to delete anything from it. Instead, adopt the practice\nof indicating missing values or irrelevant rows by an `NA` in a separate\ncolumn. In later analyses, these can be easily filtered out!\n\nThis also minimises the risk of accidentally getting rid of data that\ncould have proven important at a later point in time.\n:::\n\n### Getting the data back into R\n\nImport the Excel file via\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load library\nlibrary(readxl)\n\n# Read file contents into the variable \"kwic_data\"\nkwic_data <- read_xlsx(\"kwic_eat_09_09_2024.xlsx\")\n\n# Print the first six lines of \"kwic_data\"\nprint(head(kwic_data))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n  docname  from    to pre   keyword post  pattern object_realisation aspect_verb\n  <chr>   <dbl> <dbl> <chr> <chr>   <chr> <chr>   <chr>              <chr>      \n1 ICE_GB…   458   458 had … eaten   anyw… \"\\\\b(e… no                 perfective \n2 ICE_GB…   478   478 : 1 … eating  will… \"\\\\b(e… no                 progressive\n3 ICE_GB…   785   785 > Ye… eat     befo… \"\\\\b(e… no                 neutral    \n4 ICE_GB…  1198  1198 the … eat     them… \"\\\\b(e… yes                neutral    \n5 ICE_GB…  4529  4529 > Ye… ate     in t… \"\\\\b(e… no                 neutral    \n6 ICE_GB…   958   958 know… eat     it f… \"\\\\b(e… yes                neutral    \n```\n\n\n:::\n:::\n\n\n### Adding a case list\n\n@gries_statistics_2013 recommends setting up the first column of the\ndata frame such that it \"numbers all n cases from $1$ to $n$ so that\nevery row can be uniquely identified and so that you always restore one\nparticular ordering (e.g., the original one)\" [@gries_statistics_2013:\n26]. This is very easy to do: We specify a numeric vector ranging from 1\nto the total number of rows in the data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a new Case column (which, by default, is moved to the very end of the data frame)\nkwic_data$Case <- 1:nrow(kwic_data)\n\n# Move the Case column to the front of the data frame\nlibrary(tidyverse)\n\nkwic_data <- relocate(kwic_data, Case)\n\n# Print reordered data frame\nprint(head(kwic_data))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 10\n   Case docname        from    to pre   keyword post  pattern object_realisation\n  <int> <chr>         <dbl> <dbl> <chr> <chr>   <chr> <chr>   <chr>             \n1     1 ICE_GB/S1A-0…   458   458 had … eaten   anyw… \"\\\\b(e… no                \n2     2 ICE_GB/S1A-0…   478   478 : 1 … eating  will… \"\\\\b(e… no                \n3     3 ICE_GB/S1A-0…   785   785 > Ye… eat     befo… \"\\\\b(e… no                \n4     4 ICE_GB/S1A-0…  1198  1198 the … eat     them… \"\\\\b(e… yes               \n5     5 ICE_GB/S1A-0…  4529  4529 > Ye… ate     in t… \"\\\\b(e… no                \n6     6 ICE_GB/S1A-0…   958   958 know… eat     it f… \"\\\\b(e… yes               \n# ℹ 1 more variable: aspect_verb <chr>\n```\n\n\n:::\n:::\n\n\n## Where do I go from here?\n\nAs soon as you've fully annotated your dataset and reviewed it for\npotential coding errors, the next step involves analysing your data\n**statistically** to uncover potential patterns that are not visible to\nthe naked eye. These patterns can include (subtle to major) differences\nin frequency of occurrence, relationships with other linguistically\nrelevant features (e.g., register/genre) or probabilities (e.g., the\nprobability that an object is omitted), among many others. The following\nunits offer an introduction to statistics, where \"statistics\" is best\nunderstood as \"a collection of methods which help us to describe,\nsummarize, interpret, and analyse data\" [@heumann_introduction_2022].\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}