{
  "hash": "e9a6914c06d502766f05fd13d9455a8b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Poisson regression\"\nauthor:\n  name: \"Vladimir Buskin\" \n  orcid: \"0009-0005-5824-1012\"\n  affiliation: \n    name: \"Catholic University of Eichstätt-Ingolstadt\"\n    department: \"English Language and Linguistics\"\nformat:\n  html:\n    self-contained: true\n    logo: logo.png\n    footer: \"Regression\"\n    theme: default\n    toc: true\n    number-sections: true\n    slide-number: true\n    incremental: false\n    slide-level: 4\n    scrollable: true\neditor: visual\nbibliography: R.bib\n---\n\n\n## Recommended reading\n\n> @winter_statistics_2020: Chapter 13\n>\n> @baguleySeriousStatsGuide2012: Chapter 17.5\n\n## Preparation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggeffects)\nlibrary(readxl)\nlibrary(car)\n\n# Load datasets\nverbs <- read_xlsx(\"winter_2020_visual.xlsx\")\n```\n:::\n\n\n## The Poisson family\n\nFrequency data is ubiquitous in corpus linguistics. Given its numeric\nnature, it seems tempting to model such data using [linear\nregression](Linear_regression.qmd), but doing so is bound to cause\nproblems. Partially owing to the fact that count data is always\npositive, the residuals more often than not deviate from normality and\nadditionally display non-constant variance. The figures below illustrate\nthese issues for a linear model fitted to simulated count data.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Poisson_regression_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nA probability distribution that is much better equipped for this special\ncase of discrete, yet numeric data is the **Poisson distribution**.\nAssuming a Poisson-distributed variable $X$, its exact shape is\ndetermined by a single parameter, $\\lambda$ (lambda), which is both its\nmean **and** and variance. In other words,\n\n$$\nX \\sim Pois(\\lambda).\n$$ {#eq-pois}\n\nIts probability mass function has a notable negative skew, which becomes\nless conspicuous for increasingly higher parameter values.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Poisson_regression_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Poisson regression\n\nRecall the linear regression @eq-multreg, where the dependent variable\n$Y$ was modelled as a function of the linear sum of predictor terms\n$\\beta_pX_p$ for $p$ independent variable.\n\nThe **Poisson model** is very similar to the linear regression model,\nwith the main differences being the logarithmic transformation of the\nresponse and the exclusion of the error term:\n\n$$ \\ln(Y) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p.\n$$ {#eq-pois-full}\n\nTo remove the logarithm and obtain the model output on a more intuitive\nscale, we simply exponentiate both sides:\n\n$$ Y = e^{\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p}.\n$$ {#eq-pois-exp}\n\n### Application in R\n\nThe data provided by Winter [-@winter_statistics_2020] contains\nfrequency data for hundreds of verbs (`Freq` column) as well as a\nvariety of psycholinguistic ratings (`Sight`, `Touch`, `Sound` etc.),\nwhich were originally compiled by Lynott & Connell\n[-@LynottConnell2009].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(verbs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n  Word      DominantModality Sight Touch Sound  Taste  Smell Log10Freq  Freq\n  <chr>     <chr>            <dbl> <dbl> <dbl>  <dbl>  <dbl>     <dbl> <dbl>\n1 abrasive  Haptic            2.89 3.68  1.68  0.579  0.579      1.36     23\n2 absorbent Visual            4.14 3.14  0.714 0.476  0.476      0.903     8\n3 aching    Haptic            2.05 3.67  0.667 0.0476 0.0952     2.02    105\n4 acidic    Gustatory         2.19 1.14  0.476 4.19   2.90       1.04     11\n5 acrid     Olfactory         1.12 0.625 0.375 3      3.5        0.301     2\n6 adhesive  Haptic            3.67 4.33  1.19  0.905  1.76       1.67     47\n```\n\n\n:::\n:::\n\n\nWe will examine of the effects of different sensory ratings on the\nfrequency of a verb. When fitting the generalized linear model, it is\nimportant to indicate the argument `family = \"poisson\"` to apply the\ncorrect (logarithmic) link function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit Poisson regression model\nfreq.m1 <- glm(Freq ~ Sight + Touch + Sound + Taste + Smell, data = verbs, family = \"poisson\")\n\n# Summarise model statistics\nsummary(freq.m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Freq ~ Sight + Touch + Sound + Taste + Smell, family = \"poisson\", \n    data = verbs)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-84.27  -51.06  -30.39   -8.21  606.43  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  3.1201624  0.0118785  262.67   <2e-16 ***\nSight        0.8726608  0.0024550  355.46   <2e-16 ***\nTouch        0.1490792  0.0009801  152.10   <2e-16 ***\nSound        0.1692741  0.0011662  145.15   <2e-16 ***\nTaste        0.0790246  0.0019106   41.36   <2e-16 ***\nSmell       -0.0963918  0.0019746  -48.82   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1890910  on 361  degrees of freedom\nResidual deviance: 1633858  on 356  degrees of freedom\n  (61 observations deleted due to missingness)\nAIC: 1636345\n\nNumber of Fisher Scoring iterations: 7\n```\n\n\n:::\n:::\n\n\nThe model has identified numerous significant effects, i.e.,\n$\\beta$-values that are significantly different from 0. The low standard\nerrors hint at very robust estimates, resulting in 95% confidence\nintervals that are barely visible in the effect plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predicted values for each predictor\npred_sight <- ggpredict(freq.m1, terms = \"Sight\")\npred_touch <- ggpredict(freq.m1, terms = \"Touch\")\npred_sound <- ggpredict(freq.m1, terms = \"Sound\")\npred_taste <- ggpredict(freq.m1, terms = \"Taste\")\npred_smell <- ggpredict(freq.m1, terms = \"Smell\")\n\n# For plotting individual predictions, simply use:\n# plot(pred_sight) \n\n# Combine all predictions into one data frame\npred_all <- rbind(\n  data.frame(pred_sight, predictor = \"Sight\"),\n  data.frame(pred_touch, predictor = \"Touch\"),\n  data.frame(pred_sound, predictor = \"Sound\"),\n  data.frame(pred_taste, predictor = \"Taste\"),\n  data.frame(pred_smell, predictor = \"Smell\")\n)\n\n# Create faceted plot\nggplot(pred_all, aes(x = x, y = predicted)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.5) +\n  facet_wrap(~predictor, scales = \"free_x\") +\n  labs(x = \"Rating\", y = \"Predicted verb frequency\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](Poisson_regression_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n### Interpretation\n\nAs we can see, verbs with higher `Sight` ratings (i.e., highly visual\nverbs) are associated with the greatest increase in frequency of\noccurrence. Given a one-unit increase in $X$, we can obtain the\npercentage increase (or decrease, respectively) using the formula\n$100(e^b - 1)$. The predictor `Sight` has an estimate of approximately\n$0.87$ ($p< 0.001$, 95% CI \\[0.87, 0.88\\]), so the proportional increase\nis $100(e^{0.87} - 1) = 138.69\\%$.\n\nBy contrast, `Smell` is a associated with a lower frequency:\n$100(e^{0.10} -1) = -9.51\\%$ is the drop in frequency for each one-unit\nincrease in smell ratings. In essence: Smelly verbs are unpopular!\n",
    "supporting": [
      "Poisson_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}