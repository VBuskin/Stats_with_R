---
title: "Hypothesis testing"
author:
  name: "Vladimir Buskin" 
  orcid: "0009-0005-5824-1012"
  affiliation: 
    name: "Catholic University of EichstÃ¤tt-Ingolstadt"
    department: "English Language and Linguistics"
format:
  html:
    self-contained: true
    code-fold: false
    theme: default
    toc: true
    toc-depth: 4
    number-sections: true
    slide-number: true
    incremental: false
    slide-level: 3
    scrollable: true
editor: visual
bibliography: R.bib
---

## Suggested reading

For linguists:

> @gries_statistics_2021: Chapter 1.3.2

General:

> @baguleySeriousStatsGuide2012: Chapter 4
>
> @agrestiFoundationsStatisticsData2022: Chapter 5

## Hypothesis testing {#sec-hyp}

The null hypothesis significance testing (NHST) framework offers
researchers a convenient way of testing theoretical assumptions about a
population of interest. This chiefly involves setting up a set of
falsifiable statistical hypotheses, gathering evidence from the observed
data and computing the (in)famous '$p$-value' to determine "statistical
significance" -- a notion that is frequently misinterpreted in
scientific studies

This involves setting up a set of falsifiable statistical hypotheses
that predict the presence or absence of certain patterns in the data.
These are known as the **null hypothesis** and the **alternative
hypothesis**. They are set up **before** seeing the data and justified
by previous research. Association tests have the following components:

-   The **null hypothesis** $H_0$ could be viewed describing the
    "default state of the world" [@james_introduction_2021: 555], which
    suggests that there is no noteworthy effect to be observed. If we
    are interested in some property $\theta$ of the population (also
    known as a **population parameter**) (e.g., frequency differences
    between observed or expected counts; differences in mean reaction
    times between groups etc.), then it would have the value 0 under
    $H_0$.

-   The **alternative hypothesis** $H_1$ (or $H_a$) plainly states that
    the null hypothesis is false: There is an effect, i.e.,
    $H_1: \theta \neq 0$. The parameter thus takes values in an
    alternative range.

-   In the NHST approach, researchers typically seek evidence against
    $H_0$. To quantify the amount of evidence the observed data
    provides, specific **test statistics** are computed depending on the
    data type at hand; for instance, $\chi^2$ ('chi-squared') is typical
    for discrete and $t$ for continuous data.

-   The final rejection of $H_0$ is determined by the **significance
    probability** $p$, which denotes the probability that "the test
    statistic equals the observed value or a value even more extreme in
    the direction predicted by $H_a$"
    [@agrestiFoundationsStatisticsData2022: 163]", assuming $H_0$ is
    true.

    -   If $p$ is lower than a pre-defined threshold (typically 0.05),
        also known as the **significance level** $\alpha$, we reject
        $H_0$. However, if $p \geq$ 0.05, this **neither justifies
        rejecting nor accepting** the null hypothesis
        [@baguleySeriousStatsGuide2012: 121].

    -   For example, a $p$-value of $0.02$ means that we would see a
        particular test statistic, which often reflect usage patterns in
        linguistic data, only 2% of the time if $X$ and $Y$ were
        unrelated (i.e., if $H_0$ were true). Since $0.02$ lies below
        our significance level $\alpha$ = $0.05$, this would suggest a
        statistically significant relationship between $X$ and $Y$, and
        we could therefore reject $H_0$.

### Some examples of hypotheses

**Categorical variables**: We are interested in finding out whether
English clause `ORDER` ('sc-mc' or 'mc-sc') depends on the type of the
subordinate clause (`SUBORDTYPE`), which can be either temporal ('temp')
or causal ('caus').

Our hypotheses are:

-   $H_0:$ The variables `ORDER` and `SUBORDTYPE` are independent.

-   $H_1:$ The variables `ORDER` and `SUBORDTYPE` are **not**
    independent.

**Continuous variables**: As part of a phonetic study, we compare the
base frequencies of the F1 formants of vowels (in Hz) for male and
female speakers of Apache. We forward the following hypotheses:

-   $H_0:$ mean `F1 frequency` of men $=$ mean `F1 frequency` of women.

-   $H_1:$ mean `F1 frequency` of men $\ne$ mean `F1 frequency` of
    women.

### Hypothesis testing -- what could go wrong?

There is always a chance that we accept or reject the wrong hypothesis;
the four possible constellations are summarised in the table below [cf.
@heumann_introduction_2022: 223]:

|                           | $H_0$ is true                                       | $H_0$ is not true                                   |
|-------------------|---------------------------|---------------------------|
| $H_0$ **is not rejected** | $\color{green}{\text{Correct decision}}$            | $\color{red}{\text{Type II } (\beta)\text{-error}}$ |
| $H_0$ **is rejected**     | $\color{red}{\text{Type I } (\alpha)\text{-error}}$ | $\color{green}{\text{Correct decision}}$            |

### Advanced: Where does the $p$-value come from?

Let's say that the statistical analysis of clause `ORDER` and
`SUBORDTYPE` has returned a test statistic of $\chi^2 = 6.5$ for 2 $df$.
In order to compute the probability $P(\chi^2 \geq 5)$, we need to
consult the **sampling distribution** of this test statistic. The
sampling distribution is a probability distribution that assigns
probabilities to the values of a test statistic.

The probability density function $f(x)$ of the $\chi^2$-distribution for
2 degrees of freedom (abbreviated $df$ and affect the function's shape)
is visualised below. The $p$-value corresponds to the green area under
curve ranging from $x = 6.5$ up to $\infty$.

```{r, echo = FALSE}
# Load ggplot2
library(ggplot2)

# Define the degrees of freedom
df <- 2

# Create a sequence of x values
x <- seq(0, 30, length.out = 1000)

# Compute the chi-squared density
y <- dchisq(x, df = df)

# Create a data frame
chi_squared_data <- data.frame(x = x, y = y)

# Define the range for highlighting
highlight_data <- subset(chi_squared_data, x >= 6.5 & x <= 20)

# Generate the plot
ggplot(chi_squared_data, aes(x = x, y = y)) +
  geom_line(color = "steelblue", size = 1) + # Line for the density curve
  geom_area(data = highlight_data, aes(x = x, y = y), 
            fill = "darkgreen", alpha = 0.5) + # Highlighted area
  labs(
    title = "Chi-Squared Distribution",
    subtitle = "Probability density function with 2 degrees of freedom",
    x = "Chi-squared value",
    y = "Probability density"
  ) +
  theme_minimal() +
  coord_cartesian(ylim = c(0, 0.05), xlim = c(0, 30)) +
  theme(
    plot.title = element_text(hjust = 0.5),
    axis.title = element_text(size = 12)
  )

```

The probability $P(\chi^2 \geq 6.5)$ can be obtained by integrating over
all $\chi^2$-values in the range $0 \leq X < 6.5$ and subtracting them
from 1, which represents the total area under the curve

$$
P(X\geq 6.5) = 1 - \int_{0}^{6.5} f(x) dx
$$

```{r}
# For a chi-squared test statistic of 6.5 with 2 degree of freedom
p_value <- 1 - pchisq(6.5, df = 2)

print(p_value) # significant!
```
