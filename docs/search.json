[
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Statistics and Data Analysis for Corpus Linguists: From Theory to Practice with R",
    "section": "Preface",
    "text": "Preface\nThis collection of handouts provides a hands-on introduction to data analysis and statistical methods in quantitative corpus linguistics with R. It is designed with accessibility in mind, assuming no prior knowledge of programming or statistics. All you need to get started is a laptop; everything else will be explained within these pages.\nPrimarily, this reader is geared towards students attending the classes Language Variation (BA) and Statistics for Linguistics (MA) at the Catholic University of Eichstätt-Ingolstadt (Germany). However, it is also meant to equip students currently working on their BA/MA/PhD theses with the tools they need to conduct empirical studies on a wide array of linguistic phenomena. The methods presented here reflect the state-of-the-art in corpus-linguistic research, providing readers with current and relevant analytical techniques.",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Statistics and Data Analysis for Corpus Linguists: From Theory to Practice with R",
    "section": "Overview",
    "text": "Overview\nWe begin by establishing the general structure of corpus-linguistic studies, accompanied by key theoretical and practical considerations. The second section introduces R as an analytical tool, covering its core functionality. With these fundamentals in place, we proceed to query corpora directly within R. Chapters four and five focus on describing discrete and continuous outputs, as well as identifying meaningful associations and differences in the data (BA). Finally, to gain more nuanced insights into potential patterns, we apply common machine learning algorithms to fit, evaluate, and interpret statistical models (MA).\nThroughout this journey, readers will develop the skills to conduct robust corpus-linguistic analyses, from basic querying to advanced statistical modeling.",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#collaborators",
    "href": "index.html#collaborators",
    "title": "Statistics and Data Analysis for Corpus Linguists: From Theory to Practice with R",
    "section": "Collaborators",
    "text": "Collaborators\nVladimir Buskin, PhD student at the Department of English Language and Linguistics, Catholic University of Eichstätt-Ingolstadt\nThomas Brunner, Assistant Professor at the Department of English Language and Linguistics, Catholic University of Eichstätt-Ingolstadt\nPhilippa Adolf, PhD student at the Department of Romance Studies, University of Vienna",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "Preface"
    ]
  },
  {
    "objectID": "Distributions.html#suggested-reading",
    "href": "Distributions.html#suggested-reading",
    "title": "17  Probability distributions",
    "section": "17.1 Suggested reading",
    "text": "17.1 Suggested reading\n\nBaguley (2012): Chapter 2\nAgresti and Kateri (2022): Chapter 2\nHeumann, Schomaker, and Shalabh (2022): Chapter 8",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "Distributions.html#continuous-distributions",
    "href": "Distributions.html#continuous-distributions",
    "title": "17  Probability distributions",
    "section": "17.2 Continuous distributions",
    "text": "17.2 Continuous distributions\n\n17.2.1 The normal distribution\nA great number of numerical variables in the world follow the well-known normal (or Gaussian) distribution, which includes test scores, weight and height, among many others. The plot below illustrates its characteristic bell-shape: Most observations are in the middle, with considerably fewer near the fringes. For example, most people are rather “average” in height; there are only few people that are extremely short or extremely tall.\n\n\n\n\n\n\n\n\n\nThe normal distribution is typically described in terms of two parameters: The population mean \\(\\mu\\) and the population standard deviation \\(\\sigma\\). If a random variable \\(X\\) is normally distributed, we typically use the notation in Equation 17.1.\n\\[ X \\sim N(\\mu, \\sigma^2).\n\\tag{17.1}\\]\nThe Gaussian bell curve has several interesting properties:\n\n68% all values fall within one standard deviation of the mean,\n95% within two, and\n99.7% within three.\n\nBelow, you can see the annotated probability density function (PDF) of the normal distribution. The \\(y\\)-axis indicates the density of population values; note that since the Gaussian distribution is a continuous distribution, the probability of any given value is 0. We can only obtain probabilities for intervals of values, which are given by\n\\[\nP(a \\leq X \\leq b) = \\int_a^b f(x)dx.\n\\tag{17.2}\\]\n\n\n\n\n\n\n\n\n\nWe can find the population mean of \\(X\\) with a PDF \\(f(x)\\) via\n\\[\nE(X) = \\mu = \\int_x xf(x)dx,\n\\tag{17.3}\\]\nwhere \\(E(X)\\) denotes the expected value of \\(X\\), i.e., the mean. Essentially, multiplying every value \\(x\\) by its respective probability density \\(f(x)\\) and integrating over all possible values of \\(x\\) will return \\(E(X) = \\mu\\).",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "Distributions.html#discrete-distributions",
    "href": "Distributions.html#discrete-distributions",
    "title": "17  Probability distributions",
    "section": "17.3 Discrete distributions",
    "text": "17.3 Discrete distributions\n\n17.3.1 Bernoulli distribution\nThe Bernoulli distribution is a discrete probability distribution for random variables which have only two possible outcomes: “positive” (often coded as 1) and “negative” (often coded as 0). Examples of such variables include coin tosses (heads/tails), binary response questions (yes/no), and defect status (defective/non-defective).\nIf a random variable \\(X\\) follows a Bernoulli distribution, it is determined by the parameter \\(p\\), which is the probability of the positive case:\n\\[ X \\sim Bernoulli(p).\\] The probability mass function (PMF) of the Bernoulli distribution is given by: \\[\nP(X = x) =\n\\begin{cases}\np & \\text{if } x = 1 \\\\\n1 - p & \\text{if } x = 0\n\\end{cases}\n\\]\nwhere \\(0 \\leq p \\leq 1\\). This function shows the probability of \\(X\\) taking on the value of 1 or 0 (cf. Heumann, Schomaker, and Shalabh 2022: 162-163).\n\n\n\n\n\n\n\n\n\n\n\n17.3.2 Binomial Distribution\nThe binomial distribution models the number of successes in \\(n\\) independent Bernoulli trials, each with probability \\(p\\) of success. If a random variable \\(X\\) follows a binomial distribution with parameters \\(n\\) and \\(p\\), we write:\n\\[ X \\sim Binomial(n,p) \\]\nThe probability mass function (PMF) for the binomial distribution is:\n\\[ P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k} \\]\nwhere:\n\n\\(n\\) is the number of trials\n\\(k\\) is the number of successes \\((0 \\leq k \\leq n)\\)\n\\(p\\) is the probability of success on each trial \\((0 \\leq p \\leq 1)\\)\n\\(\\binom{n}{k}\\) is the binomial coefficient (“n choose k”)\n\n\n\n17.3.3 Multinomial Distribution\nThe multinomial distribution is a generalisation of the binomial distribution to the case where each trial has \\(k\\) possible outcomes (rather than just two). If a random variable \\(\\mathbf{X} = (X_1,...,X_k)\\) follows a multinomial distribution, we write:\n\\[ \\mathbf{X} \\sim Multinomial(n,\\mathbf{p}) \\]\nThe probability mass function (PMF) for the multinomial distribution is:\n\\[ P(\\mathbf{X} = \\mathbf{x}) = \\frac{n!}{x_1!x_2!...x_k!} p_1^{x_1}p_2^{x_2}...p_k^{x_k} \\]\nwhere:\n\n\\(n\\) is the number of trials\n\\(\\mathbf{x} = (x_1,...,x_k)\\) represents the number of occurrences of each outcome\n\\(\\mathbf{p} = (p_1,...,p_k)\\) represents the probabilities of each outcome\n\\(\\sum_{i=1}^k x_i = n\\) and \\(\\sum_{i=1}^k p_i = 1\\)",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "Distributions.html#sampling-distributions",
    "href": "Distributions.html#sampling-distributions",
    "title": "17  Probability distributions",
    "section": "17.4 Sampling Distributions",
    "text": "17.4 Sampling Distributions\nSampling distributions describe the distribution of statistics calculated from repeated random samples. Here are the most common ones:\n\n17.4.1 Chi-squared Distribution\nThe chi-squared distribution with \\(k\\) degrees of freedom, denoted \\(\\chi^2(k)\\), is the distribution of the sum of squares of \\(k\\) independent standard normal random variables. Its probability density function (PDF) is:\n\\[ f(x) = \\frac{x^{k/2-1}e^{-x/2}}{2^{k/2}\\Gamma(k/2)} \\]\nwhere:\n\n\\(x &gt; 0\\)\n\\(k\\) is the degrees of freedom\n\\(\\Gamma\\) is the gamma function\n\n\n\n17.4.2 Student’s t-Distribution\nThe Student’s t-distribution with \\(\\nu\\) degrees of freedom has the following PDF:\n\\[ f(x) = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\sqrt{\\nu\\pi}\\Gamma(\\frac{\\nu}{2})}\\left(1+\\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}} \\]\nwhere:\n\n\\(-\\infty &lt; x &lt; \\infty\\)\n\\(\\nu &gt; 0\\) is the degrees of freedom\n\\(\\Gamma\\) is the gamma function\n\nKey properties:\n\nAs \\(\\nu \\to \\infty\\), the t-distribution approaches the standard normal distribution\nThe distribution is symmetric about 0\nThe variance is \\(\\frac{\\nu}{\\nu-2}\\) for \\(\\nu &gt; 2\\)\n\n\n\n17.4.3 F-Distribution\nThe F-distribution with parameters \\(d_1\\) and \\(d_2\\) (degrees of freedom) has the following PDF:\n\\[ f(x) = \\frac{\\sqrt{\\frac{(d_1x)^{d_1}d_2^{d_2}}{(d_1x+d_2)^{d_1+d_2}}}}{x\\beta(\\frac{d_1}{2},\\frac{d_2}{2})} \\]\nwhere:\n\n\\(x &gt; 0\\)\n\\(d_1, d_2 &gt; 0\\) are the degrees of freedom\n\\(\\beta\\) is the beta function\n\nThe F-distribution is commonly used in:\n\nAnalysis of Variance (ANOVA)\nTesting equality of variances\nModel comparison in regression analysis\n\nEach of these sampling distributions plays a crucial role in statistical inference:\n\nChi-squared: Used for goodness-of-fit tests and tests of independence\nt-distribution: Used for inference about means when population variance is unknown\nF-distribution: Used for comparing variances and in ANOVA procedures\n\n\n\n\n\nAgresti, Alan, and Maria Kateri. 2022. Foundations of Statistics for Data Scientists: With r and Python. Boca Raton: CRC Press.\n\n\nBaguley, Thomas. 2012. Serious Stats: A Guide to Advanced Statistics for the Behavioral Sciences. Houndmills, Basingstoke: Palgrave Macmillan.\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "Categorical_data.html#preparation",
    "href": "Categorical_data.html#preparation",
    "title": "15  Categorical data",
    "section": "15.1 Preparation",
    "text": "15.1 Preparation\nPlease download the file “Paquot_Larsson_2020_data.xlsx” (Paquot and Larsson 2020)1 and store it in the same folder as your currently active R-script. Then run the code lines below:\n1 The original supplementary materials can be downloaded from the publisher’s website [Last accessed April 28, 2024].\n# Libraries\nlibrary(\"readxl\") # for loading Excel data\nlibrary(\"tidyverse\") # data manipulation and visualisation\n\n# Load data from working directory\ncl.order &lt;- read_xlsx(\"Paquot_Larsson_2020_data.xlsx\")",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Categorical data</span>"
    ]
  },
  {
    "objectID": "Categorical_data.html#frequency-tables",
    "href": "Categorical_data.html#frequency-tables",
    "title": "15  Categorical data",
    "section": "15.2 Frequency tables",
    "text": "15.2 Frequency tables\n\n15.2.1 One variable\nEach categorical variable in the data frame is made up of two or more categories. A simple descriptive measure is the frequency of each category. The table below indicates how often each clause order occurs in the ORDER column.\n\norder_freq1 &lt;- table(cl.order$ORDER) \n\nprint(order_freq1)\n\n\nmc-sc sc-mc \n  275   128 \n\n\nThe notation cl.order$ORDER subsets the cl.order according to the column ORDER (see data frames).\nAlternatively, you could use xtabs() to achieve the same result – perhaps with a slightly more intuitive syntax.\n\norder_freq2 &lt;- xtabs(~ ORDER, cl.order)\n\nprint(order_freq2)\n\nORDER\nmc-sc sc-mc \n  275   128 \n\n\n\n\n15.2.2 Two or more variables\nIf we are interested in the relationship between multiple categorical variables, we can cross-tabulate the frequencies of their categories. For example, what is the distribution of clause order depending on the type of subordinate clause? The output is also referred to as a contingency table.\n\norder_counts1 &lt;- table(cl.order$ORDER, cl.order$SUBORDTYPE)\n\nprint(order_counts1)\n\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n\n\nHere is the xtabs() alternative:\n\norder_counts2 &lt;- xtabs(~ ORDER + SUBORDTYPE, cl.order)\n\nprint(order_counts2)\n\n       SUBORDTYPE\nORDER   caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n\n\n\n\n\n\n\n\nHow do I obtain percentage tables?\n\n\n\n\n\nThere are two ways to convert the raw frequency counts to percentage tables:\n\nManually divide all cells by the total number of observations (which correspond to the sum of all cells) and multiply the result by 100.\n\n\npct1 &lt;- order_counts1/sum(order_counts1) * 100\n\n\nUse the prop.table() function and multiply the result by 100.\n\n\npct2 &lt;- prop.table(order_counts1) * 100",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Categorical data</span>"
    ]
  },
  {
    "objectID": "Categorical_data.html#plotting-categorical-data",
    "href": "Categorical_data.html#plotting-categorical-data",
    "title": "15  Categorical data",
    "section": "15.3 Plotting categorical data",
    "text": "15.3 Plotting categorical data\nThis section demonstrates both the in-built plotting functions of R (‘Base R’) as well as the more modern versions provided by the tidyverse package.\n\n15.3.1 Moscaiplot\nA straightforward way to visualise a contingency table is the mosaicplot:\n\nmosaicplot(order_counts2, color = TRUE)\n\n\n\n\n\n\n\n\n\n\n15.3.2 Barplots\nThe workhorse of categorical data analysis is the barplot. Base R functions usually require a table object as input, whereas ggplot2 can operate on the raw dataset.\n\n\n15.3.3 One variable\n\nBase Rggplot2\n\n\n\nBase R barplot with barplot(); requires the counts as computed by tables() or xtabs()\n\n\norder_freq1 &lt;- table(cl.order$ORDER)\n\nbarplot(order_freq1)\n\n\n\n\n\n\n\n\n\n\n\nBarplot with geom_bar() using the raw input data\n\n\nlibrary(tidyverse)\n\nggplot(cl.order, aes(x = ORDER)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.3.4 Two variables\n\nBarplots with the fill argument\n\n\nBase RBase R (fully customised)ggplot2ggplot2 (fully customised)\n\n\n\norder_counts2 &lt;- xtabs(~ ORDER + SUBORDTYPE, cl.order)\n\nbarplot(order_counts2, \n        beside = TRUE,  # Make bars side-by-side\n        legend = TRUE)  # Add a legend\n\n\n\n\n\n\n\n\n\n\n\norder_counts2 &lt;- xtabs(~ ORDER + SUBORDTYPE, cl.order)\n\nbarplot(order_counts2, \n        beside = TRUE,  # Make bars dodged (i.e., side by side)\n        main = \"Distribution of ORDER by SUBORDTYPE (Base R)\", \n        xlab = \"ORDER\", \n        ylab = \"Frequency\", \n        col = c(\"lightblue\", \"lightgreen\"), # Customize colors\n        legend = TRUE,  # Add a legend\n        args.legend = list(title = \"SUBORDTYPE\", x = \"topright\"))\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\nggplot(cl.order, aes(x = ORDER, fill = SUBORDTYPE)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\nggplot(cl.order, aes(x = ORDER, fill = SUBORDTYPE)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    title = \"Clause order by subordinate clause type\",\n    x = \"Clause order\",\n    y = \"Frequency\",\n    fill = \"Type of subordinate clause\"\n  ) +\n  theme_bw()",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Categorical data</span>"
    ]
  },
  {
    "objectID": "Categorical_data.html#plotting-percentages",
    "href": "Categorical_data.html#plotting-percentages",
    "title": "15  Categorical data",
    "section": "15.4 Plotting percentages",
    "text": "15.4 Plotting percentages\n\n\n\n\n\n\nHow do I plot percentages in Base R?\n\n\n\n\n\nIn very much the same way as with the raw counts:\n\nbarplot(pct1, \n        beside = TRUE,  # Make bars side-by-side\n        legend = TRUE)  # Add a legend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow do I plot percentages with ggplot2?\n\n\n\n\n\nHere, a few tweaks are necessary. In general, the ggplot() function prefers to works with data frames rather than cross-tables, so we’ll have to coerce it into one first:\n\npct1_df &lt;- as.data.frame(pct1)\n\nprint(pct1_df)\n\n   Var1 Var2      Freq\n1 mc-sc caus 45.657568\n2 sc-mc caus  3.722084\n3 mc-sc temp 22.580645\n4 sc-mc temp 28.039702\n\n\nNow we can plot the percentages with geom_col(). This geom (= ‘geometric object’) allows us to manually specify what should be mapped onto the y-axis:\n\nlibrary(tidyverse)\n\nggplot(pct1_df, aes(x = Var1, y = Freq, fill = Var2)) +\n  geom_col(position = \"dodge\")",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Categorical data</span>"
    ]
  },
  {
    "objectID": "Categorical_data.html#exporting-tables-to-ms-word",
    "href": "Categorical_data.html#exporting-tables-to-ms-word",
    "title": "15  Categorical data",
    "section": "15.5 Exporting tables to MS Word",
    "text": "15.5 Exporting tables to MS Word\nThe flextable package greatly facilitates generating publication-ready tables in R. It provides the best output if used on a table created by xtabs(). The full guide to this package can be found here.\n\n# Load library after installation\nlibrary(flextable)\n\n# Create a table\ntab1 &lt;- xtabs(~ ORDER + SUBORDTYPE, cl.order)\n\n# Convert a table to a flextable with as_flextable()\noutput_1 &lt;- as_flextable(tab1)\n\n# Print output\nprint(output_1)\n\n\nUnfortunately, the output cannot really be customised. However, if you’d like to add some further options, the crosstable package provides a remedy. In fact, it is even easier to use as it doesn’t require you to compute any tables beforehand.\n\n# Required libraries\nlibrary(crosstable)\nlibrary(flextable)\n\n# Create the cross table\noutput2 &lt;- crosstable(cl.order, ORDER, by = SUBORDTYPE, \n                 total = \"both\",\n                 percent_digits = 2)\n\nas_flextable(output2)\n\nlabelvariableSUBORDTYPETotalcaustempORDERmc-sc184 (66.91%)91 (33.09%)275 (68.24%)sc-mc15 (11.72%)113 (88.28%)128 (31.76%)Total199 (49.38%)204 (50.62%)403 (100.00%)\n\n\n\n\n\n\nPaquot, Magali, and Tove Larsson. 2020. “Descriptive Statistics and Visualization with r.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Thomas Gries, 375–99. Cham: Springer.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Categorical data</span>"
    ]
  },
  {
    "objectID": "Concordancing.html#suggested-reading",
    "href": "Concordancing.html#suggested-reading",
    "title": "11  Concordancing",
    "section": "11.1 Suggested reading",
    "text": "11.1 Suggested reading\nIn-depth introduction to concordancing with R:\n\nSchweinberger (2024)\n\nNaturale Language Processing (NLP) with quanteda:\n\nBenoit et al. (2018)\nOnline reference\n\nOn corpus-linguistic theory:\n\nWulff and Baker (2020)\nLange and Leuckert (2020)\nMcEnery, Xiao, and Yukio (2006)",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Concordancing</span>"
    ]
  },
  {
    "objectID": "Concordancing.html#preparation",
    "href": "Concordancing.html#preparation",
    "title": "11  Concordancing",
    "section": "11.2 Preparation",
    "text": "11.2 Preparation\n\n\n\n\n\n\nWorking directory\n\n\n\nIn order for R to be able to recognise the data, it is crucial to set up the working directory correctly.\n\nMake sure your R-script and the corpus (e.g., ‘ICE-GB’) are stored in the same folder.\nIn RStudio, now navigate to Session &gt; Set working directory &gt; To Source File Location. This ensures that the folder where you have placed your R-script will function as your working directory until you close RStudio again.\n\nTo see your working directory in your files pane, click on Files &gt; 'Blue wheel symbol' &gt; Go to working directory.\n\n\nIn addition, make sure you have installed quanteda. Load it at the beginning of your script:\n\nlibrary(quanteda) # Package for Natural Language Processing in R\nlibrary(lattice) # for dotplots\n\nTo load a corpus object into R, place it in your working directory and read it into your working environment with readRDS().1\n1 The ICE-GB.RDS file you’ve been provided with has been pre-processed and saved in this specific format for practical reasons.\n# Load corpus from directory\nICE_GB &lt;- readRDS(\"ICE_GB.RDS\")\n\nIf you encounter any error messages at this stage, ensure you followed steps 1 and 2 in the callout box above.",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Concordancing</span>"
    ]
  },
  {
    "objectID": "Concordancing.html#concordancing",
    "href": "Concordancing.html#concordancing",
    "title": "11  Concordancing",
    "section": "11.3 Concordancing",
    "text": "11.3 Concordancing\nA core task in corpus-linguistic research involves finding occurrences of a single word or multi-word sequence in the corpus. Lange & Leuckert (2020: 55, emphasis in original) explain that specialised software typically “provide[s] the surrounding context as well as the name of the file in which the word could be identified.” Inspecting the context is particularly important in comparative research, as it may be indicative of distinct usage patterns.\n\n11.3.1 Simple queries\nTo obtain such a keyword in context (KWIC) in R, we use the kwic() function. We supply the corpus as well as the keyword we’re interested in:\n\nquery1 &lt;- kwic(ICE_GB, \"eat\")\n\nThe output in query1 contains concordance lines that list all occurrences of the keyword, including the document, context to the left, the keyword itself, and the context to the right. The final column reiterates our search expression.\n\nhead(query1)\n\nKeyword-in-context with 6 matches.                                                            \n  [ICE_GB/S1A-006.txt, 785]           So I' d rather | eat |\n [ICE_GB/S1A-009.txt, 1198]              I must &lt;, &gt; | eat |\n  [ICE_GB/S1A-010.txt, 958]         to &lt;, &gt; actually | eat |\n  [ICE_GB/S1A-018.txt, 455] order one first and then | eat |\n  [ICE_GB/S1A-018.txt, 498]  A &gt; The bargain hunting | eat |\n [ICE_GB/S1A-023.txt, 1853]       B &gt; Oh name please | eat |\n                            \n beforehand just to avoid uh\n them &lt; ICE-GB:S1A-009#71:  \n it for one' s              \n it and then sort of        \n &lt; ICE-GB:S1A-018#29: 1     \n something &lt;,, &gt;            \n\n\nFor a full screen display of the KWIC data frame, try View():\n\nView(query1)\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-006.txt\n785\n785\nSo I ' d rather\neat\nbeforehand just to avoid uh\neat\n\n\nICE_GB/S1A-009.txt\n1198\n1198\nI must &lt; , &gt;\neat\nthem &lt; ICE-GB:S1A-009 #71 :\neat\n\n\nICE_GB/S1A-010.txt\n958\n958\nto &lt; , &gt; actually\neat\nit for one ' s\neat\n\n\nICE_GB/S1A-018.txt\n455\n455\norder one first and then\neat\nit and then sort of\neat\n\n\nICE_GB/S1A-018.txt\n498\n498\nA &gt; The bargain hunting\neat\n&lt; ICE-GB:S1A-018 #29 : 1\neat\n\n\nICE_GB/S1A-023.txt\n1853\n1853\nB &gt; Oh name please\neat\nsomething &lt; , , &gt;\neat\n\n\n\n\n\n\n\n\n\n11.3.2 Multi-word queries\nIf the search expression exceeds a single word, we need to mark it as a multi-word sequence by means of the phrase() function. For instance, if we were interested in the pattern eat a, we’d have to adjust the code as follows:\n\nquery2 &lt;- kwic(ICE_GB, phrase(\"eat a\"))\n\n\nView(query2)\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-059.txt\n2230\n2231\n1 : B &gt; I\neat a\n&lt; , &gt; very balanced\neat a\n\n\nICE_GB/W2B-014.txt\n1045\n1046\n: 1 &gt; We can't\neat a\nlot of Welsh or Scottish\neat a\n\n\nICE_GB/W2B-022.txt\n589\n590\nhave few labour-saving devices ,\neat a\ndiet low in protein ,\neat a\n\n\n\n\n\n\n\n\n\n11.3.3 Multiple simultaneous queries\nA very powerful advantage of quanteda over traditional corpus software is that we can query a corpus for a multitude of keywords at the same time. Say, we need our output to contain hits for eat, drink as well as sleep. Instead of a single keyword, we supply a character vector containing the strings of interest.\n\nquery3 &lt;- kwic(ICE_GB, c(\"eat\", \"drink\", \"sleep\"))\n\n\nView(query3)\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-006.txt\n785\n785\nSo I ' d rather\neat\nbeforehand just to avoid uh\neat\n\n\nICE_GB/S1A-009.txt\n869\n869\n: A &gt; Do you\ndrink\nquite a lot of it\ndrink\n\n\nICE_GB/S1A-009.txt\n1198\n1198\nI must &lt; , &gt;\neat\nthem &lt; ICE-GB:S1A-009 #71 :\neat\n\n\nICE_GB/S1A-010.txt\n958\n958\nto &lt; , &gt; actually\neat\nit for one ' s\neat\n\n\nICE_GB/S1A-014.txt\n3262\n3262\nyou were advised not to\ndrink\nwater in Leningrad because they\ndrink\n\n\nICE_GB/S1A-016.txt\n3290\n3290\n&gt; I couldn't I couldn't\nsleep\nif I didn't read &lt;\nsleep\n\n\n\n\n\n\n\n\n\n11.3.4 Window size\nSome studies require more detailed examination of the preceding or following context of the keyword. We can easily adjust the window size to suit our needs:\n\nquery4 &lt;- kwic(ICE_GB, \"eat\", window = 20) \n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-006.txt\n785\n785\n#49 : 1 : A &gt; Yeah &lt; ICE-GB:S1A-006 #50 : 1 : A &gt; So I ' d rather\neat\nbeforehand just to avoid uh &lt; , , &gt; any problems there &lt; ICE-GB:S1A-006 #51 : 1 : B &gt;\neat\n\n\nICE_GB/S1A-009.txt\n1198\n1198\n&lt; , &gt; in in the summer &lt; ICE-GB:S1A-009 #70 : 1 : A &gt; I must &lt; , &gt;\neat\nthem &lt; ICE-GB:S1A-009 #71 : 1 : A &gt; Yes &lt; ICE-GB:S1A-009 #72 : 1 : B &gt; You ought\neat\n\n\nICE_GB/S1A-010.txt\n958\n958\n1 : B &gt; You know I mean it would seem to be squandering it to &lt; , &gt; actually\neat\nit for one ' s own enjoyment &lt; , , &gt; &lt; ICE-GB:S1A-010 #49 : 1 : A &gt; Mm\neat\n\n\nICE_GB/S1A-018.txt\n455\n455\ns so &lt; ICE-GB:S1A-018 #27 : 1 : A &gt; What you should do is order one first and then\neat\nit and then sort of carry on from there &lt; laughter &gt; &lt; , &gt; by which time you wouldn't\neat\n\n\nICE_GB/S1A-018.txt\n498\n498\nsecond anyway so &lt; laugh &gt; &lt; , &gt; &lt; ICE-GB:S1A-018 #28 : 1 : A &gt; The bargain hunting\neat\n&lt; ICE-GB:S1A-018 #29 : 1 : B &gt; So all right what did I have &lt; ICE-GB:S1A-018 #30 : 1\neat\n\n\nICE_GB/S1A-023.txt\n1853\n1853\n&gt; I can't bear it &lt; , , &gt; &lt; ICE-GB:S1A-023 #121 : 1 : B &gt; Oh name please\neat\nsomething &lt; , , &gt; &lt; ICE-GB:S1A-023 #122 : 1 : A &gt; Oh actually Dad asked me if &lt;\neat\n\n\n\n\n\n\n\n\n\n11.3.5 Saving your output\nYou can store your results in a spreadsheet file just as described in the unit on importing and exporting data.\n\nMicrosoft Excel (.xlsx)\n\n\nlibrary(writexl) # required for writing files to MS Excel\n\nwrite_xlsx(query2, \"myresults1.xlsx\")\n\n\nLibreOffice (.csv)\n\n\nwrite.csv(query2, \"myresults1.csv\")\n\nAs soon as you have annotated your data, you can load .xlsx files back into R with read_xlsx() from the readxl package and .csv files using the Base R function read.csv().",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Concordancing</span>"
    ]
  },
  {
    "objectID": "Concordancing.html#characterising-the-output",
    "href": "Concordancing.html#characterising-the-output",
    "title": "11  Concordancing",
    "section": "11.4 Characterising the output",
    "text": "11.4 Characterising the output\nRecall our initial query of the eat, whose output we stored in query1:\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-006.txt\n785\n785\nSo I ' d rather\neat\nbeforehand just to avoid uh\neat\n\n\nICE_GB/S1A-009.txt\n1198\n1198\nI must &lt; , &gt;\neat\nthem &lt; ICE-GB:S1A-009 #71 :\neat\n\n\nICE_GB/S1A-010.txt\n958\n958\nto &lt; , &gt; actually\neat\nit for one ' s\neat\n\n\nICE_GB/S1A-018.txt\n455\n455\norder one first and then\neat\nit and then sort of\neat\n\n\nICE_GB/S1A-018.txt\n498\n498\nA &gt; The bargain hunting\neat\n&lt; ICE-GB:S1A-018 #29 : 1\neat\n\n\nICE_GB/S1A-023.txt\n1853\n1853\nB &gt; Oh name please\neat\nsomething &lt; , , &gt;\neat\n\n\n\n\n\n\n\nFirst, we may be interested in obtaining some general information on our results, such as …\n\n… how many tokens (= individual hits) does the query return?\n\nThe nrow() function counts the number of rows in a data frame — these always correspond to the number of observations in our sample (here: 53).\n\nnrow(query1)\n\n[1] 53\n\n\n\n… how many types (= distinct hits) does the query return?\n\nApparently, there are 52 counts of eat in lower case and 1 in upper case. Their sum corresponds to our 53 observations in total.\n\ntable(query1$keyword)\n\n\neat Eat \n 52   1 \n\n\n\n… how is the keyword distributed across corpus files?\n\nThis question relates to the notion of dispersion: Is a keyword spread relatively evenly across corpus files or does it only occur in specific ones?\n\nquery_distrib &lt;- table(query1$docname, query1$keyword)\n\n# Create a simple dot plot\ndotplot(query_distrib, auto.key = list(columns = 2, title = \"Tokens\", cex.title = 1))\n\n\n\n\n\n\n\n\nIt seems that eat occurs at least once in most text categories (both spoken and written), but seems to be much more common in face-to-face conversations (S1A). This is not surprising: It is certainly more common to discuss food in a casual chat with friends than in an academic essay (unless, of course, its main subject matter is food). Dispersion measures can thus be viewed as indicators of contextual preferences associated with lexemes or more grammatical patterns.\n\n\n\n\n\n\nAdvanced: More on dispersion\n\n\n\n\n\nThe empirical study of dispersion has attracted a lot of attention in recent years Gries (2020). A reason for this is the necessity of finding a dispersion measure that is minimally correlated with token frequency. One such measure is the Kullback-Leibler divergence \\(KLD\\), which comes from the field of information theory and is closely related to entropy.\nMathematically, \\(KLD\\) measures the difference between two probability distributions \\(p\\) and \\(q\\).\n\\[ KLD(p \\parallel q) = \\sum\\limits_{x \\in X} p(x) \\log \\frac{p(x)}{q(x)}\n\\tag{11.1}\\]\nLet \\(f\\) denote the overall frequency of a keyword in the corpus, \\(v\\) its frequency in each corpus part, \\(s\\) the sizes of each corpus part (as fractions) and \\(n\\) the total number of corpus parts. We thus compare the posterior (= “actual”) distribution of keywords \\(\\frac{v_i}{f}\\) for \\(i = 1, ..., n\\) with their prior distribution, which assumes all words are spread evenly across corpus parts (hence the division by \\(s_i\\)).\n\\[ KLD = \\sum\\limits_{i=1}^n \\frac{v_i}{f} \\times \\log_2\\left({\\frac{v_i}{f} \\times \\frac{1}{s_i}}\\right)\n\\tag{11.2}\\]\nIn R, let’s calculate the dispersion of the verbs eat, drink, and sleep from query3.\n\n# Let's filter out the upper-case variants:\nquery3_reduced &lt;- query3[query3$keyword %in% c(\"eat\", \"drink\", \"sleep\"),]\ntable(query3_reduced$keyword)\n\n\ndrink   eat sleep \n   48    52    41 \n\n# Extract text categories\nquery_registers &lt;- separate_wider_delim(query3_reduced, cols = docname, delim = \"-\", names = c(\"Text_category\", \"File_number\"))\n\n# Get separate data frames for each verb\neat &lt;- filter(query_registers, keyword == \"eat\")\ndrink &lt;- filter(query_registers, keyword == \"drink\")\nsleep &lt;- filter(query_registers, keyword == \"sleep\")\n\n## Get frequency distribution across files\nv_eat &lt;- table(eat$Text_category)\nv_drink &lt;- table(drink$Text_category)\nv_sleep &lt;- table(sleep$Text_category)\n\n## Get total frequencies\nf_eat &lt;- nrow(eat)\nf_drink &lt;- nrow(drink)\nf_sleep &lt;- nrow(sleep)\n\n# The next step is a little trickier. First we need to find out how many distinct corpus parts there are in the ICE corpus.\n\n## Check ICE-corpus structure and convert to data frame\nICE_GB_str &lt;- as.data.frame(summary(ICE_GB))\n\n## Separate files from text categores\nICE_GB_texts &lt;- separate_wider_delim(ICE_GB_str, cols = Var1, delim = \"-\", names = c(\"Text_category\", \"File\"))\n\n## Get number of distinct text categories\nn &lt;- length(unique(ICE_GB_texts$Text_category))\n\n## Get proportions of distinct text categories (s)\ns &lt;- table(ICE_GB_texts$Text_category)/sum(table(ICE_GB_texts$Text_category))\n\n## Unfortunately not all of these corpus parts are represented in our queries. We need to correct the proportions in s for the missing ones!\n\n## Store unique ICE text categories \nICE_unique_texts &lt;- unique(ICE_GB_texts$Text_category)\n\n## Make sure only those text proportions are included where the keywords actually occur\ns_eat &lt;- s[match(names(v_eat), ICE_unique_texts)]\ns_drink &lt;- s[match(names(v_drink), ICE_unique_texts)]\ns_sleep &lt;- s[match(names(v_sleep), ICE_unique_texts)]\n\n# Compute KLD for each verb\nkld_eat &lt;- sum(v_eat/f_eat * log2(v_eat/f_eat * 1/s_eat)); kld_eat\n\n[1] 0.6747268\n\nkld_drink &lt;- sum(v_drink/f_drink * log2(v_drink/f_drink * 1/s_drink)); kld_drink\n\n[1] 0.8463608\n\nkld_sleep &lt;- sum(v_sleep/f_sleep * log2(v_sleep/f_sleep * 1/s_sleep)); kld_sleep\n\n[1] 0.7047421\n\n# Plot\nkld_df &lt;- data.frame(kld_eat, kld_drink, kld_sleep)\n\nbarplot(as.numeric(kld_df), names.arg = names(kld_df), col = \"steelblue\",\n        xlab = \"Variable\", ylab = \"KLD Value (= deviance from even distribution)\", main = \"Dispersion of 'eat', 'drink', and 'sleep'\")\n\n\n\n\n\n\n\n\nThe plot indicates that drink is the most unevenly distributed verb out of the three considered (high KDL \\(\\sim\\) low dispersion), whereas eat appears to be slightly more evenly distributed across corpus files. The verb sleep assumes an intermediary position.",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Concordancing</span>"
    ]
  },
  {
    "objectID": "Concordancing.html#i-need-a-proper-user-interface-some-alternatives",
    "href": "Concordancing.html#i-need-a-proper-user-interface-some-alternatives",
    "title": "11  Concordancing",
    "section": "11.5 “I need a proper user interface”: Some alternatives",
    "text": "11.5 “I need a proper user interface”: Some alternatives\nThere is a wide variety of concordancing software available, both free and paid. Among the most popular options are AntConc (Anthony 2020) and SketchEngine (Kilgarriff et al. 2004). However, as Schweinberger (2024) notes, the exact processes these tools use to generate output are not always fully transparent, making them something of a “black box.” In contrast, programming languages like R or Python allow researchers to document each step of their analysis clearly, providing full transparency from start to finish.\nThe following apps attempt to reconcile the need for an intuitive user interface with transparent data handling. The full source code is documented in the respective GitHub repositories.\n\nQuantedaApp is an interface for the R package quanteda (Benoit et al. 2018).\nPyConc is an interface for the Python package spaCy (Honnibal and Montani 2017).\n\n\n\n\n\nAnthony, Lawrence. 2020. AntConc (Version 3.5.9). Tokyo, Japan: Waseda University. https://www.laurenceanthony.net/software.\n\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Müller, and Akitaka Matsuo. 2018. “Quanteda: An r Package for the Quantitative Analysis of Textual Data.” Journal of Open Source Software 3 (30): 774. https://doi.org/10.21105/joss.00774.\n\n\nGries, Stefan Thomas. 2020. “Analyzing Dispersion.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Thomas Gries, 99–118. Cham: Springer.\n\n\nHonnibal, Matthew, and Ines Montani. 2017. “spaCy 2: Natural Language Understanding with Bloom Embeddings, Convolutional Neural Networks and Incremental Parsing.”\n\n\nKilgarriff, Adam, Pavel Rychly, Pavel Smrz, and David Tugwell. 2004. “ITRI-04-08 the Sketch Engine.” Information Technology 105: 116.\n\n\nLange, Claudia, and Sven Leuckert. 2020. Corpus Linguistics for World Englishes: A Guide for Research. New York: Taylor; Francis.\n\n\nMcEnery, Tony, Richard Xiao, and Tono Yukio. 2006. Corpus-Based Language Studies: An Advanced Resource Book. London: Routledge.\n\n\nSchweinberger, Martin. 2024. Concordancing with r. 2024.05.07 ed. Brisbane: The Language Technology; Data Analysis Laboratory (LADAL). https://ladal.edu.au/kwics.html.\n\n\nSönning, Lukas. 2024. “Evaluation of Keyness Metrics: Performance and Reliability,” Corpus Linguistics and Linguistic Theory, 20 (2): 263–88. https://doi.org/10.1515/cllt-2022-0116.\n\n\nWulff, Stefanie, and Paul Baker. 2020. “Analyzing Concordances.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Th. Gries, 161–79. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-46216-1_8.",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Concordancing</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "28  References",
    "section": "",
    "text": "References\n\n\nAgresti, Alan, and Maria Kateri. 2022. Foundations of Statistics for\nData Scientists: With r and Python. Boca Raton: CRC\nPress.\n\n\nAnthony, Lawrence. 2020. AntConc (Version 3.5.9).\nTokyo, Japan: Waseda University. https://www.laurenceanthony.net/software.\n\n\nBaayen, R. Harald, and Maja Linke. 2020. “Generalized\nAdditive Mixed Models.” In\nA Practical Handbook of\nCorpus Linguistics, edited by Magali\nPaquot and Stefan Thomas Gries, 563–91. Cham: Springer.\n\n\nBaguley, Thomas. 2012. Serious Stats: A\nGuide to Advanced Statistics for\nthe Behavioral Sciences. Houndmills,\nBasingstoke: Palgrave Macmillan.\n\n\nBalota, David A., Melvin J. Yap, Keith A. Hutchison, Michael J. Cortese,\nBrett Kessler, Bjorn Loftis, James H. Neely, Douglas L. Nelson, Greg B.\nSimpson, and Rebecca Treiman. 2007. “The English\nLexicon Project.” Behavior Research\nMethods 39 (3): 445–59. https://doi.org/10.3758/BF03193014.\n\n\nBarbieri, Federica. 2007. “Older Men and Younger Women: A\nCorpus-Based Study of Quotative Use in American English.”\nEnglish World-Wide 28 (1): 23–45. https://doi.org/10.1075/eww.28.1.03bar.\n\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng,\nStefan Müller, and Akitaka Matsuo. 2018. “Quanteda: An r Package\nfor the Quantitative Analysis of Textual Data.” Journal of\nOpen Source Software 3 (30): 774. https://doi.org/10.21105/joss.00774.\n\n\nBooth, Wayne C., Gregory G. Colomb, and Joseph M. Williams. 2008.\nThe Craft of Research. 3rd ed. Chicago: The University of\nChicago Press.\n\n\nBreiman, Leo. 1984. Classification and Regression Trees.\nBelmont, Calif.: Wadsworth International Group.\n\n\n———. 2001. “Random Forests.” Machine Learning 45\n(1): 5–32. https://doi.org/10.1023/A:1010933404324.\n\n\nBuskin, Vladimir. n.d. “Definite Null Instantiation\nin English(es): A Usage-based\nConstruction Grammar Approach.” Constructions and\nFrames.\n\n\nCiaccio, Laura Anna, and João Veríssimo. 2022. “Investigating\nVariability in Morphological Processing with Bayesian\nDistributional Models.” Psychonomic Bulletin &\nReview 29 (6): 2264–74. https://doi.org/10.3758/s13423-022-02109-w.\n\n\nDebeer, Dries, and Carolin Strobl. 2020. “Conditional Permutation\nImportance Revisited.” Bioinformatics 21 (1): 307.\n\n\nGao, Chuanji, Svetlana V. Shinkareva, and Rutvik H. Desai. 2022.\n“SCOPE: The South\nCarolina Psycholinguistic Metabase.” Behavior\nResearch Methods 55 (6): 2853–84. https://doi.org/10.3758/s13428-022-01934-0.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis\nUsing Regression and\nMultilevel/Hierarchical Models.\nAnalytical Methods for Social Research. Cambridge: Cambridge University\nPress.\n\n\nGlass, Lelia. 2021. “English Verbs Can Omit Their Objects When\nThey Describe Routines.” English Language and\nLinguistics 26 (1): 49–73. https://doi.org/10.1017/S1360674321000022.\n\n\nGoldberg, Adele E. 2001. “Patient Arguments of Causative Verbs Can\nBe Omitted: The Role of Information Structure in Argument\nDistribution.” Language Sciences 23 (4): 503–24.\n\n\nGreenwell, Brandon. 2022. Tree-Based Methods for Statistical\nLearning in r. London & New York: Taylor & Francis Group.\nhttps://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=3288358.\n\n\nGreenwell, Brandon M. 2017. “Pdp: An r Package for Constructing\nPartial Dependence Plots.” The R Journal 9 (1): 421–36.\n\n\nGries, Stefan Thomas. 2013. Statistics for Linguistics with r: A\nPractical Introduction. 2nd rev. ed. Berlin; Boston: De Gruyter\nMouton.\n\n\n———. 2021. Statistics for Linguistics with r: A Practical\nIntroduction. 3rd rev. ed. Berlin; Boston: De Gruyter Mouton.\n\n\nGries, Stefan, and Sandra Deshors. 2014. “Using Regressions to\nExplore Deviations Between Interlanguage and Native Language.”\nCorpora 9: 109–36.\n\n\nHastie, Trevor, and Robert Tibshirani. 1991. Generalized\nAdditive Models. London: Chapman &\nHall.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome H. Friedman. 2017. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction. 2nd ed. New York, NY: Springer.\n\n\nHazen, Kirk. 2015. An Introduction to Language. Chichester:\nWiley Blackwell.\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022.\nIntroduction to Statistics and Data Analysis: With Exercises,\nSolutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nHonnibal, Matthew, and Ines Montani. 2017. “spaCy 2: Natural Language Understanding with\nBloom Embeddings, Convolutional Neural Networks and\nIncremental Parsing.”\n\n\nHosmer, David W., and Stanley Lemeshow. 2008. Applied Logistic\nRegression. 2nd ed. New York: Wiley.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning: With Applications in\nr. New York: Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nKilgarriff, Adam, Pavel Rychly, Pavel Smrz, and David Tugwell. 2004.\n“ITRI-04-08 the Sketch Engine.” Information\nTechnology 105: 116.\n\n\nLange, Claudia, and Sven Leuckert. 2020. Corpus Linguistics for\nWorld Englishes: A Guide for Research. New York: Taylor; Francis.\n\n\nLevshina, Natalia. 2015. How to Do Linguistics with r: Data\nExploration and Statistical Analysis. Amsterdam; Philadelphia: John\nBenjamins Publishing Company.\n\n\n———. 2020. “Conditional Inference Trees and Random\nForests.” In A Practical Handbook of Corpus Linguistics,\nedited by Magali Paquot and Stefan Thomas Gries, 611–43. Cham: Springer.\n\n\nMair, Patrick. 2018. Modern Psychometrics with\nR. Cham: Springer. https://doi.org/10.1007/978-3-319-93177-7.\n\n\nMcEnery, Tony, Richard Xiao, and Tono Yukio. 2006. Corpus-Based\nLanguage Studies: An Advanced Resource Book. London: Routledge.\n\n\nO’Connell, Ann A. 2006. Logistic Regression\nModels for Ordinal Response\nVariables. Vol. 146. Thousand Oaks, Calif.: Sage.\n\n\nPaquot, Magali, and Tove Larsson. 2020. “Descriptive Statistics\nand Visualization with r.” In A Practical Handbook of Corpus\nLinguistics, edited by Magali Paquot and Stefan Thomas Gries,\n375–99. Cham: Springer.\n\n\nPowers, Daniel A., and Yu Xie. 2008. Statistical\nMethods for Categorical Data\nAnalysis. 2. ed. Bingley: Emerald.\n\n\nResnik, Philip. 1996. “Selectional Constraints: An\nInformation-Theoretic Model and Its Computational Realization” 61\n(1): 127–59.\n\n\nSchäfer, Roland. 2020. “Mixed-Effects Regression Modeling.”\nIn A Practical Handbook of Corpus Linguistics, edited by Magali\nPaquot and Stefan Thomas Gries, 535–61. Cham: Springer.\n\n\nSchweinberger, Martin. 2024. Concordancing with r. 2024.05.07\ned. Brisbane: The Language Technology; Data Analysis Laboratory (LADAL).\nhttps://ladal.edu.au/kwics.html.\n\n\nSönning, Lukas. 2024. “Evaluation of Keyness Metrics: Performance\nand Reliability,” Corpus Linguistics and\nLinguistic Theory, 20 (2): 263–88. https://doi.org/10.1515/cllt-2022-0116.\n\n\nStrobl, Carolin, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin,\nand Achim Zeileis. 2008. “Conditional Variable Importance for\nRandom Forests.” BMC Bioinformatics 9 (1): 307.\n\n\nStrobl, Carolin, Anne-Laure Boulesteix, Achim Zeileis, and Torsten\nHothorn. 2007. “Bias in Random Forest Variable Importance\nMeasures: Illustrations, Sources and a Solution.” BMC\nBioinformatics 8 (1): 25.\n\n\nStrobl, Carolin, James Malley, and Gerhard Tutz. 2009. “An\nIntroduction to Recursive Partitioning: Rationale, Application, and\nCharacteristics of Classification and Regression Trees, Bagging, and\nRandom Forests.” Psychological Methods 14: 323–48. https://doi.org/10.1037/a0016973.\n\n\nUnuabonah, Foluke Olayinka, and Ulrike Gut. 2018. “Commentary\nPragmatic Markers in Nigerian English.”\nEnglish World-Wide 39 (2): 190–213.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using\nr. New York; London: Routledge.\n\n\nWood, Simon N. 2006. Generalized Additive\nModels: An Introduction with\nR. Boca Raton: Chapman & Hall/CRC.\n\n\nWulff, Stefanie, and Paul Baker. 2020. “Analyzing\nConcordances.” In A Practical\nHandbook of Corpus\nLinguistics, edited by Magali Paquot and Stefan Th.\nGries, 161–79. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-46216-1_8.",
    "crumbs": [
      "References",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "Research_questions.html#what-makes-a-good-research-question",
    "href": "Research_questions.html#what-makes-a-good-research-question",
    "title": "2  Research questions",
    "section": "2.1 What makes a good research question?",
    "text": "2.1 What makes a good research question?\n\n2.1.1 The structure of research questions\nExample: I am studying the pronunciation of [r] in New York because I want to find out whether different social groups differ with respect to their pronunciation, in order to help my readers understand how social factors influence our linguistic behavior.\n\nTopic: I am studying...\nIndirect question: Because I want to find out what, why, how...\n(General) linguistic significance: ... in order to help the readers understand how, why, or whether...\n\n\nCf. Booth, Colomb, and Williams (2008): 45–48\n\n\n\n2.1.2 Hints for good research questions\n\n\n\n\n\n\nTip\n\n\n\n\nA research question must be simple, specific, and doable. It is better to make a small contribution to a particular problem than to aim at covering a whole subfield of sociolinguistics.\nKeep a research question stable in your paper, even if you come across interesting new ideas as you work on it.\nThe research question must be an explicit part of your paper. Mention it in the introduction and keep referring back to it from time to time.\nKeep in mind that the research question will determine both methodology and data.\n\n\nCf. Hazen (2015): 9–10\n\n\n\n\n\n2.1.3 Hypotheses\nResearch questions must be complemented by a set of falsifiable hypotheses. These will be covered in-depth in the chapter on hypothesis testing. In short,\n\nHypothesis \\(H_1\\) predicts a specific relationship between a dependent variable and an independent variable. If \\(H_1\\) holds, studies often speak of a correlation (or association) between variables.\nits opposite, Hypothesis \\(H_0\\), describes the state of affairs where the predicted relationship between the variables does not hold.\n\nThe predictions made by the hypotheses are based on previous research. Here is an example:\nResearch question: I am studying the pronunciation of [r] in New York because I want to find out whether different social groups differ with respect to their pronunciation, in order to help my readers understand how social factors influence our linguistic behavior.\n\nHypothesis \\(H_1\\): There is a difference in the use of postvocalic /r/ between upper-class and lower-class speakers.\nHypothesis \\(H_0\\): There is no difference in the use of postvocalic /r/ between upper-class and lower-class speakers.\n\n\n\n2.1.4 Exercises\n\nExercise 2.1 Assess the following research questions!\n\nI will try to find the specific linguistic features which a Shetlander uses when speaking Scottish standard English, i.e., Shetland accent in Scottish Standard English.\nIn this paper, I am going to analyze the use of American and Indian dialect features among Indian immigrants to the US because I want to find out whether they correlate with their willingness to integrate into American society; this will show the degree to which linguistic adaptation depends on cultural orientation.\nIn this paper, I am going to address the question of how gender influences language.\nThe study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all.\nIn this paper, adverts by Coca-Cola and Pepsi will be analyzed for the use of verb forms.\n\n\n\nExercise 2.2 Develop valid research questions on the basis of the following linguistic variables:\n\nSpeakers in bilingual conversations are especially likely to code-switch when there is a significant change of topic.\nWomen use more tentative language in conversations than men.\n\n\n\n\n\n\nBooth, Wayne C., Gregory G. Colomb, and Joseph M. Williams. 2008. The Craft of Research. 3rd ed. Chicago: The University of Chicago Press.\n\n\nHazen, Kirk. 2015. An Introduction to Language. Chichester: Wiley Blackwell.",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Research questions</span>"
    ]
  },
  {
    "objectID": "Hypothesis_testing.html#preparation",
    "href": "Hypothesis_testing.html#preparation",
    "title": "18  Hypothesis testing",
    "section": "18.2 Preparation",
    "text": "18.2 Preparation",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "Hypothesis_testing.html#sec-hyp",
    "href": "Hypothesis_testing.html#sec-hyp",
    "title": "18  Hypothesis testing",
    "section": "18.2 Hypothesis testing",
    "text": "18.2 Hypothesis testing\nThe null hypothesis significance testing (NHST) framework offers researchers a convenient way of testing theoretical assumptions about a population of interest (e.g., a speech community). This involves setting up a set of falsifiable statistical hypotheses that predict the presence or absence of certain patterns in the data. These are known as the null hypothesis \\(H_0\\) and the alternative hypothesis \\(H_1\\) (or \\(H_a\\)). They are set up before seeing the data and justified by previous research.\n\nGiven two categorical variables \\(X\\) and \\(Y\\), we assume under \\(H_0\\) that both variables are independent from each other. This hypothesis describes the “default state of the world” (James et al. 2021: 555), i.e., what we would usually expect to see. There is no association between the variables of interest and, therefore, no effect.\nBy contrast, the alternative hypothesis \\(H_1\\) claims that \\(X\\) and \\(Y\\) are not independent, i.e., that \\(H_0\\) does not hold. \\(X\\) and \\(Y\\) then appear to be correlated in some way, i.e., there is some kind of effect.\n\nIn the subsequent sections, we will consider two scenarios:\n\nWe are interested in finding out whether English clause ORDER (‘sc-mc’ or ‘mc-sc’) depends on the type of the subordinate clause (SUBORDTYPE), which can be either temporal (‘temp’) or causal (‘caus’).\n\nOur hypotheses are:\n\n\\(H_0:\\) The variables ORDER and SUBORDTYPE are independent.\n\\(H_1:\\) The variables ORDER and SUBORDTYPE are not independent.\n\n\nAs part of a phonetic study, we compare the base frequencies of the F1 formants for male and female speakers of Apache. We forward the following hypotheses:\n\n\n\\(H_0:\\) mean F1 frequency of men \\(=\\) mean F1 frequency of women.\n\\(H_1:\\) mean F1 frequency of men \\(\\ne\\) mean F1 frequency of women.\n\nBased on our data, we can decide to either accept or reject \\(H_0\\). Rejecting \\(H_0\\) can be viewed as evidence in favour of \\(H_1\\) and thus marks a potential ‘discovery’ in the data. However, there is always a chance that we accept or reject the wrong hypothesis; the four possible constellations are summarised in the table below (cf. Heumann, Schomaker, and Shalabh 2022: 223):\n\n\n\n\n\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is not true\n\n\n\n\n\\(H_0\\) is not rejected\n\\(\\color{green}{\\text{Correct decision}}\\)\n\\(\\color{red}{\\text{Type II } (\\beta)\\text{-error}}\\)\n\n\n\\(H_0\\) is rejected\n\\(\\color{red}{\\text{Type I } (\\alpha)\\text{-error}}\\)\n\\(\\color{green}{\\text{Correct decision}}\\)\n\n\n\nThe probability of a Type I error, which refers to the rejection of \\(H_0\\) although it is true, is called the significance level \\(\\alpha\\), which has a conventional value of \\(0.05\\) (i.e., a 5% chance of committing a Type I error). Nevertheless, it is always recommended to state explicitly the \\(\\alpha\\)-level used for rejecting/accepting \\(H_0\\).",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "Hypothesis_testing.html#constructing-the-critical-region",
    "href": "Hypothesis_testing.html#constructing-the-critical-region",
    "title": "18  Hypothesis testing",
    "section": "18.3 Constructing the critical region",
    "text": "18.3 Constructing the critical region\nAn important question remains: How great should the difference be for us to reject \\(H_0\\)? The \\(p\\)-value measures the probability of encountering a specific value of a test statistic (\\(\\chi^2\\)-score, \\(t\\), \\(F\\) etc.) on the condition that \\(H_0\\) is true.\n\n\n\n\n\n\nA more precise definition of \\(p\\)-values\n\n\n\n“The \\(P\\)-value is the probability, presuming that \\(H_0\\) is true, that the test statistic equals the observed value or a value even more extreme in the direction predicted by \\(H_a\\) (Agresti and Kateri 2022: 163)”.\n\n\nFor example, a \\(p\\)-value of \\(0.02\\) means that we would see a particular test statistic only 2% of the time if \\(X\\) and \\(Y\\) were unrelated (or if there was no difference between \\(\\bar{x}\\) and \\(\\bar{y}\\), respectively). Since our significance level \\(\\alpha\\) is set to \\(0.05\\), we only reject the null hypothesis if this conditional probability is lower than 5%.\nWe obtain \\(p\\)-values by consulting the probability density functions of the underlying sampling distributions:\n\nProbability density function for the \\(\\chi^2\\)-distribution with \\(df = 1\\)\n\n\n\nCode\n# Generate random samples from a chi-squared distribution with 1 degree of freedom\nx &lt;- rchisq(100000, df = 1)\n\n# Create histogram\nhist(x,\n     breaks = \"Scott\",\n     freq = FALSE,\n     xlim = c(0, 20),\n     ylim = c(0, 0.2),\n     ylab = \"Probability density of observing a specific score\",\n     xlab = \"Chi-squared score\",\n     main = \"Histogram for a chi-squared distribution with 1 degree of freedom (df)\",\n     cex.main = 0.9)\n\n# Overlay PDF\ncurve(dchisq(x, df = 1), from = 0, to = 150, n = 5000, col = \"orange\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\n\n\nProbability density function for the \\(t\\)-distribution with \\(df = 112.19\\)\n\n\n\nCode\n# Given t-statistic and degrees of freedom\nt_statistic &lt;- 2.4416\ndf &lt;- 112.19\n\n# Generate random samples from a t-distribution with the given degrees of freedom\nx &lt;- rt(100000, df = df)\n\n# Create histogram\nhist(x,\n     breaks = \"Scott\",\n     freq = FALSE,\n     xlim = c(-5, 5),\n     ylim = c(0, 0.4),\n     ylab = \"Probability density of observing a specific score\",\n     xlab = \"t-score\",\n     main = \"Histogram for a t-distribution with 112.19 degrees of freedom\",\n     cex.main = 0.9)\n\n# Overlay PDF\ncurve(dt(x, df = df), from = -5, to = 5, n = 5000, col = \"orange\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgresti, Alan, and Maria Kateri. 2022. Foundations of Statistics for Data Scientists: With r and Python. Boca Raton: CRC Press.\n\n\nBaguley, Thomas. 2012. Serious Stats: A Guide to Advanced Statistics for the Behavioral Sciences. Houndmills, Basingstoke: Palgrave Macmillan.\n\n\nGries, Stefan Thomas. 2021. Statistics for Linguistics with r: A Practical Introduction. 3rd rev. ed. Berlin; Boston: De Gruyter Mouton.\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. New York: Springer. https://doi.org/10.1007/978-1-0716-1418-1.",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "Hypothesis_testing.html#suggested-reading",
    "href": "Hypothesis_testing.html#suggested-reading",
    "title": "18  Hypothesis testing",
    "section": "18.1 Suggested reading",
    "text": "18.1 Suggested reading\nFor linguists:\n\nGries (2021): Chapter 1.3.2\n\nGeneral:\n\nBaguley (2012): Chapter 4\nAgresti and Kateri (2022): Chapter 5",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "Linguistic_variables.html#what-is-a-linguistic-variable",
    "href": "Linguistic_variables.html#what-is-a-linguistic-variable",
    "title": "3  Linguistic variables",
    "section": "3.1 What is a linguistic variable?",
    "text": "3.1 What is a linguistic variable?\n\nThe classical view: Labov (1972) defines a linguistic variable as “two ways of saying the same thing.”\nA restriction: Meyerhoff (2009: 11) summarized, “In sum, a sociolinguistic variable can be defined as a linguistic variable that is constrained by social or non-linguistic factors…”\nA more open view: Kiesling (2011) argued, “Given the variability of what counts as a variable, we must define what counts as a variable more broadly than ‘two or more ways of saying the same thing’. We will simply say that a linguistic variable is a choice or option about speaking in a speech community… Note that this definition does not in any way require us to state that the meaning be the same, although there should be some kind of equivalence noted.”\n\n\n\n\n\n\n\nDiscussion\n\n\n\nWhich of the following variables are good sociolinguistic variables, and which of them are poor?\n\n/fɔːθ flɔː/ vs. /fɔːrθ flɔːr/\nThis enables him to preside over the process which I have described vs. This enables him to preside over the process that I have described vs. This enables him to preside over the process ∅ I have described.\nThe pair found the briefcase on a bus station bench at Bath central bus station. vs. The briefcase was found on a bus station bench at Bath central bus station by the pair.\nArt is after all the subject of attention for both critic and historian, even though the functions and methods of the two sorts of writer have drawn apart. vs. Art histories often make an attempt to keep to chronology, although the difficulties include the crucial fact that in art there is no clear sequence of events. vs. Many of his readers approved his sensitive and appreciative understanding of paintings, though without sharing his political views.\n/pleɪŋ/ vs. /pleɪn/\n[tʰ] in /tɔp/ vs. [t] in stop.",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linguistic variables</span>"
    ]
  },
  {
    "objectID": "Linguistic_variables.html#the-principle-of-accountability",
    "href": "Linguistic_variables.html#the-principle-of-accountability",
    "title": "3  Linguistic variables",
    "section": "3.2 The principle of accountability",
    "text": "3.2 The principle of accountability\n\n\n\n\n\n\nTask\n\n\n\nTwo linguists aim to study the preference for passives among men and women. They extract all the passives from 500,000 words of male speech and all passives from 500,000 words of female speech and report the results. What’s wrong?",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linguistic variables</span>"
    ]
  },
  {
    "objectID": "Linguistic_variables.html#subtypes-of-variables",
    "href": "Linguistic_variables.html#subtypes-of-variables",
    "title": "3  Linguistic variables",
    "section": "3.3 Subtypes of variables",
    "text": "3.3 Subtypes of variables\n\n3.3.1 Linguistic perspective\n\nphonetic/phonological\nmorphological\nsyntactic\npragmatic\n\n\n\n3.3.2 Sociolinguistic perspective\nSociolinguistic variables also differ with regard to their salience in society.\n\nStereotypes are strongly socially marked and part of popular discourse about language.\n\nh-dropping in Cockney\nCanadian eh at the end of sentences\nAustralian dinkum: I was fair dinkum about my interest in their culture ‘authentic, genuine’\n\nMarkers show both social and style stratification; all members of a society react similarly in taking care to avoid the pattern in formal registers.\n\n(r)\n(th)\n\nIndicators differentiate social groups. However, people are not aware of them and therefore do not avoid them in formal registers.\n\nSame vowel in God and Guard in New York City\n\n\n\nCf. Mesthrie (2011).",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linguistic variables</span>"
    ]
  },
  {
    "objectID": "Linguistic_variables.html#statistical-perspective",
    "href": "Linguistic_variables.html#statistical-perspective",
    "title": "3  Linguistic variables",
    "section": "3.4 Statistical perspective",
    "text": "3.4 Statistical perspective\n\n3.4.1 Dependent and independent variables\nTask: Decide which of the variables are independent, and which are dependent variables!\n\n\n\n\nKenyan speakers\nSingaporean speakers\n\n\n\n\nAverage sentence length\n10.5\n9.8\n\n\n\n\n\n\n\nmale\nfemale\n\n\n\n\nactually\n65\n98\n\n\npossibly\n55\n77\n\n\nreally\n54\n55\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: English is the most important language today.\nstrongly agree\nagree\nneutral\ndisagree\nstrongly disagree\n\n\n\n\nN\n12\n33\n58\n12\n8\n\n\n\n\n\n3.4.2 Interval, ordinal, and nominal\n\nCOMPLEXITY (ordinal)\n\nthe book (low)\nthe brown book (middle)\nthe book I had bought in Europe (high)\n\nLENGTH (interval)\n\nbook (1)\nthe book (2)\nthe book I had bought in Europe (7)\n\nANIMACY (nominal)\n\nHe picked up the book.\nHe picked his dad up.\n\n\n\nCf. Gries, Stefan Th. Statistics for Linguistics with R. De Gruyter Mouton, 2013, p. 9.\n\nTask: Now, go back to Tables 1–3 and decide which of the variables are interval variables, ordinal or nominal variables!",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linguistic variables</span>"
    ]
  },
  {
    "objectID": "Linguistic_variables.html#many-morphosyntactic-variables-in-english",
    "href": "Linguistic_variables.html#many-morphosyntactic-variables-in-english",
    "title": "3  Linguistic variables",
    "section": "3.4 Many morphosyntactic variables in English",
    "text": "3.4 Many morphosyntactic variables in English\n\n\n\n\n\n\n\nVariable\nExample\n\n\n\n\nIndefinite Pronouns\neverybody vs. everyone\n\n\nCase and order of coordinated pronouns\nmy husband and I vs. my husband and me vs. me and my husband\n\n\nthat vs. zero complementation\nI don’t think that/Ø it’s a problem.\n\n\nthat vs. gerundial complementation\nremember that vs. remember V-ing; try to vs. try and vs. try V-ing\n\n\nParticle placement\nset the computer up vs. set up the computer\n\n\nThe dative alternation\ngive the book to John vs. give John the book\n\n\nThe genitive alternation\nJohn’s house vs. the house of John\n\n\nRelativization strategies\nwh-word vs. that vs. Ø\n\n\nAnalytic vs. synthetic comparatives\nwarmer vs. more scary\n\n\nPlural existentials\nthere are some places vs. there’s some places\n\n\nFuture temporal reference\nwill vs. going to vs. progressive etc.\n\n\nDeontic modality\nmust vs. have to vs. need to vs. got to etc.\n\n\nStative possession\nhave vs. have got vs. got\n\n\nQuotatives\nsay vs. be like vs. go etc.\n\n\nnot vs. no\nnot anybody vs. nobody; not anyone vs. no one; not anything vs. nothing\n\n\nNOT vs. AUX contraction\nthat’s not vs. that isn’t etc.\n\n\n\n\nCf. Gardner et al. (2021).\n\n\n\n\n\nGardner, Matt Hunt et al. 2021. “Variation Isn’t That Hard: Morphosyntactic Choice Does Not Predict Production Difficulty.” PloS One 16 (6): e0252602–2.\n\n\nKiesling, Scott F. 2011. Linguistic Variation and Change. Edinburgh: Edinburgh University Press.\n\n\nLabov, William. 1972. Sociolinguistic Patterns. Philadelphia: University of Pennsylvania Press.\n\n\nMesthrie, Rajend. 2011. Introducing Sociolinguistics. 2nd ed. Edinburgh: Edinburgh University Press.\n\n\nMeyerhoff, Miriam. 2009. Introducing Sociolinguistics. London: Routledge.",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linguistic variables</span>"
    ]
  },
  {
    "objectID": "PCA.html#recommended-reading",
    "href": "PCA.html#recommended-reading",
    "title": "26  Principal Components Analysis",
    "section": "26.1 Recommended reading",
    "text": "26.1 Recommended reading\nFor linguists:\n\nLevshina (2015: Chapter 18)\n\nGeneral:\n\nMair (2018: Chapter 6)",
    "crumbs": [
      "Multivariate Data Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "PCA.html#sec-pca-prep",
    "href": "PCA.html#sec-pca-prep",
    "title": "26  Principal Components Analysis",
    "section": "26.2 Preparation",
    "text": "26.2 Preparation\nThis unit relies on psycholinguistic data from the South Carolina Psycholinguistic Metabase (Gao, Shinkareva, and Desai 2022).1 Detailed descriptions of the variables can be found here.\n1 One exception is the variable Resnik_strength [Resnik (1996)], which was computed manually and appended to the data frame.The data frame scope_sem_df contains semantic ratings for a sample of 1,702 transitive verbs. Note that all columns have been standardised (cf. ?scale() for details).\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(lattice)\nlibrary(corrplot)\nlibrary(psych)\nlibrary(GPArotation)\nlibrary(gridExtra)\n\n# Load data\nscope_sem_df &lt;- readRDS(\"scope_sem.RDS\")\n\n# Select subset\nscope_sem_sub &lt;- scope_sem_df[,1:11]\n\n# Overview\nglimpse(scope_sem_sub)\n\nRows: 1,702\nColumns: 11\n$ Verb               &lt;chr&gt; \"abstain\", \"abstract\", \"abuse\", \"accelerate\", \"acce…\n$ Resnik_strength    &lt;dbl&gt; 0.40909889, 0.18206692, 0.12473608, -0.76972217, -1…\n$ Conc_Brys          &lt;dbl&gt; -0.94444378, -1.92983639, -0.59478833, 0.22107437, …\n$ Nsenses_WordNet    &lt;dbl&gt; -0.68843996, 0.27755219, 0.00155443, -0.68843996, 0…\n$ Nmeanings_Websters &lt;dbl&gt; -0.95559835, 0.73781281, 0.73781281, -0.27823388, 0…\n$ Visual_Lanc        &lt;dbl&gt; -2.2545455, 0.6103733, 1.3354358, -0.4342084, -0.34…\n$ Auditory_Lanc      &lt;dbl&gt; -0.84225787, -0.35605108, 1.54797548, 0.18795651, 1…\n$ Haptic_Lanc        &lt;dbl&gt; -0.75523987, -0.29089287, 1.25099360, -0.18911818, …\n$ Olfactory_Lanc     &lt;dbl&gt; -0.14444936, -0.37350419, -0.53335522, -0.37350419,…\n$ Gustatory_Lanc     &lt;dbl&gt; 0.27698988, -0.10105698, -0.36148925, -0.52110903, …\n$ Interoceptive_Lanc &lt;dbl&gt; 1.08153427, -0.06560311, 1.64313895, 1.45452985, 0.…",
    "crumbs": [
      "Multivariate Data Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "PCA.html#descriptive-overview",
    "href": "PCA.html#descriptive-overview",
    "title": "26  Principal Components Analysis",
    "section": "26.3 Descriptive overview",
    "text": "26.3 Descriptive overview\nA popular descriptive measure for associations between continuous variables \\(x\\) and \\(y\\) is the Pearson product-moment correlation coefficient (or simply Pearson’s \\(r\\); cf. Equation 26.1). It varies on a scale from \\(-1\\) to \\(1\\) and indicates the extent to which two variables form a straight-line relationship (Heumann, Schomaker, and Shalabh 2022: 153-154). One of its core components is the covariance between \\(x\\) and \\(y\\) which “measures the average tendency of two variables to covary (change together)” (Baguley 2012: 206).\n\\[\nr_{xy} = \\frac{Cov(x, y)}{\\sqrt{Var(x)}\\sqrt{Var(y)}}= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}.\n\\tag{26.1}\\]\nIn R, we can compute Pearson’s \\(r\\) by using the cor() function.\n\n# Check correlation between number of senses and concreteness\ncor(scope_sem_sub[,-1]$Nsenses_WordNet, scope_sem_sub[,-1]$Conc_Brys) # low\n\n[1] 0.2351554\n\n# Check correlation between haptic experience and concreteness\ncor(scope_sem_sub[,-1]$Haptic_Lanc, scope_sem_sub[,-1]$Conc_Brys) # high\n\n[1] 0.5676945\n\n\nIf the data frame consists of numeric columns only (i.e., if it is a matrix), we can apply cor() to the full dataset and obtain the correlation matrix (also known as covariance matrix).\n\n# Generate correlation matrix\ncor_mat1 &lt;- cor(scope_sem_sub[,-1])\n\nhead(cor_mat1)\n\n                   Resnik_strength  Conc_Brys Nsenses_WordNet\nResnik_strength         1.00000000  0.1166670     -0.37442983\nConc_Brys               0.11666697  1.0000000      0.23515537\nNsenses_WordNet        -0.37442983  0.2351554      1.00000000\nNmeanings_Websters     -0.34225250  0.2023356      0.68509560\nVisual_Lanc             0.05471417  0.5519836      0.17154846\nAuditory_Lanc          -0.11162700 -0.2683646     -0.02960745\n                   Nmeanings_Websters Visual_Lanc Auditory_Lanc  Haptic_Lanc\nResnik_strength            -0.3422525  0.05471417   -0.11162700  0.008260683\nConc_Brys                   0.2023356  0.55198358   -0.26836458  0.567694470\nNsenses_WordNet             0.6850956  0.17154846   -0.02960745  0.239470104\nNmeanings_Websters          1.0000000  0.14597243   -0.04656650  0.193226862\nVisual_Lanc                 0.1459724  1.00000000   -0.11674896  0.404536416\nAuditory_Lanc              -0.0465665 -0.11674896    1.00000000 -0.289586292\n                   Olfactory_Lanc Gustatory_Lanc Interoceptive_Lanc\nResnik_strength        0.05131717    0.015087346      -9.459551e-02\nConc_Brys              0.21354305    0.123459754      -3.257702e-01\nNsenses_WordNet       -0.03353627   -0.018262178      -1.392934e-02\nNmeanings_Websters    -0.01898766    0.001409646      -4.460375e-05\nVisual_Lanc            0.15319007    0.055176064      -3.424087e-01\nAuditory_Lanc         -0.06123191   -0.047086877       1.799479e-01\n\n# Plot correlation matrix\ncorrplot(cor_mat1, col = topo.colors(200), tl.col = \"darkgrey\", number.cex = 0.5, tl.cex = 0.5)\n\n\n\n\n\n\n\n\nSince the upper triangle mirrors the lower one, it is enough to only examine one of them. The diagonal values are not particularly insightful and can be ignored.\n\n# Levelplot\nseq1 &lt;- seq(-1, 1, by = 0.01)\n\nlevelplot(cor_mat1, aspect = \"fill\", col.regions = topo.colors(length(seq1)),\n          at = seq1, scales = list(x = list(rot = 45)),\n          xlab = \"\", ylab = \"\")\n\n\n\n\n\n\n\n\nNeedless to say, the above correlation matrices are hard to interpret – even more so if the number of variables were to increase further.\nPrincipal Components Analysis offers a technique to break down a high-dimensional dataset into a much smaller set of “meta-variables”, i.e., principle components (PCs) which capture the bulk of the variance in the data. This is also known as dimension reduction, which allows researchers to see overarching patterns in the data and re-use the output for further analysis (e.g., clustering or predictive modelling).",
    "crumbs": [
      "Multivariate Data Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "PCA.html#basics-of-pca",
    "href": "PCA.html#basics-of-pca",
    "title": "26  Principal Components Analysis",
    "section": "26.4 Basics of PCA",
    "text": "26.4 Basics of PCA\nPCA “repackages” large sets of variables by forming uncorrelated linear combinations of them, yielding \\(k\\) principal components \\(Z_1, ..., Z_k\\) (PCs hf.) of the dataset (for \\(1, ..., k\\)). PCs are ordered such that the first PC explains the most variance in the data, with each subsequent PC explaining the maximum remaining variance while being uncorrelated with previous PCs.\nEach PC comprises a set of loadings (or weights) \\(w_{nm}\\), which are comparable to the coefficients of regression equations. For instance, the first PC has the general form shown in Equation 26.2, where \\(x_m\\) stand for continuous input variables in the \\(n \\times m\\) data matrix \\(\\mathbf{X}\\).\n\\[\nZ_{1} = w_{11}\n\\begin{pmatrix}\nx_{11} \\\\\nx_{21} \\\\\n\\vdots \\\\\nx_{n1}\n\\end{pmatrix}\n+ w_{21}\n\\begin{pmatrix}\nx_{12} \\\\\nx_{22} \\\\\n\\vdots \\\\\nx_{n2}\n\\end{pmatrix}\n+ \\dots + w_{m1}\n\\begin{pmatrix}\nx_{1m} \\\\\nx_{2m} \\\\\n\\vdots \\\\\nx_{nm}\n\\end{pmatrix}\n\\tag{26.2}\\]\nIf a feature positively loads on a principal component (i.e., \\(w &gt; 0\\)), it means that as the value of this feature increases, the score for this principal component also increases. The magnitude of \\(w\\) indicates the strength of this relationship. Conversely, negative loadings (\\(w &lt; 0\\)) indicate that as the feature value increases, the PC score decreases as well.\n\n\n\n\n\n\nHow do we find PCs?\n\n\n\n\n\nPCs are identified using common techniques from matrix algebra, namely singular value decomposition and eigenvalue decomposition. By breaking down the input data into products of several further matrices, it becomes possible to characterise the exact ‘shape’ of its variance (Mair 2018: 181).\n\n\n\nThe figure below offers a visual summary of PCA:",
    "crumbs": [
      "Multivariate Data Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "PCA.html#application-in-r",
    "href": "PCA.html#application-in-r",
    "title": "26  Principal Components Analysis",
    "section": "26.5 Application in R",
    "text": "26.5 Application in R\n\n26.5.1 Fitting the model and identifying number of PCs\nFirst, we fit a PCA object with the number of PCs equivalent to the number of columns in scope_sem_sub.\n\n# Fit initial PCA\npca1 &lt;- principal(scope_sem_sub[,-1],\n                  nfactors = ncol(scope_sem_sub[,-1]),\n                  rotate = \"none\")\n\n# Print loadings\nloadings(pca1)\n\n\nLoadings:\n                   PC1    PC2    PC3    PC4    PC5    PC6    PC7    PC8   \nResnik_strength            0.666 -0.271 -0.100  0.250  0.627              \nConc_Brys           0.813  0.210 -0.173         0.149        -0.170 -0.260\nNsenses_WordNet     0.523 -0.696  0.124                0.241              \nNmeanings_Websters  0.493 -0.683  0.149                0.343              \nVisual_Lanc         0.691  0.168 -0.236  0.382  0.152 -0.127  0.484  0.136\nAuditory_Lanc      -0.388 -0.228  0.199  0.734  0.413        -0.208       \nHaptic_Lanc         0.728  0.113        -0.272  0.401 -0.254 -0.266  0.120\nOlfactory_Lanc      0.324  0.444  0.671  0.163 -0.200               -0.347\nGustatory_Lanc      0.256  0.377  0.759        -0.160                0.377\nInteroceptive_Lanc -0.341 -0.164  0.577 -0.366  0.543         0.265 -0.100\n                   PC9    PC10  \nResnik_strength                 \nConc_Brys          -0.378       \nNsenses_WordNet            0.405\nNmeanings_Websters        -0.373\nVisual_Lanc                     \nAuditory_Lanc                   \nHaptic_Lanc         0.266       \nOlfactory_Lanc      0.234       \nGustatory_Lanc     -0.187       \nInteroceptive_Lanc -0.120       \n\n                 PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10\nSS loadings    2.629 1.898 1.595 0.937 0.806 0.657 0.461 0.378 0.330 0.309\nProportion Var 0.263 0.190 0.160 0.094 0.081 0.066 0.046 0.038 0.033 0.031\nCumulative Var 0.263 0.453 0.612 0.706 0.787 0.852 0.898 0.936 0.969 1.000\n\n\nIt is common practice to retain only those PCs with eigenvalues (variances) \\(&gt; 1\\) (cf. scree plot).\n\n# Scree plot\nbarplot(pca1$values, main = \"Scree plot\", ylab = \"Variances\", xlab = \"PC\", # first three PCs\n        names.arg = 1:length(pca1$values))\n  abline(h = 1, col = \"blue\", lty = \"dotted\")\n\n\n\n\n\n\n\n\nAlternatively, one can perform parallel analysis to identify statistically significant PCs whose variances are “larger than the 95% quantile […] of those obtained from random or resampled data” (Mair 2018: 31). The corresponding function is fa.parallel() from the psych package.\n\npca.pa &lt;- fa.parallel(scope_sem_sub[,-1], # raw data\n                     fa = \"pc\", # Use PCA instead of factor analysis\n                     cor = \"cor\",  # Use Pearson correlations (default for PCA)\n                     n.iter = 200, # Number of iterations (increase for more stable results)\n                     quant = 0.95, # Use 95th percentile (common choice)\n                     fm = \"minres\") # Factor method\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  3 \n\n\n\n\n26.5.2 Accessing and visualising the loadings\nSince three PCs appear to be enough to explain the majority of variance in the data, we will refit the model with nfactors = 3.\n\npca2 &lt;- principal(scope_sem_sub[,-1],\n                  nfactors = 3,\n                  rotate = \"none\")\n\nA convenient function for printing the PCA loadings is loadings(). Weights close to \\(0\\) are not displayed.\n\nloadings(pca2)\n\n\nLoadings:\n                   PC1    PC2    PC3   \nResnik_strength            0.666 -0.271\nConc_Brys           0.813  0.210 -0.173\nNsenses_WordNet     0.523 -0.696  0.124\nNmeanings_Websters  0.493 -0.683  0.149\nVisual_Lanc         0.691  0.168 -0.236\nAuditory_Lanc      -0.388 -0.228  0.199\nHaptic_Lanc         0.728  0.113       \nOlfactory_Lanc      0.324  0.444  0.671\nGustatory_Lanc      0.256  0.377  0.759\nInteroceptive_Lanc -0.341 -0.164  0.577\n\n                 PC1   PC2   PC3\nSS loadings    2.629 1.898 1.595\nProportion Var 0.263 0.190 0.160\nCumulative Var 0.263 0.453 0.612\n\n\nIn order to see what features load particularly strongly on the PCs, we can draw a path diagram with diagram(). Note that the red arrows indicate negative weights (i.e., negative “regression coefficients”).\n\ndiagram(pca2, main = NA)\n\n\n\n\n\n\n\n\nThe generic plot method returns a scatterplot of the loadings:\n\nplot(pca2, labels = colnames(scope_sem_sub[,-1]), main = NA)\n\n\n\n\n\n\n\n\nFinally, you can obtain the PC scores for each observation in the input data by accessing the $scores element:\n\nhead(pca2$scores, n = 15)\n\n              PC1         PC2         PC3\n [1,] -1.45999990  0.38323657  0.61549383\n [2,] -0.32170158 -0.60027352 -0.08271852\n [3,]  0.12196548 -0.68757984  0.33959072\n [4,] -0.57929327 -0.35785887  0.26877126\n [5,] -0.34097381 -1.35963060  0.54717808\n [6,] -0.04799048 -0.34404820 -0.19668070\n [7,] -0.33873248  0.52372694 -0.28318588\n [8,] -1.11868861  0.26178424 -0.66465979\n [9,]  0.15263031  0.60489417 -0.84324699\n[10,] -1.75834143 -0.47957110  0.44313029\n[11,] -1.26440095 -1.15766536  0.46594800\n[12,]  0.10641410  0.05075197  0.48556702\n[13,] -1.26133394 -0.35022899 -0.36512925\n[14,] -0.28070472  0.60992380 -1.29547347\n[15,] -0.72805598 -0.45777808  0.56031788\n\n\nBiplots offer juxtaposed visualisations of PC scores (points) and loadings (arrows).\n\n# PC1 and PC2\nbiplot(pca2, choose = c(1, 2), \n       main = NA, pch = 20, col = c(\"darkgrey\", \"blue\"))\n\n\n\n\n\n\n\n# PC2 and PC3\nbiplot(pca2, choose = c(2, 3), \n       main = NA, pch = 20, col = c(\"darkgrey\", \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the PCA output\n\n\n\nAfter inspecting the loadings and biplots, we can see the following patterns:\n\nExternal sensation: Higher ratings in concreteness (i.e., direct perception with one’s senses) as well as the visual and haptic dimensions of verbs are associated with an increase in PC1.\nSenses and selection: PC2 displays notable negative loadings in features relating to the number of meanings a verb has and how much information it carries about the meaning of its objects. PC2 scores decrease if a verb has fewer meanings, but they increase if it displays higher selectional preference strength.\nInternal sensation: PC3 captures variance in olfactory, gustatory and interoceptive2 ratings.\n\n2 Here interoceptive means “[t]o what extent one experiences the referent by sensations inside one’s body” (Gao, Shinkareva, and Desai 2022: 2859).\n\n\n\n\n\nBaguley, Thomas. 2012. Serious Stats: A Guide to Advanced Statistics for the Behavioral Sciences. Houndmills, Basingstoke: Palgrave Macmillan.\n\n\nGao, Chuanji, Svetlana V. Shinkareva, and Rutvik H. Desai. 2022. “SCOPE: The South Carolina Psycholinguistic Metabase.” Behavior Research Methods 55 (6): 2853–84. https://doi.org/10.3758/s13428-022-01934-0.\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nLevshina, Natalia. 2015. How to Do Linguistics with r: Data Exploration and Statistical Analysis. Amsterdam; Philadelphia: John Benjamins Publishing Company.\n\n\nMair, Patrick. 2018. Modern Psychometrics with R. Cham: Springer. https://doi.org/10.1007/978-3-319-93177-7.\n\n\nResnik, Philip. 1996. “Selectional Constraints: An Information-Theoretic Model and Its Computational Realization” 61 (1): 127–59.",
    "crumbs": [
      "Multivariate Data Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  }
]