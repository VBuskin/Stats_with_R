[
  {
    "objectID": "Libraries.html#installing-and-loading-packages",
    "href": "Libraries.html#installing-and-loading-packages",
    "title": "4  Libraries",
    "section": "4.1 Installing and loading packages",
    "text": "4.1 Installing and loading packages\nPackages expand the basic functionality of R by providing numerous quality-of-life improvements that not only considerably simplify common data wrangling tasks but which also provide frameworks for state-of-the-art methods for statistical analysis and natural language processing (NLP), among many other things.\n\n\n\n\n\n\nHow do I install a library?\n\n\n\n\n\nNnavigate to Packages &gt; Install and verify that the pop-up window says Install from: Repository (CRAN). You can now type in the name of the package you would like to install under Packages.\nVideo tutorial on YouTube\n\n\n\nThis reader will use functions from a variety of R packages. Please install the following ones:\n\nquanteda (for the analysis of text data)\ntidyverse (a framework for data manipulation and visualisation)\nreadxl (for importing Microsoft Excel files)\nwritexl (for exporting Microsoft Excel files)\nkableExtra (for creating beautiful tables)\n\nOnce the installation has been completed, you can proceed to load the libraries using the code below. You can ignore the warning messages.\n\nlibrary(quanteda)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(writexl)\nlibrary(kableExtra)\n\n\n\n\n\n\n\nActivating libraries\n\n\n\n\n\nWhenever you start a new R session (i.e., open RStudio), your libraries and their respective functions will be inactive. To re-activate a library, either use the library() function or simply select it in the Packages tab.\n\n\n\nIt is good practice to only activate those packages that are necessary for your analysis. While it won’t be a problem for the small set of packages as shown here, loading dozens of packages increases the risk of obtaining “homonymous” functions which have the same name but perform different operations. In this case, it might be helpful to “disambiguate” them by directly indicating which package a function is from:\n\nreadxl::read_xlsx(...)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Libraries</span>"
    ]
  },
  {
    "objectID": "Exploring_RStudio.html#the-rstudio-interface",
    "href": "Exploring_RStudio.html#the-rstudio-interface",
    "title": "1  Exploring RStudio",
    "section": "1.1 The RStudio interface",
    "text": "1.1 The RStudio interface\nOnce you’ve opened RStudio, you will see several empty panes similar to this:\n\n\n\n\n\n\n1.1.1 Console\nLet’s focus on the console window on the left. This is where we can directly communicate with R by entering “code”. The way this code is written follows certain arbitrary conventions – just like natural languages such as English or German. Here is an example in the R programming language:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nEntering this command into your console and pressing ENTER will display the sentence “Hello world!” in a new line.\nAs we can see, anything that R should understand as a simple sequence of letters or words must be enclosed by quotation marks \"...\". Anything inside them will be interpreted as a so-called string. Their counterpart are numbers or integers, as illustrated here:\n\n2 + 2 - 1\n\n[1] 3\n\n\nNaturally, you can use R for more sophisticated computations:\n\n\\(2 + (3 \\times 3)\\)\n\n2 + (3 * 3)\n\n[1] 11\n\n\n\\(\\sqrt{9}\\)\n\nsqrt(9)\n\n[1] 3\n\n\n\\(\\frac{16}{2^3}\\)\n\n16 / 2 ^ 3\n\n[1] 2\n\n\n\n\n\n1.1.2 Working environment\nWhile it is certainly useful to print text or numbers to your console, it may sometimes make more sense to (at least temporally) store them somewhere, so you can re-use them later when you need them. In fact, R gives us a way of storing data in virtual, container-like objects: variables. We can assign strings or numbers to a variable by using the assignment operator &lt;-.\nWhen you run the commands below, you will (hopefully) notice two items popping up in your Environment/Workspace tab in the top right corner.\n\ngreeting &lt;- \"Hello world!\"\n\nquick_math &lt;- 2 + 2 - 1\n\nIf we now want to display the content in the console, we can simply apply the print() function to the variable itself:\n\nprint(greeting)\n\n[1] \"Hello world!\"\n\nprint(quick_math)\n\n[1] 3\n\n\nWe can also embed variables in other statements. For example, let’s take the content of quick_math and multiply it with 2.\n\nhard_math &lt;- quick_math * 2\n\nprint(hard_math)\n\n[1] 6\n\n\n\n\n1.1.3 R Scripts\nWorking with the console has a very plain, yet important disadvantage: Once we close RStudio, the console is wiped clean, erasing everything we’ve typed into it during our precious R session.\nThe remedy for this nuisance are scripts. Essentially, a script is the programmer’s equivalent of a Word document: It allows you to save all the code you’ve written to a file, which you can seamlessly continue working on the next time you open it.\n\nTo create a new R script you can either go to:\n\n“File” \\(\\rightarrow\\) “New” \\(\\rightarrow\\) “R Script” or …\nclick on the icon with the + sign and select “R Script” …\nor simply press Ctrl+Shift+N (MacOS: Cmd+Shift+N)\n\n\nDon’t forget to save your script with Ctrl+S (Cmd+S)! It is good practice to save your files regularly.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exploring RStudio</span>"
    ]
  },
  {
    "objectID": "Vectors_Factors.html#suggested-reading",
    "href": "Vectors_Factors.html#suggested-reading",
    "title": "2  Vectors",
    "section": "2.1 Suggested reading",
    "text": "2.1 Suggested reading\n\nGries (2013: Chapter 2)\nJames et al. (2021: Chapter 2.3)\nWinter (2019: Chapter 1)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Vectors_Factors.html#word-frequencies-i",
    "href": "Vectors_Factors.html#word-frequencies-i",
    "title": "2  Vectors",
    "section": "2.2 Word frequencies I",
    "text": "2.2 Word frequencies I\nYou are given the following token counts of English verb lemmas in the International Corpus of English:\n\n\n\nLemma\nFrequency\n\n\n\n\nstart\n418\n\n\nenjoy\n139\n\n\nbegin\n337\n\n\nhelp\n281\n\n\n\nIt is always a good idea to visualise frequency data in some way. Quite conveniently, R happens to provide us with an abundance of plotting functions. To create a two-dimensional plot, we need to generate two objects in R: one for the individual lemmas and one for the frequency counts.\nLet’s combine the lemmas start, enjoy, begin and help using R’s c() function and store them in an object lemma. Enter the following line into a new R script and click on Run (or simply press Ctrl+Enter/Cmd+Enter).\n\nlemma &lt;- c(\"start\", \"enjoy\", \"begin\", \"help\")\n\nWe can now do the same for the frequency information:\n\nfrequency &lt;- c(418, 139, 337, 281)\n\n\n\n\n\n\n\nWhen do I use quotation marks?\n\n\n\n\n\nLetters and numbers represent two distinct data types in R. Anything that should be understood as a simple sequence of letters or words must be enclosed by quotation marks \"...\". An expression such as start will then be evaluated as a string.\nNumbers (or integers), by contrast, appear without quotation marks.\n\n\n\nOur linguistic data is now stored in two variables lemma and frequency, which you can conceptualise as virtual container-like objects. You will also notice that they are now showing in the Environment tab in the top right corner of RStudio.\nThe combination of categorical labels and numeric information renders our data ideally suited for a barplot. R’s most basic barplot function (which is, unsurprisingly, called barplot()) needs at the very least …\n\na height argument, i.e., our y-axis values and\na names.arg argument, i.e., our x-axis labels.\n\n\nbarplot(frequency, names.arg = lemma, col = \"skyblue\")\n\n\n\n\n\n\n\n\nAfter some tinkering, our plot looks more presentable:\n\nbarplot(frequency, names.arg = lemma, \n        main = \"Frequency of Lemmas\", # title\n        xlab = \"Lemmas\",  # label for x-axis\n        ylab = \"Frequency\", # label for y-axis\n        col = \"steelblue\") # color\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does ‘#’ mean? On comments in R\n\n\n\n\n\nIn R, everything followed by the hashtag # will be interpreted as a comment and won’t be evaluated by the R compiler. While comments don’t affect the output of our code in the slightest, they are crucial to any kind of programming project.\nAdding prose annotations to your code will make not only it easier to understand for others but also for your future self. Poor documentation is a common, yet unnecessary source of frustration for all parties involved …\n\n\n\n\n\n\n\n\nIn RStudio, you now have the option to save the plot to your computer. Once the figure has appeared in your “Plots” panel, you can click on “Export” in the menu bar below and proceed to choose the desired output format and file directory.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Vectors_Factors.html#some-technical-details",
    "href": "Vectors_Factors.html#some-technical-details",
    "title": "2  Vectors",
    "section": "2.3 Some technical details",
    "text": "2.3 Some technical details\nThe example above demonstrates one of the most important data structures in R: Vectors. They form the cornerstone of various more complex objects such as data frames, and are essential to handling large data sets (e.g., corpora). And yet, vectors are very simple in that they merely constitute one-dimensional sequences of characters or numbers – no more, no less.\n\nprint(lemma)\n\n[1] \"start\" \"enjoy\" \"begin\" \"help\" \n\nprint(frequency)\n\n[1] 418 139 337 281\n\n\nThe individual elements in these two vectors are not randomly jumbling around in virtual space, but are in fact following a clear order. Each element has an “ID” (or index), by which we can access it. For example, if we want to print the first lemma in our lemma variable, we can use this notation:\n\nlemma[1]\n\n[1] \"start\"\n\n\nSimilarly, we can subset frequency according to, for example, its third element:\n\nfrequency[3]\n\n[1] 337\n\n\nWe can also obtain entire ranges of elements, such as everything from the second to the fourth one:\n\nfrequency[2:4]\n\n[1] 139 337 281",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Vectors_Factors.html#practical",
    "href": "Vectors_Factors.html#practical",
    "title": "2  Vectors",
    "section": "2.4 Practical",
    "text": "2.4 Practical\n\nCreate a vector that lists the third person personal pronouns of English (subject and object forms). Store them in a variable pp3.\n\n\n\nSolution:\npp3 &lt;- c(\"he\", \"she\", \"it\", \"him\", \"her\", \"they\", \"them\")\n\n\n\nNow print …\n\n… the fourth element in pp3.\n\n\n\nSolution:\nprint(pp3[4]) # or simply\n\npp3[4]\n\n\n\n… elements 3 through 5.\n\n\n\nSolution:\npp3[3:5]\n\n\n\n… all elements.\n\n\n\nSolution:\npp3\n\n\n\n… elements 1, 3 and 5.\n\n\n\nSolution:\npp3[c(1, 3, 5)]\n\n\nWhen working with large datasets, we often don’t know whether an element is in the vector to begin with, let alone its position. For instance, if we wanted to check whether they is in pp3 or not, we could use the handy notation below, returning a TRUE or FALSE value:\n\n\n\"they\" %in% pp3\n\n[1] TRUE\n\n\nAscertain whether the following items are in pp3:\n\nhim\n\n\nSolution:\n\"him\" %in% pp3 # TRUE\n\n\nyou\n\n\nSolution:\n\"you\" %in% pp3 # FALSE\n\n\nit and them\n\n\nSolution:\nc(\"it\", \"them\") %in% pp3 # TRUE TRUE\n\n\nwe, us and me\n\n\nSolution:\nc(\"we\", \"us\", \"them\") %in% pp3 # FALSE FALSE TRUE\n\n\n\n\nOnce we are sure that an element is in the vector of interest, another common problem that arises is finding its location. Luckily, R has got us covered! The which() function returns the index of an element. You can read this notation as “Which element in pp3 is they?”. The output suggests that is in position 6. Note that the number obtained depends on the order of elements you’ve chosen when creating pp3.\n\n\nwhich(pp3 == \"they\") # Note the two equal signs == !\n\n[1] 6\n\n\nFind the locations of it and them in pp3.\n\n\nSolution:\n# \"him\"\nwhich(pp3 == \"it\")\n\n# \"you\"\nwhich(pp3 == \"them\")",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Data_frames.html#word-frequencies-ii",
    "href": "Data_frames.html#word-frequencies-ii",
    "title": "3  Data frames",
    "section": "3.1 Word frequencies II",
    "text": "3.1 Word frequencies II\nRecall our corpus-linguistic data from the previous unit:\n\n\n\nLemma\nFrequency\n\n\n\n\nstart\n418\n\n\nenjoy\n139\n\n\nbegin\n337\n\n\nhelp\n281\n\n\n\nWe thought of the columns as one-dimensional, indexed lists of elements:\n\nlemma &lt;- c(\"start\", \"enjoy\", \"begin\", \"help\")\n\nfrequency &lt;- c(418, 139, 337, 281)\n\nActually, R allows to combine these two vectors into something that resembles a real spreadsheet. To this end, we need to apply the data.frame() to two vectors of our choice.\n\ndata &lt;- data.frame(lemma, frequency)\n\nprint(data)\n\n  lemma frequency\n1 start       418\n2 enjoy       139\n3 begin       337\n4  help       281\n\n\nThe variable data is no longer a vector, but a data frame (often abbreviated as ‘df’). Once again, each element carries its own label and can, therefore, be accessed or manipulated.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "Data_frames.html#some-technical-details",
    "href": "Data_frames.html#some-technical-details",
    "title": "3  Data frames",
    "section": "3.2 Some technical details",
    "text": "3.2 Some technical details\nSince we now have two dimensions, the subsetting notation in square brackets [ ] has to reflect that. This is the general pattern:\n\\[ df[row, column] \\] Following this logic, we can get the element in the first row of the first column like so:\n\ndata[1,1]\n\n[1] \"start\"\n\n\nIf we, however, need the entire first row, we simply omit the column part. Note that the comma , still needs to be present!\n\ndata[1,]\n\n  lemma frequency\n1 start       418\n\n\nSubsetting by columns is interesting. We can either use the explicit notation with square brackets or the column operator $:\n\ndata[,1]\n\n[1] \"start\" \"enjoy\" \"begin\" \"help\" \n\ndata$lemma\n\n[1] \"start\" \"enjoy\" \"begin\" \"help\"",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "Data_frames.html#practical",
    "href": "Data_frames.html#practical",
    "title": "3  Data frames",
    "section": "3.3 Practical",
    "text": "3.3 Practical\n\nRecreate the barplot from the previous unit by subsetting the data variable accordingly.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "Concordancing.html#introduction",
    "href": "Concordancing.html#introduction",
    "title": "5  Querying a corpus",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nThis section offers a short introduction to corpus queries in R. Our goal will be to obtain data on the genitive alternation in British English. To this end, we will rely on the British component of the International Corpus of English (ICE-GB).",
    "crumbs": [
      "Corpus linguistics with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Querying a corpus</span>"
    ]
  },
  {
    "objectID": "Concordancing.html#preparation",
    "href": "Concordancing.html#preparation",
    "title": "5  Querying a corpus",
    "section": "5.2 Preparation",
    "text": "5.2 Preparation\n\n5.2.1 Directory structure\nIn order for R to be able to recognise the data, it is crucial to set up the working directory correctly.\n\nMake sure your R-script and the corpus (e.g., ‘ICE-GB’) are stored in the same folder.\nIn RStudio, now navigate to Session &gt; Set working directory &gt; To Source File Location. This ensures that the folder where you have placed your R-script will function as your working directory until you close RStudio again. To see your working directory in your files pane, click on Files &gt; 'Blue wheel symbol' &gt; Go to working directory.\n\n\n\n5.2.2 Installing and loading packages (move to earlier session)\nPackages expand the basic functionality of R by providing numerous quality-of-life improvements that not only considerably simplify common data wrangling tasks but which also provide frameworks for state-of-the-art methods for statistical analysis and natural language processing (NLP).\nIn order to install a package, you navigate to Packages &gt; Install and verify that the pop-up window says Install from: Repository (CRAN). You can now type in the name of the package you would like to install under Packages.\nTask: Install the tidyverse and quanteda packages!\nOnce the installation has been completed, you can proceed to load the libraries using the code below. You can ignore the warning messages.\n\nlibrary(tidyverse)\nlibrary(quanteda)\n\nRemember that you will have to reload these libraries whenever you start a new R session (i.e., open RStudio).",
    "crumbs": [
      "Corpus linguistics with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Querying a corpus</span>"
    ]
  },
  {
    "objectID": "Concordancing.html#loading-the-corpus",
    "href": "Concordancing.html#loading-the-corpus",
    "title": "5  Querying a corpus",
    "section": "5.3 Loading the corpus",
    "text": "5.3 Loading the corpus\nAfter specifying the working directory and loading the libraries we will need, we can read in the corpus files into a corpus object in R.\nFirst, simply copy-paste the following code chunk at the beginning of your R-script. Once you run it, it will load the function read_GB() into R’s working memory (it should now appear in the Environment tab!). This function will automatically handle the entire reading-in process.\nTo now get all corpus files into R, all we have to do is call the function:\n\nICE_GB &lt;- readRDS(\"ICE_GB.RDS\")\n\nIf you encounter error messages, make sure you followed steps 1 and 2 above.",
    "crumbs": [
      "Corpus linguistics with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Querying a corpus</span>"
    ]
  },
  {
    "objectID": "Concordancing.html#concordancing",
    "href": "Concordancing.html#concordancing",
    "title": "5  Querying a corpus",
    "section": "5.4 Concordancing",
    "text": "5.4 Concordancing\n\nlibrary(quanteda)\nlibrary(tidyverse)\n\n# Load corpus\nICE_GB &lt;- readRDS(\"ICE_GB.RDS\")\n\n\n5.4.0.1 Basic use\n\nConcordances\n\n\n# Query the corpus\nquery1 &lt;- kwic(ICE_GB, pattern = \"provid(e|es|ing|ed)\", valuetype = \"regex\")\n\nquery1 %&gt;%\n  as_tibble() %&gt;% \n  count(keyword)\n\n# A tibble: 10 × 2\n   keyword          n\n   &lt;chr&gt;        &lt;int&gt;\n 1 Provided         5\n 2 Provident        1\n 3 Providing        1\n 4 provide        165\n 5 provided       118\n 6 providential     1\n 7 provider         1\n 8 providers        3\n 9 provides        72\n10 providing       52\n\n# Print first six lines to console\n#head(as_tibble(kwic_provide))\n\n# View output in separate window\n#View(kwic_provide)\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-003.txt\n1994\n1994\n&gt; Uhm or whatever they\nprovide\n&lt; ICE-GB:S1A-003 #82 : 1\nprovid(e|es|ing|ed)\n\n\nICE_GB/S1A-010.txt\n827\n827\nquandary about what to to\nprovide\n&lt; , , &gt; &lt;\nprovid(e|es|ing|ed)\n\n\nICE_GB/S1A-023.txt\n3230\n3230\nfrom there is Gospel Oak\nprovided\nyou know the time of\nprovid(e|es|ing|ed)\n\n\nICE_GB/S1A-024.txt\n1851\n1851\nwhat L S E is\nproviding\n&lt; ICE-GB:S1A-024 #92 : 1\nprovid(e|es|ing|ed)\n\n\nICE_GB/S1A-031.txt\n998\n998\nher own &lt; , &gt;\nproviding\nher &lt; ICE-GB:S1A-031 #47 :\nprovid(e|es|ing|ed)\n\n\nICE_GB/S1A-038.txt\n730\n730\nthis as viewing banking as\nproviding\n&lt; , , &gt; &lt;\nprovid(e|es|ing|ed)\n\n\n\n\n\n\n\n\n\n5.4.0.2 Increase search window\n\n# Query the corpus\nkwic_provide2 &lt;- kwic(tokens(ICE_GB_corpus),\n                     pattern = \"provide\",\n                     window = 20) # choose window size\n\n# View output in separate window\n#View(kwic_provide)\n\n\n\n5.4.0.3 Regular expressions\nRegular expressions (or ‘regex’) help us find more complex patterns in strings of text. Suppose we are interested in finding all inflectional forms of the lemma PROVIDE in a corpus, i.e., provide, provides, providing and provided. Insteading of searching for all forms individually, we can construct a regular expression of the form\n\\[\n\\text{provide(s | ing | ed)?}\n\\] which can be read as ‘Match the sequence of letters &lt;provide&gt; as well as when it is optionally followed by the letters &lt;s&gt; or &lt;ing&gt; or &lt;ed&gt;’. Notice how optionality is signified by the ‘?’ operator and alternatives by ‘|’.\nTo activate regular expression in a KWIC query, simply set the valuetype argument to \"regex\":\n\n# Query the corpus\nkwic_provide3 &lt;- kwic(ICE_GB,\n                     pattern = \"provide(s|ing|ed)?\",\n                     valuetype = \"regex\", # query format\n                     window = 20)\n\n                     \n\n# View output in separate window\n#View(kwic_provide)\n\nThe number of hits has more than doubled. However, upon closer inspection, we’ll notice a number of false positives (e.g., providential).\n\n\n5.4.1 RegEx: A Cheatsheet\n\n\n5.4.2 Example usage\n\n\n5.4.3 Exporting the results",
    "crumbs": [
      "Corpus linguistics with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Querying a corpus</span>"
    ]
  },
  {
    "objectID": "Variables.html#preparation",
    "href": "Variables.html#preparation",
    "title": "7  Variables in statistics",
    "section": "7.1 Preparation",
    "text": "7.1 Preparation\nPlease download the file “Paquot_Larsson_2020_data.xlsx” (Paquot and Larsson 2020)1 and store it in your working directory.\n\n# Libraries\nlibrary(\"nycflights13\")\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\nlibrary(\"ggthemes\")\n\n# Load data\ncl.order &lt;- read_xlsx(\"Paquot_Larsson_2020_data.xlsx\")\n\n# Inspect data\nstr(cl.order)\n\ntibble [403 × 8] (S3: tbl_df/tbl/data.frame)\n $ CASE       : num [1:403] 4777 1698 953 1681 4055 ...\n $ ORDER      : chr [1:403] \"sc-mc\" \"mc-sc\" \"sc-mc\" \"mc-sc\" ...\n $ SUBORDTYPE : chr [1:403] \"temp\" \"temp\" \"temp\" \"temp\" ...\n $ LEN_MC     : num [1:403] 4 7 12 6 9 9 9 4 6 4 ...\n $ LEN_SC     : num [1:403] 10 6 7 15 5 5 12 2 24 11 ...\n $ LENGTH_DIFF: num [1:403] -6 1 5 -9 4 4 -3 2 -18 -7 ...\n $ CONJ       : chr [1:403] \"als/when\" \"als/when\" \"als/when\" \"als/when\" ...\n $ MORETHAN2CL: chr [1:403] \"no\" \"no\" \"yes\" \"no\" ...\n\nhead(cl.order)\n\n# A tibble: 6 × 8\n   CASE ORDER SUBORDTYPE LEN_MC LEN_SC LENGTH_DIFF CONJ     MORETHAN2CL\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      \n1  4777 sc-mc temp            4     10          -6 als/when no         \n2  1698 mc-sc temp            7      6           1 als/when no         \n3   953 sc-mc temp           12      7           5 als/when yes        \n4  1681 mc-sc temp            6     15          -9 als/when no         \n5  4055 sc-mc temp            9      5           4 als/when yes        \n6   967 sc-mc temp            9      5           4 als/when yes",
    "crumbs": [
      "Descriptive statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Understanding and visualising variables</span>"
    ]
  },
  {
    "objectID": "Variables.html#types-of-variables",
    "href": "Variables.html#types-of-variables",
    "title": "7  Variables in statistics",
    "section": "7.2 Types of variables",
    "text": "7.2 Types of variables\nThe concept of the variable allows us to quantify various aspects of our observations.\n\nnominal/categorical: These variables have a limited number of levels which cannot be ordered in a meaningful way. For instance, it does not matter which value of SUBORDTYPE or MORETHAN2CL comes first or last:\n\nunique(cl.order$SUBORDTYPE)\n\n[1] \"temp\" \"caus\"\n\nunique(cl.order$MORETHAN2CL)\n\n[1] \"no\"  \"yes\"\n\n\nordinal: Such variables can be ordered, but the intervals between their individuals values are not meaningful. Heumann (2022: 6) provides a pertinent example:\n“[T]he satisfaction with a product (unsatisfied–satisfied–very satisfied) is an ordinal variable because the values this variable can take can be ordered but the differences between ‘unsatisfied–satisfied’ and ‘satisfied–very satisfied’ cannot be compared in a numerical way”.\nIn the case of interval-scaled variables, the differences between the values can be interpreted, but their ratios must be treated with caution. A temperature of 4°C is 6 degrees warmer than -2°C; however, this does not imply that 4°C is three times warmer than -2°C. This is because the temperature scale has no true zero point; 0°C simply signifies another point on the scale and not the absence of temperature altogether.\nRatio-scaled variables allow both a meaningful interpretation of the differences between their values and (!) of the ratios between them. Within the context of clause length, LENGTH_DIFF values such as 4 and 8 not only suggest that the latter is four units greater than the former but also that their ratio \\(\\frac{8}{4} = 2\\) is a valid way to describe the relationship between these values. Here a LENGTH_DIFF of 0 can be clearly viewed as the absence of a length difference.\n\n\n\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nPaquot, Magali, and Tove Larsson. 2020. “Descriptive Statistics and Visualization with r.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Thomas Gries, 375–99. Cham: Springer.",
    "crumbs": [
      "Descriptive statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Understanding and visualising variables</span>"
    ]
  },
  {
    "objectID": "Variables.html#introduction-to-ggplot2",
    "href": "Variables.html#introduction-to-ggplot2",
    "title": "7  Understanding and visualising variables",
    "section": "7.3 Introduction to ggplot2",
    "text": "7.3 Introduction to ggplot2\n\n7.3.1 Building a ggplot\n\nA ggplot requires at minimum three elements: (1) a data frame, (2) axis labels, and (3) a plotting option (also known as “geom”). We combine them with the + sign.\n\n\n# Supply data frame\nggplot(data = cl.order,\n      # Supply axis labels\n        mapping = aes(x = LEN_MC, y = LEN_SC)) +\n      # Set plotting option (here: scatterplot)\n        geom_point()\n\n\n\n\n\n\n\n\n\n\n7.3.2 Adding layers\n\nVisualise a third variable using the colors argument as part of the aes() function.\n\n\nggplot(data = cl.order,\n        mapping = aes(x = LEN_MC, \n                      y = LEN_SC,\n                      color = ORDER)) +\n        geom_point()\n\n\n\n\n\n\n\n\n\nAdjust further visual parameters as you see fit:\n\n\nggplot(data = cl.order,\n  mapping = aes(x = LEN_MC, y = LEN_SC)) +\n1  geom_point(aes(color = ORDER, shape = SUBORDTYPE)) +\n  labs(\n2    title = \"Length of main and subordinate clauses\",\n    subtitle = \"Dimensions for different ordering types\",\n    x = \"Length of main clause\",\n    y = \"Length of subordinate clause\",\n    color = \"ORDER\",\n    shape = \"SUBORDTYPE\"\n  ) +\n3  theme_classic()\n\n\n1\n\nMap variables to axes, colours and shapes.\n\n2\n\nAdd a legend with a title, subtitle and axis labels.\n\n3\n\nChange the overall theme of the plot.",
    "crumbs": [
      "Descriptive statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Understanding and visualising variables</span>"
    ]
  },
  {
    "objectID": "Variables.html#visualising-distributions",
    "href": "Variables.html#visualising-distributions",
    "title": "7  Understanding and visualising variables",
    "section": "7.4 Visualising distributions",
    "text": "7.4 Visualising distributions\n\n7.4.1 A categorical variable\n\nBarplot with geom_bar()\n\n\nggplot(cl.order, aes(x = ORDER)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n7.4.2 A numerical variable\n\nHistogram with geom_histogram()\nDensitiy plot with …\n\n\nggplot(cl.order, aes(x = LEN_MC)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\n\nDensity plot with geom_density()\n\n\nggplot(cl.order, aes(x = LEN_MC)) +\n  geom_density(linewidth = 0.5)\n\n\n\n\n\n\n\n\n\n\n7.4.3 A numerical and categorical variable\n\nBoxplot with geom_boxplot()\n\n\nggplot(cl.order, aes(x = ORDER, y = LEN_MC)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nDensitiy plot using the optional arguments color and/or fill\n\n\nggplot(cl.order, aes(x = LEN_MC, fill = ORDER)) +\n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nA barplot with geom_col()\n\n\nggplot(cl.order, aes(x = ORDER, y = LEN_MC)) +\n  geom_col(aes(x = ORDER, y = LEN_MC))\n\n\n\n\n\n\n\n\n\n\n7.4.4 Two categorical variables\n\nBarplots with the fill argument\n\n\nggplot(cl.order, aes(x = ORDER, fill = SUBORDTYPE)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n7.4.5 Two numerical variables\n\nScatterplot with geom_point() (cf. 1.1.1)\n\n\nggplot(cl.order, aes(x = LEN_MC, y = LEN_SC)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nLine plot with geom_line(); the example is based on the flights data set from the previous session)\n\n\nnycflights13::flights %&gt;%  \n  group_by(hour = sched_dep_time %/% 100) %&gt;% \n  summarize(prop_cancelled = mean(is.na(dep_time)), n = n()) %&gt;%  \n  filter(hour &gt; 1) -&gt; flights2\n  \n  ggplot(flights2, aes(x = hour, y = prop_cancelled)) +\n  geom_line(color = \"grey50\") + \n  geom_point()\n\n\n\n\n\n\n\n\n\n\n7.4.6 Multivariate plots\n\nAdvanced scatterplot with four variables: LEN_MC (x), LEN_SC (y), ORDER (colour) and SUBORDTYPE (shape)\n\n\n# 4 variables\nggplot(cl.order, aes(x = LEN_MC, y = LEN_SC)) +\n  geom_point(aes(color = ORDER, shape = SUBORDTYPE))\n\n\n\n\n\n\n\n\n\nFacets\n\n\n# 5 variables\nggplot(cl.order, aes(x = LEN_MC, y = LEN_SC)) +\n  geom_point(aes(color = ORDER, shape = SUBORDTYPE)) +\n  facet_wrap(~MORETHAN2CL)\n\n\n\n\n\n\n\n\n\n\n7.4.7 Saving your plot\n\nSave last plot displayed in the viewer to your working directory:\n\n\nggplot(cl.order, aes(x = LEN_MC, y = LEN_SC)) +\n          geom_point()\n\nggsave(\"figures/clause_length_plot.png\")\n\n\n\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nPaquot, Magali, and Tove Larsson. 2020. “Descriptive Statistics and Visualization with r.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Thomas Gries, 375–99. Cham: Springer.",
    "crumbs": [
      "Descriptive statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Understanding and visualising variables</span>"
    ]
  },
  {
    "objectID": "Summary_statistics.html#measures-of-central-tendency",
    "href": "Summary_statistics.html#measures-of-central-tendency",
    "title": "9  Describing continuous data",
    "section": "9.1 Measures of central tendency",
    "text": "9.1 Measures of central tendency\n\n9.1.1 The mean\n\nA useful summary statistic is the arithmetic mean \\(\\bar{x}\\) (cf. Heumann, Schomaker, and Shalabh 2022: 38). Consider a variable \\(X\\) with observations \\(x_1, x_2, ..., x_n\\) from a sample of size \\(n\\). The sample mean then corresponds to\n\\[\n\\bar{x}= \\frac{x_1 + x_2 + ... + x_n}{n} \\\\ = \\frac{1}{n}\\sum_{i=1}^n{x_i}.\n\\]\n\nIn R, we can obtain the average value of a numeric vector with the mean() function.\n\n# Using mean()\nmean(cl.order$LEN_MC)\n\n[1] 9.265509\n\n# or by hand:\nmean &lt;- 1/length(cl.order$LEN_MC) * sum(cl.order$LEN_MC)\n\nVisualisation:\n\nHistogramDensity plot\n\n\n\n# Plot distribution of LEN_MC\ncl.length.hist &lt;- ggplot(cl.order, aes(x = LEN_MC)) +\n                  geom_histogram(binwidth = 2)\n\ncl.length.hist +\n  # Add mean\n  geom_vline(aes(xintercept = mean(LEN_MC)),\n             color = \"steelblue\",\n             linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n# Plot distribution of LEN_MC\ncl.length.dens &lt;- ggplot(cl.order, aes(x = LEN_MC)) +\n                  geom_density()\n\ncl.length.dens +\n  # Add mean\n  geom_vline(aes(xintercept = mean(LEN_MC)),\n             color = \"steelblue\",\n             linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.2 The median\n\nThe median() function computes the “the halfway point of the data (50% of the data are above the median; 50% of the data are below” (Winter 2020: 58)\n\n\\[\n\\tilde{x}_{0.5} =\n\\begin{cases}\nx_{((n+1)/2)} & \\text{if } n \\text{ is odd.} \\\\\n\\frac{1}{2}(x_{n/2}+x_{(n/2+1)}) & \\text{if } n \\text{ is even.}\n\\end{cases}\n\\]\n\n# Using median()\nmedian(cl.order$LEN_MC)\n\n[1] 8\n\n# or by hand:\nsample_sorted &lt;- sort(cl.order$LEN_MC) # sort values in ascending order\n\nn &lt;- length(cl.order$LEN_MC) # sample size is 403 (odd number!)\n\nmedian &lt;- sample_sorted[(n + 1) %/% 2] # compute median\n\nVisualisation:\n\nHistogramDensity plot\n\n\n\ncl.length.hist +\n  # Add mean\n  geom_vline(aes(xintercept = mean(LEN_MC)), color = \"steelblue\", linewidth = 1) +\n  # Add median\n  geom_vline(aes(xintercept = median(LEN_MC)), color = \"red\", linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\ncl.length.dens +\n  # Add mean\n  geom_vline(aes(xintercept = mean(LEN_MC)), color = \"steelblue\", linewidth = 1) +\n  # Add median\n  geom_vline(aes(xintercept = median(LEN_MC)), color = \"red\", linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.3 Sample variance and standard deviation\n\nIn order to assess how well the mean represents the data, it is instructive to compute the variance var() and the standard deviation sd() for a sample.\nThe sample variance is defined as\n\n\\[s^2 = \\frac{1}{n}\\sum_{i=1}^n{(x_i - \\bar{x})^2}. \\]\n\n# Using var()\nvar(cl.order$LEN_MC)\n\n[1] 25.12585\n\n# or by hand:\n\nsample_data &lt;- cl.order$LEN_MC\n\n# Calculate the sample standard deviation\n\nvar &lt;- 1 / length(sample_data) * sum((sample_data - mean(sample_data))^2) # formula above\n\n# Note that R's var() function applies an additional bias correction measure:\n\nvar_corrected &lt;- 1 / (length(sample_data) - 1) * sum((sample_data - mean(sample_data))^2) # equivalent to var()\n\n\nCorrespondingly, the standard deviation of the mean is the square root of the variance (cf. Heumann, Schomaker, and Shalabh 2022: 51-2):\n\n\\[ s = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n{(x_i - \\bar{x})^2}} \\]\n\n# Using sd()\nsd(cl.order$LEN_MC)\n\n[1] 5.012569\n\n# or by hand:\n\nsample_data &lt;- cl.order$LEN_MC\n\n# Calculate the sample standard deviation\n\nsd &lt;- sqrt(1 / (length(sample_data) - 1)* sum((sample_data - mean(sample_data))^2))\n\nApplication and visualisation:\n\nExample 1Example 2\n\n\n\ncl.length.hist +\n  # Add verticle line for the mean\n  geom_vline(aes(xintercept = mean(LEN_MC)), color = \"steelblue\", linewidth = 1) +\n  # Add -1sd\n  geom_vline(aes(xintercept = mean(LEN_MC) - sd(LEN_MC)), color = \"orange\", linewidth = 1) +\n  # Add +1sd\n  geom_vline(aes(xintercept = mean(LEN_MC) + sd(LEN_MC)), color = \"orange\", linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n# Create data frame with mean and sd for each clause ORDER\n\ncl.order %&gt;% \n  # Select variables of interest\n  select(ORDER, LEN_MC) %&gt;% \n  # Group results of following operations by ORDER\n  group_by(ORDER) %&gt;% \n    # Create grouped summary of mean and sd for each ORDER\n    summarise(mean = mean(LEN_MC),\n                sd = sd(LEN_MC)) -&gt; cl_mean_sd; cl_mean_sd\n\n# A tibble: 2 × 3\n  ORDER  mean    sd\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 mc-sc  9.04  4.91\n2 sc-mc  9.75  5.22\n\n# Plot results \n\nggplot(cl_mean_sd, aes(x = ORDER, y = mean)) +\n  # Barplot with a specific variable mapped onto y-axis\n  geom_col() +\n  # Add mean and standard deviation to the plot\n  geom_errorbar(aes(x = ORDER,\n                    ymin = mean-sd,\n                    ymax = mean+sd), width = .2) +\n  theme_classic() +\n  labs(y = \"Mean length of main clauses\", x = \"Clause order\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.4 Quantiles\n\nWhile median() divides the data into two equal sets (i.e., two 50% quantiles), the quantile() function makes it possible to partition the data further.\n\nquantile(cl.order$LEN_MC)\n\n  0%  25%  50%  75% 100% \n   2    6    8   11   31 \n\n\nquantile(x, 0) and quantile(x, 1) thus show the minimum and maximum values, respectively.\n\nquantile(cl.order$LEN_MC, 0)\n\n0% \n 2 \n\nquantile(cl.order$LEN_MC, 1)\n\n100% \n  31 \n\n\n\n\n\n9.1.5 Quartiles and boxplots\n\nConsider the distribution of clause length by clause order:\n\n\nVersion 1Version 2\n\n\n\nggplot(cl.order, aes(x = ORDER, y = LEN_MC)) +\n  geom_boxplot() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nggplot(cl.order, aes(x = ORDER, y = LEN_MC)) +\n  geom_boxplot() +\n  geom_jitter() + # add data points \n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\nCompare it to the corresponding rotated density plot:\n\n\nggplot(cl.order, aes(x = LEN_MC, fill = ORDER)) +\n  geom_density(alpha = 0.5) +\n  coord_flip() +\n  theme_classic()",
    "crumbs": [
      "Descriptive statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Describing continuous data</span>"
    ]
  },
  {
    "objectID": "Summary_statistics.html#theoretical-distributions",
    "href": "Summary_statistics.html#theoretical-distributions",
    "title": "9  Describing continuous data",
    "section": "9.3 Theoretical distributions",
    "text": "9.3 Theoretical distributions\n\n9.3.1 The normal distribution\nA great number of numerical variables in the world follow the well-known normal (or Gaussian) distribution, which includes test scores, weight and height, among many others.\nIf a random variable \\(X\\) is normally distributed, it is determined by the parameters \\(\\mu\\) (the mean) and \\(\\sigma\\) (the standard deviation). Formally, we can summarise this using the notation\n\\[ X \\sim N(\\mu, \\sigma^2).\\] The probability density function (PDF) of the normal distribution has a characteristic bell-shape. The density values on the \\(y\\)-axis indicate the likelihood of encountering a specific value of \\(X\\) (cf. Winter 2020: 56; Heumann, Schomaker, and Shalabh 2022: 173-177).\n\n\n\n\n\n\n\n\n\n\n\n9.3.2 Bernoulli distribution\nThe Bernoulli distribution is a discrete probability distribution for random variables which have only two possible outcomes: “positive” (often coded as 1) and “negative” (often coded as 0). Examples of such variables include coin tosses (heads/tails), binary response questions (yes/no), and defect status (defective/non-defective).\nIf a random variable \\(X\\) follows a Bernoulli distribution, it is determined by the parameter \\(p\\), which is the probability of the positive case:\n\\[ X \\sim Bernoulli(p).\\] The probability mass function (PMF) of the Bernoulli distribution is given by: \\[\nP(X = x) =\n\\begin{cases}\np & \\text{if } x = 1 \\\\\n1 - p & \\text{if } x = 0\n\\end{cases}\n\\]\nwhere \\(0 \\leq p \\leq 1\\). This function shows the probability of \\(X\\) taking on the value of 1 or 0 (cf. Heumann, Schomaker, and Shalabh 2022: 162-163).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtensions\n\n\n\nA Bernoulli experiment presupposes a single trial (e.g., tossing a coin once). If we are interested in the distribution of a binary discrete variable over \\(n\\) Bernoulli trials, we can describe it in terms of the binomial distribution (Heumann, Schomaker, and Shalabh 2022: 163-166).\nCategorical variables with more than 2 outcomes and \\(n\\) Bernoulli trials can be modelled using the multinomial distribution (Heumann, Schomaker, and Shalabh 2022: 167-169).\n\n\n\n\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using r. New York; London: Routledge.",
    "crumbs": [
      "Descriptive statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Summary statistics: Theory and practice</span>"
    ]
  },
  {
    "objectID": "Hypothesis_testing.html#preparation",
    "href": "Hypothesis_testing.html#preparation",
    "title": "11  Hypothesis testing",
    "section": "11.1 Preparation",
    "text": "11.1 Preparation\n\nLoad packages:\n\n\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\n\n\nLoad the data sets:\n\n\ndata &lt;- read_xlsx(\"Paquot_Larsson_2020_data.xlsx\")\n\ndata_vowels &lt;- read.csv(\"Vowels_Apache.csv\", sep = \"\\t\")",
    "crumbs": [
      "Inferential statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "Hypothesis_testing.html#hypothesis-testing",
    "href": "Hypothesis_testing.html#hypothesis-testing",
    "title": "11  Hypothesis testing",
    "section": "11.2 Hypothesis testing",
    "text": "11.2 Hypothesis testing\nThe first step is to define the null hypothesis \\(H_0\\) and the alternative hypothesis \\(H_1\\) (or \\(H_a\\)).\nGiven two categorical variables \\(X\\) and \\(Y\\), we assume under \\(H_0\\) that both variables are independent from each other. This hypothesis describes the “default state of the world” (James et al. 2021: 555), i.e., what we would usually expect to see. By contrast, the alternative hypothesis \\(H_1\\) states that \\(X\\) and \\(Y\\) are not independent, i.e., that \\(H_0\\) does not hold.\nIn this unit, we will consider two scenarios:\n\nWe are interested in finding out whether English clause ORDER (‘sc-mc’ or ‘mc-sc’) depends on the type of the subordinate clause (SUBORDTYPE), which can be either temporal (‘temp’) or causal (‘caus’).\n\nOur hypotheses are:\n\n\\(H_0:\\) The variables ORDER and SUBORDTYPE are independent.\n\\(H_1:\\) The variables ORDER and SUBORDTYPE are not independent.\n\n\nAs part of a phonetic study, we compare the base frequencies of the F1 formants for male and female speakers of Apache. We forward the following hypotheses:\n\n\n\\(H_0:\\) mean F1 frequency of men \\(=\\) mean F1 frequency of women.\n\\(H_1:\\) mean F1 frequency of men \\(\\ne\\) mean F1 frequency of women.\n\nBased on our data, we can decide to either accept or reject \\(H_0\\). Rejecting \\(H_0\\) can be viewed as evidence in favour of \\(H_1\\) and thus marks a potential ‘discovery’ in the data. However, there is always a chance that we accept or reject the wrong hypothesis; the four possible constellations are summarised in the table below (cf. Heumann, Schomaker, and Shalabh 2022: 223):\n\n\n\n\n\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is not true\n\n\n\n\n\\(H_0\\) is not rejected\n\\(\\color{green}{\\text{Correct decision}}\\)\n\\(\\color{red}{\\text{Type II } (\\beta)\\text{-error}}\\)\n\n\n\\(H_0\\) is rejected\n\\(\\color{red}{\\text{Type I } (\\alpha)\\text{-error}}\\)\n\\(\\color{green}{\\text{Correct decision}}\\)\n\n\n\nThe probability of a Type I error, which refers to the rejection of \\(H_0\\) although it is true, is called the significance level \\(\\alpha\\), which has a conventional value of \\(0.05\\) (i.e., a 5% chance of committing a Type I error).",
    "crumbs": [
      "Inferential statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "Hypothesis_testing.html#constructing-the-critical-region",
    "href": "Hypothesis_testing.html#constructing-the-critical-region",
    "title": "11  Hypothesis testing",
    "section": "11.3 Constructing the critical region",
    "text": "11.3 Constructing the critical region\nAn important question remains: How great should the difference be for us to reject \\(H_0\\)? The \\(p\\)-value measures the probability of encountering a specific value of a test statistic under the assumption that \\(H_0\\) holds. For example, a \\(p\\)-value of \\(0.02\\) means that we would see a particular \\(\\chi^2\\)-score (or \\(T\\), \\(F\\) etc.) only 2% of the time if \\(X\\) and \\(Y\\) were unrelated (or if there was no difference between \\(\\bar{x}\\) and \\(\\bar{y}\\), respectively). Since our significance level \\(\\alpha\\) is set to \\(0.05\\), we only reject the null hypothesis if this probability is lower than 5%.\nWe obtain \\(p\\)-values by consulting the probability density functions of the underlying distributions:\n\nProbability density function for the \\(\\chi^2\\)-distribution with \\(df = 1\\)\n\n\n\nCode\n# Generate random samples from a chi-squared distribution with 1 degree of freedom\nx &lt;- rchisq(100000, df = 1)\n\n# Create histogram\nhist(x,\n     breaks = \"Scott\",\n     freq = FALSE,\n     xlim = c(0, 20),\n     ylim = c(0, 0.2),\n     ylab = \"Probability density of observing a specific score\",\n     xlab = \"Chi-squared score\",\n     main = \"Histogram for a chi-squared distribution with 1 degree of freedom (df)\",\n     cex.main = 0.9)\n\n# Overlay PDF\ncurve(dchisq(x, df = 1), from = 0, to = 150, n = 5000, col = \"orange\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\n\n\nProbability density function for the \\(t\\)-distribution with \\(df = 112.19\\)\n\n\n\nCode\n# Given t-statistic and degrees of freedom\nt_statistic &lt;- 2.4416\ndf &lt;- 112.19\n\n# Generate random samples from a t-distribution with the given degrees of freedom\nx &lt;- rt(100000, df = df)\n\n# Create histogram\nhist(x,\n     breaks = \"Scott\",\n     freq = FALSE,\n     xlim = c(-5, 5),\n     ylim = c(0, 0.4),\n     ylab = \"Probability density of observing a specific score\",\n     xlab = \"t-score\",\n     main = \"Histogram for a t-distribution with 112.19 degrees of freedom\",\n     cex.main = 0.9)\n\n# Overlay PDF\ncurve(dt(x, df = df), from = -5, to = 5, n = 5000, col = \"orange\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. New York: Springer. https://doi.org/10.1007/978-1-0716-1418-1.",
    "crumbs": [
      "Inferential statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "Chi_square_test.html#preparation",
    "href": "Chi_square_test.html#preparation",
    "title": "9  Chi square test",
    "section": "9.1 Preparation",
    "text": "9.1 Preparation\n\nLoad packages:\n\n\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\n\n\nLoad the data sets:\n\n\ndata &lt;- read_xlsx(\"Paquot_Larsson_2020_data.xlsx\")\n\n\n9.1.1 The \\(\\chi^2\\)-test\nThe next step is to compute a test statistic that indicates how strongly our data conforms to \\(H_0\\). For instance, the Pearson \\(\\chi^2\\) statistic is commonly used for categorical variables. It requires two types of values: the observed frequencies \\(n_{ij}\\) in our data set and the expected frequencies \\(m_{ij}\\), which we would expect to see if \\(H_0\\) was true.\nThis table represents a generic contingency table where \\(X\\) and \\(Y\\) are categorical variables. Each \\(x_i\\) represents a category of \\(X\\) and each \\(y_j\\) represents a category of \\(Y\\). In the table, each cell indicates the count of observation \\(n_{ij}\\) corresponding to the \\(i\\)-th row and \\(j\\)-th column.\n\n\n\n\n\n\n\\(Y\\)\n\n\n\n\n\n\n\n\n\n\\(y_1\\)\n\\(y_2\\)\n…\n\\(y_J\\)\n\n\n\n\n\\(x_1\\)\n\\(n_{11}\\)\n\\(n_{12}\\)\n…\n\\(n_{1J}\\)\n\n\n\n\n\\(x_2\\)\n\\(n_{21}\\)\n\\(n_{22}\\)\n…\n\\(n_{2J}\\)\n\n\n\n\\(X\\)\n…\n…\n…\n…\n…\n\n\n\n\n\\(x_I\\)\n\\(n_{I1}\\)\n\\(n_{I2}\\)\n…\n\\(n_{3J}\\)\n\n\n\n\n\n# Cross-tabulate the frequencies for the variables of interest\n\nfreqs &lt;- table(data$ORDER, data$SUBORDTYPE); freqs\n\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n\n\nWe calculate the expected frequencies by using the formula\n\\[\nm_{ij} = \\frac{i\\textrm{th row sum} \\times j \\textrm{th column sum}}{\\textrm{number of observations}}.\n\\]\n\n# Compute expected frequencies\n\n## Calculate row totals\nrow_totals &lt;- rowSums(freqs)\n\n## Calculate column totals\ncol_totals &lt;- colSums(freqs)\n\n## Total number of observations\ntotal_obs &lt;- sum(freqs)\n\n## Calculate expected frequencies\nexpected_table &lt;- outer(row_totals, col_totals) / total_obs\n\nexpected_table\n\n           caus      temp\nmc-sc 135.79404 139.20596\nsc-mc  63.20596  64.79404\n\n\n\n\n\n\n\n\nDefinition of the \\(\\chi^2\\)-test\n\n\n\n\n\nGiven a sample with \\(n\\) observations and \\(k\\) degrees of freedom (\\(df\\))1, the \\(\\chi^2\\)-statistic measures how much the observed frequencies deviate from the expected frequencies for each cell in a contingency table (cf. Heumann, Schomaker, and Shalabh 2022: 249-251):\n\\[\n\\chi^2 = \\sum_{i=1}^{I}\\sum_{i=j}^{J}{\\frac{(n_{ij} - m_{ij})^2}{m_{ij}}}.\n\\] The test stipulates that …\n\nall observations are independent of each other,\n80% of the expected frequencies are \\(\\geq\\) 5, and\nall observed frequencies are \\(\\geq\\) 1.\n\n\n\n\n\n\n\n\n\n\nImplementation in R: Manual vs. automatic\n\n\n\n\n\n\n# Compute chi-squared scores for all cells\n## Create empty chi_squared_table for later storage\nchi_squared_table &lt;- matrix(NA, nrow = 2, ncol = 2,\n                            dimnames = list(c(\"mc-sc\", \"sc-mc\"), c(\"caus\", \"temp\")))\n\n# Loop: Repeat the following commands ...\nfor (i in 1:2) { # for each of the 2 rows and\n  for (j in 1:2) { # for each of the 2 columns:\n    observed_freq &lt;- freqs[i, j] # 1. Get the observed frequencies\n    expected_freq &lt;- expected_table[i, j] # 2. Get the expected frequencies\n    chi_squared_score &lt;- ((observed_freq - expected_freq)^2) / expected_freq # 3. Compute chi-squared scores\n    chi_squared_table[i, j] &lt;- chi_squared_score # 4. Store output in the chi_squared_table\n  }\n}\n\nchi_squared_table\n\n          caus     temp\nmc-sc 17.11278 16.69335\nsc-mc 36.76575 35.86463\n\n\nOr, more elegantly:\n\nfreqs_test &lt;- chisq.test(freqs)\n\n# Expected frequencies\nfreqs_test$expected\n\n       \n             caus      temp\n  mc-sc 135.79404 139.20596\n  sc-mc  63.20596  64.79404\n\n# Chi-squared scores\n(freqs_test$residuals)^2\n\n       \n            caus     temp\n  mc-sc 17.11278 16.69335\n  sc-mc 36.76575 35.86463\n\n# Test statistics\nfreqs_test\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  freqs\nX-squared = 104.24, df = 1, p-value &lt; 2.2e-16\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf the data does not meet the (expected) frequency requirements for the \\(\\chi^2\\)-test, the Fisher’s Exact Test is a viable alternative (see ?fisher.test() for details).",
    "crumbs": [
      "Inferential statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chi square test</span>"
    ]
  },
  {
    "objectID": "Chi_square_test.html#workflow-in-r",
    "href": "Chi_square_test.html#workflow-in-r",
    "title": "9  Chi square test",
    "section": "9.2 Workflow in R",
    "text": "9.2 Workflow in R\n\n9.2.1 \\(\\chi^2\\)-test\n\n9.2.1.1 Define hypotheses\n\n\\(H_0:\\) The variables ORDER and SUBORDTYPE are independent.\n\\(H_1:\\) The variables ORDER and SUBORDTYPE are not independent.\n\n\n\n9.2.1.2 Descriptive overview\nWe plot the distribution of clause ORDER depending on SUBORDTYPE. This requires (a) selecting the desired variables, (b) computing the token frequencies and (c) computing the percentages.\n\n\nCode\n# Filter data so as to show only those observations that are relevant\n\ndata %&gt;% \n  # Filter columns\n  select(ORDER, SUBORDTYPE) %&gt;%\n    # Count observations \n    count(ORDER, SUBORDTYPE) %&gt;%  \n    # Compute percentages\n    mutate(pct = n/sum(n) * 100) -&gt; data_order_subord\n\nknitr::kable(data_order_subord)\n\n\n\n\n\nORDER\nSUBORDTYPE\nn\npct\n\n\n\n\nmc-sc\ncaus\n184\n45.657568\n\n\nmc-sc\ntemp\n91\n22.580645\n\n\nsc-mc\ncaus\n15\n3.722084\n\n\nsc-mc\ntemp\n113\n28.039702\n\n\n\n\n\nNext, we visualise the ORDER distribution using a barplot with a custom y-axis, requiring geom_col().\n\n\nCode\n# Plot distribution\n\ndata_order_subord %&gt;%\n  # Map variables onto axes\n  ggplot(aes(x = ORDER, y = pct, fill = SUBORDTYPE)) +\n    # Define plot type\n    geom_col(pos = \"dodge\") +\n    # Define theme\n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n9.2.1.3 Running the test\n\n# Cross-tabulate the frequencies for the variables of interest\n\nfreqs &lt;- table(data$ORDER, data$SUBORDTYPE)\n\nfreqs ## Assumption met: all observed freqs =&gt; 1\n\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n\n# Run a chis-quared test on the absolute frequencies and print the results\n\ntest &lt;- chisq.test(freqs, correct = FALSE)\n\n# Inspect expected frequencies\n\ntest$expected # Assumption met: all expected frequences =&gt; 5\n\n       \n             caus      temp\n  mc-sc 135.79404 139.20596\n  sc-mc  63.20596  64.79404\n\n\n\n\n9.2.1.4 Optional: Effect size\nThe sample-size independent effect size measure Cramer’s V (\\(\\phi\\)) is defined as\n\\[V = \\sqrt{\\frac{\\chi^2}{N \\times df}}.\\] The outcome varies between \\(0\\) (= no correlation) and \\(1\\) (= perfect correlation); cf. also Gries (2013: 186).\n\n\nCode\n# Compute Cramer's V\n\n## By hand:\n\n# Given chi-squared statistic\nchi_squared &lt;- unname(test$statistic)\n\n# Total number of observations\ntotal_obs &lt;- sum(freqs)\n\nsqrt(chi_squared / total_obs * (min(dim(freqs)) - 1))\n\n\n[1] 0.5139168\n\n\nCode\n## Automatically:\nlibrary(\"confintr\") # Load library\n\ncramersv(test)\n\n\n[1] 0.5139168\n\n\n\n\n9.2.1.5 Reporting the results\nAccording to a \\(\\chi^2\\)-test, there is a significant association between clause ORDERand SUBORDTYPE at \\(p &lt; 0.001\\) (\\(\\chi^2 = 106.44, df = 1\\)), thus justifying the rejection of \\(H_0\\).\n\n\n\n\nGries, Stefan Thomas. 2013. Statistics for Linguistics with r: A Practical Introduction. 2nd rev. ed. Berlin; Boston: De Gruyter Mouton.\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.",
    "crumbs": [
      "Inferential statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chi square test</span>"
    ]
  },
  {
    "objectID": "t_test.html#the-t-test",
    "href": "t_test.html#the-t-test",
    "title": "10  t-test",
    "section": "10.1 The \\(t\\)-test",
    "text": "10.1 The \\(t\\)-test\nSince the \\(\\chi^2\\) measure exclusively works with categorical variables, a separate test statistic is required if one of them is a continuous variable. The \\(t\\) statistic is often used for research questions involving differences between sample means. The way \\(t\\) is calculated depends on the sources of \\(X\\) and \\(Y\\): Are they from the same sample or from two (in-)dependent ones?\nFirst, we consider two independent samples from a population:\n\nSample \\(X\\) with the observations \\(x_1, x_2, ..., {x_n}_1\\), sample size \\(n_1\\), sample mean \\(\\bar{x}\\) and sample variance \\(s^2_x\\).\nSample \\(Y\\) with the observations \\(y_1, y_2, ..., {y_n}_2\\), sample size \\(n_2\\), sample mean \\(\\bar{y}\\) and sample variance \\(s^2_y\\).\n\n\n\n\n\n\n\nDefinition of the \\(t\\)-test\n\n\n\n\n\nThe \\(t\\)-statistic after Welch is given by:\n\\[\nt(x, y) = \\frac{|\\bar{x} - \\bar{y}|}{\\sqrt{\\frac{s^2_x}{n_1} + \\frac{s^2_y}{n_2}}}\n\\]\n\nIf there is more than one observation for a given subject (e.g, before and after an experiment), the samples are called dependent or paired. The paired \\(t\\)-test assumes two continuous variables \\(X\\) and \\(Y\\).\nIn the paired test, the variable \\(d\\) denotes the difference between them, i.e., \\(x - y\\). The corresponding test statistic is obtained via\n\n\\[\nt(x, y) = t(d) = \\frac{\\bar{d}}{s_d} \\sqrt{n}.\n\\]\nNote the difference \\(\\bar{d} = \\frac{1}{n}\\sum_{i=1}^n{d_i}\\) and the variance\n\\[\ns^2_d = \\frac{\\sum_{i=1}^n({d_i} - \\bar{d})^2}{n-1}.\n\\]\nTraditionally, the \\(t\\)-test is based on the assumptions of …\n\nNormality and\nVariance homogeneity (i.e., equal sample variances). Note that this does not apply to the \\(t\\)-test after Welch, which can handle unequal variances.\n\n\n\n\n\n\n\n\n\n\nImplementation in R: Manual vs. automatic\n\n\n\n\n\nBy hand:\n\n# Subset the data by sex\ndata_m &lt;- data_vowels[data_vowels$SEX == \"M\", ]\ndata_f &lt;- data_vowels[data_vowels$SEX == \"F\", ]\n\n# Compute sample means\nmean_m &lt;- mean(data_m$HZ_F1)\nmean_f &lt;- mean(data_f$HZ_F1)\n\n# Compute sample variances\nvar_m &lt;- var(data_m$HZ_F1)\nvar_f &lt;- var(data_f$HZ_F1)\n\n# Determine sample sizes\nn_m &lt;- length(data_m$HZ_F1)\nn_f &lt;- length(data_f$HZ_F1)\n\n# Compute t-statistic\nt_statistic &lt;- abs(mean_m - mean_f) / sqrt((var_m / n_m) + (var_f / n_f))\n\n# Compute degrees of freedom (simple version)\ndf &lt;- n_m + n_f - 2\n\n# Find the p-value using the cumulative distribution function (CDF) of the t-distribution\np_value &lt;- 2 * pt(-t_statistic, df)\n\nOr, more concisely:\n\nt.test(data_vowels$HZ_F1 ~ data_vowels$SEX, paired = FALSE) # there is a significant difference!\n\n\n    Welch Two Sample t-test\n\ndata:  data_vowels$HZ_F1 by data_vowels$SEX\nt = 2.4416, df = 112.19, p-value = 0.01619\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n  8.403651 80.758016\nsample estimates:\nmean in group F mean in group M \n       528.8548        484.2740 \n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf at least one assumption of the \\(t\\)-test has been violated, it is important to use a non-parametric test such as the Wilcoxon-Mann-Whitney (WMW) U-Test instead. In essence, this test compares the probabilities of encountering a value \\(x\\) from sample \\(X\\) that is greater than a value \\(y\\) from sample \\(Y\\). For details, see ?wilcox.test().",
    "crumbs": [
      "Inferential statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "t_test.html#workflow-in-r",
    "href": "t_test.html#workflow-in-r",
    "title": "10  t-test",
    "section": "10.2 Workflow in R",
    "text": "10.2 Workflow in R\n\n10.2.1 Define hypotheses\n\n\\(H_0:\\) mean F1 frequency of men \\(=\\) mean F1 frequency of women.\n\\(H_1:\\) mean F1 frequency of men \\(\\ne\\) mean F1 frequency of women.\n\n\n\n10.2.2 Descriptive overview\nWe select the variables of interest and proceed calculate the mean F1 frequencies for each level of SEX, requiring a grouped data frame.\n\n\nCode\n# Filter data so as to show only those observations that are relevant\n\ndata_vowels %&gt;% \n  # Filter columns\n  select(HZ_F1, SEX) %&gt;%\n    # Define grouping variable\n    group_by(SEX) %&gt;% \n      # Compute mean and standard deviation for each sex\n      summarise(mean = mean(HZ_F1),\n                sd = sd(HZ_F1)) -&gt; data_vowels_stats\n\nknitr::kable(data_vowels_stats)\n\n\n\n\n\nSEX\nmean\nsd\n\n\n\n\nF\n528.8548\n110.80099\n\n\nM\n484.2740\n87.90112\n\n\n\n\n\n\n\nCode\n# Plot distribution\n\n## Plot means\n\ndata_vowels_stats %&gt;% \n  ggplot(aes(x = SEX, y = mean)) +\n    geom_col() +\n    geom_errorbar(aes(x = SEX,\n                    ymin = mean-sd,\n                    ymax = mean+sd), width = .2) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\nCode\n## Plot quartiles\ndata_vowels %&gt;% \n  ggplot(aes(x = SEX, y = HZ_F1)) +\n    geom_boxplot() +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n10.2.3 Check \\(t\\)-test assumptions\n\n# Normality\n\nshapiro.test(data_vowels$HZ_F1) # H0: data points follow the normal distribution\n\n\n    Shapiro-Wilk normality test\n\ndata:  data_vowels$HZ_F1\nW = 0.98996, p-value = 0.5311\n\n# Check histogram\n\nggplot(data_vowels, aes(x = HZ_F1)) +\n  geom_histogram(bins = 30) +\n  theme_classic()\n\n\n\n\n\n\n\n# Variance homogeneity\n\nvar.test(data_vowels$HZ_F1 ~ data_vowels$SEX) # H0: variances are not too different from each other\n\n\n    F test to compare two variances\n\ndata:  data_vowels$HZ_F1 by data_vowels$SEX\nF = 1.5889, num df = 59, denom df = 59, p-value = 0.07789\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.949093 2.660040\nsample estimates:\nratio of variances \n          1.588907 \n\n\n\n\n10.2.4 Running the test\n\n# t-test for two independent samples \n\nt.test(data_vowels$HZ_F1 ~ data_vowels$SEX, paired = FALSE) # there is a significant difference!\n\n\n    Welch Two Sample t-test\n\ndata:  data_vowels$HZ_F1 by data_vowels$SEX\nt = 2.4416, df = 112.19, p-value = 0.01619\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n  8.403651 80.758016\nsample estimates:\nmean in group F mean in group M \n       528.8548        484.2740 \n\n\n\n\n10.2.5 Optional: Effect size\nCohen’s d is a possible effect size measure for continuous data and is obtained by dividing the difference of both sample means by the pooled standard deviation:\n\\[\\frac{\\bar{x} - \\bar{y}}{\\sqrt{\\frac{{(n_1 - 1)s_x^2 + (n_2 - 1)s_y^2}}{{n_1 + n_2 - 2}}}}.\\]\n\n\nCode\nlibrary(\"effsize\")\n\n# By hand:\n## Compute pooled standard deviation sp\nsp &lt;- sqrt(((n_m - 1) * var_m + (n_f - 1) * var_f) / (n_m + n_f - 2))\n\n## Compute Cohen's d\nd &lt;- abs(mean_m - mean_f) / sp\n\n# Automatically:\ncohen.d(data_vowels$HZ_F1, data_vowels$SEX) # see also ?cohen.d for more details\n\n\n\nCohen's d\n\nd estimate: 0.4457697 (small)\n95 percent confidence interval:\n     lower      upper \n0.07976048 0.81177897 \n\n\n\n\n10.2.6 Reporting the results\nAccording to a two-sample \\(t\\)-test, there is a significant difference between the mean F1 frequencies of male and female speakers of Apache (\\(t = 2.44\\), \\(df = 112.19\\), \\(p &lt; 0.05\\)). Therefore, \\(H_0\\) will be rejected.",
    "crumbs": [
      "Inferential statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "Linear_regression.html#preparation",
    "href": "Linear_regression.html#preparation",
    "title": "11  Linear and multiple regression",
    "section": "11.1 Preparation",
    "text": "11.1 Preparation\n\n# Load libraries\n\nlibrary(\"readxl\")\nlibrary(\"writexl\")\nlibrary(\"tidyverse\")\n\n# Load file\nELP &lt;- read_xlsx(\"ELP.xlsx\")\n\n# Inspect file structure\nstr(ELP)",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear and multiple regression</span>"
    ]
  },
  {
    "objectID": "Linear_regression.html#introduction",
    "href": "Linear_regression.html#introduction",
    "title": "11  Linear and multiple regression",
    "section": "11.2 Introduction",
    "text": "11.2 Introduction\nConsider the distribution of the continuous variable RT (reaction times) from the ELP (English Lexicon Project) dataset. We will \\(log\\)-transform the reaction times to even out the differences between extremely high and extremely low frequency counts (cf. Winter 2020: 90-94).\n\nLog-transformedDefault\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe are investigating the relationship between reaction times RT and the frequency Freq of a lexical stimulus.\n\n\n\n\n\n\n\n\n\nSome open questions:\n\nCan word frequency help us explain variation in reaction times?\nIf it can, then how could we characterise the effect of word frequency? In other words, does it increase or decrease reaction times?\nWhat reaction times should we expect for new observations?\n\n\n11.2.1 Building a statistical model\n\nHere RT is a response or target that we wish to explain. We generically refer to the response as \\(Y\\).\nFreq is a feature, input, or predictor, which we name \\(X\\).\n\nWe can thus summarise our preliminary and fairly general statistical model as\n\\[ Y = f(X) + \\epsilon. \\] While the term \\(f(X)\\) denotes the contribution of \\(X\\) to the explanation of \\(Y\\), \\(\\epsilon\\) describes the errors of the model.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear and multiple regression</span>"
    ]
  },
  {
    "objectID": "Linear_regression.html#linear-regression",
    "href": "Linear_regression.html#linear-regression",
    "title": "11  Linear and multiple regression",
    "section": "11.3 Linear Regression",
    "text": "11.3 Linear Regression\n\nLinear regression is a simple approach to supervised machine learning where the response variable is known. It assumes that the dependence of \\(Y\\) on \\(X\\) is linear.\nThis approach is suited for numerical response variables. The predictors can be either numerical or discrete/categorical.\nAlthough it may seem overly simplistic, linear regression is extremely useful both conceptually and practically.\n\n\n11.3.1 Model with a single predictor \\(X\\)\n\nA linear model of our data would have the form\n\n\\[ Y = \\beta_0 + \\beta_1X + \\epsilon \\]\nor, in more concrete terms,\n\\[ \\text{Reaction time} = \\beta_0 + \\beta_1\\text{Frequency} + \\text{Model Error,} \\]\nwhere \\(\\beta_0\\) and \\(\\beta_1\\) are two unknown constants that represent the intercept and slope, respectively. Together they are referred to as the model coefficients (or parameters), and \\(\\epsilon\\) is the error term. The fact that assumptions are made about the form of the model renders it a parametric model.\n\nGiven the existing data on \\(X\\) and \\(Y\\), which is also known as the training data, we estimate the model coefficients \\(\\beta_0\\) and \\(\\beta_1\\). While we use the training data to estimate these coefficients, we cannot know the true values of the coefficients, i.e., the exact true relationship between the variables.\nThe most common way of estimating parameters for linear models is the least squares approach. In essence, the parameters are chosen such that the residual sum of squares1, i.e., the sum of the differences between observed and predicted values, is as low as possible.\nWe can then predict future sales using the formula\n\n\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x,\n\\]\nwhere \\(\\hat{y}\\) indicates a prediction of \\(Y\\) on the basis of the predictor values \\(X = x\\). The hat symbol ^ marks estimated values.\n\nIn R, we can fit a linear model with the lm() function.\n\n\n# Fit linear model\n\nrt.lm1 = lm(log(RT) ~ log(Freq), data = ELP)\n\n# View model data\n\nsummary(rt.lm1)\n\n\nCall:\nlm(formula = log(RT) ~ log(Freq), data = ELP)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.29765 -0.08203 -0.01205  0.07298  0.43407 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.633361   0.004286 1547.82   &lt;2e-16 ***\nlog(Freq)   -0.048602   0.002201  -22.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1235 on 878 degrees of freedom\nMultiple R-squared:  0.357, Adjusted R-squared:  0.3563 \nF-statistic: 487.5 on 1 and 878 DF,  p-value: &lt; 2.2e-16\n\n\nThe model statistics comprise the following elements:\n\nCall, i.e., the model formula.\nResiduals: These indicate the difference between the observed values in the data set and the values predicted by the model (= the fitted values). These correspond to the error term \\(\\epsilon\\). The lower the residuals, the better the model describes the data.\n\n\n# Show fitted values (= predictions) for the first six observations\n\nhead(rt.lm1$fitted.values)\n\n       1        2        3        4        5        6 \n6.635345 6.563152 6.789806 6.613980 6.630529 6.574894 \n\n# Show deviation of the fitted values from the observed values\n\nhead(rt.lm1$residuals)\n\n          1           2           3           4           5           6 \n 0.03778825 -0.02277165  0.07759556  0.03387713  0.15222949 -0.10432670 \n\n\n\nCoefficients: The regression coefficients correspond to \\(\\hat{\\beta}_0\\) (“Intercept”) and \\(\\hat{\\beta}_1\\) (“log(Freq)”), respectively. The model shows that for a one-unit increase in log-frequency the log-reaction time decreases by approx. -0.05.\n\n\n# Convert coefficients to a tibble \n\nlibrary(\"broom\")\n\ntidy_model &lt;- tidy(rt.lm1)\n\ntidy_model\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   6.63     0.00429    1548.  0       \n2 log(Freq)    -0.0486   0.00220     -22.1 2.88e-86\n\n\n\n\\(p\\)-values and \\(t\\)-statistic: Given the null hypothesis \\(H_0\\) that there is no correlation between log(RT) and log(Freq) (i.e., \\(H_0: \\beta_1 = 0\\)), a \\(p\\)-value lower than 0.05 indicates that \\(\\beta_1\\) considerably deviates from 0, thus providing evidence for the alternative hypothesis \\(H_1: \\beta_1 \\ne 0\\). Since \\(p &lt; 0.001\\), we can reject \\(H_0\\).\nThe \\(p\\)-value itself crucially depends on the \\(t\\)-statistic2, which measures “the number of standard deviations that \\(\\hat{\\beta_1}\\) is away from 0” (James et al. 2021: 67) . The standard error (SE) reflects how much an estimated coefficient differs on average from the true values of \\(\\beta_0\\) and \\(\\beta_1\\). They can be used to compute the 95% confidence interval \\([\\hat{\\beta}_1 - 2 \\cdot SE(\\hat{β}_1), \\hat{\\beta}_1 + 2 \\cdot SE(\\hat{β}_1)]\\); the true estimate of the parameter \\(\\beta_1\\) lies within the specified range 95% of the time.\n\n\n# Compute confidence intervals for intercept and log(Freq)\n\ntidy_model_ci &lt;- tidy(rt.lm1, conf.int = TRUE)\n\ntidy_model_ci\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   6.63     0.00429    1548.  0          6.62      6.64  \n2 log(Freq)    -0.0486   0.00220     -22.1 2.88e-86  -0.0529   -0.0443\n\n\nThe estimated parameter for log(Freq), which is -0.049, thus has the 95% confidence interval [-0.053, -0.044].\n\nThe residual standard error (RSE)3 is an estimation of the average deviation from the observed values.\n\\(R^2\\)4 is important for assessing model fit because it “measures the proportion of variability in \\(Y\\) that can be explained using \\(X\\)” [James et al. (2021): 70; emphasis removed], varying between 0 and 1.\nThe \\(F\\)-statistic is used to measure the association between the dependent variable and the independent variable(s). Generally speaking, values greater than 1 indicate a possible correlation. A very low \\(p\\)-value suggests that the null hypothesis \\(H_0: \\beta_1 = 0\\) can be rejected.\n\n\n\n11.3.2 Multiple linear regression\nIn multiple linear regression, more than one predictor variable is taken into account. For instance, modelling log(RT) as a function of log(Freq), POS and Length requires a more complex model of the form\n\\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon.\\]\nPredictions are then obtained via the formula\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2 + ... + \\hat{\\beta}_px_p.\n\\]\nIn R, a multiple regression model is fitted as in the code example below:\n\n# Fit multiple regression model\n\nrt.lm2 &lt;- lm(log(RT) ~ log(Freq) + POS + Length, data = ELP)\n\n# View model statistics\n\nsummary(rt.lm2)\n\n\nCall:\nlm(formula = log(RT) ~ log(Freq) + POS + Length, data = ELP)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.26955 -0.07853 -0.00672  0.07067  0.39528 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.459742   0.016946 381.205  &lt; 2e-16 ***\nlog(Freq)   -0.038071   0.002130 -17.874  &lt; 2e-16 ***\nPOSNN       -0.006242   0.010157  -0.615  0.53902    \nPOSVB       -0.035234   0.012125  -2.906  0.00375 ** \nLength       0.023094   0.001711  13.495  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1114 on 875 degrees of freedom\nMultiple R-squared:  0.4784,    Adjusted R-squared:  0.476 \nF-statistic: 200.6 on 4 and 875 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear and multiple regression</span>"
    ]
  },
  {
    "objectID": "Linear_regression.html#visualising-regression-models",
    "href": "Linear_regression.html#visualising-regression-models",
    "title": "11  Linear and multiple regression",
    "section": "11.4 Visualising regression models",
    "text": "11.4 Visualising regression models\nPlot coefficient estimates:\n\n# Tidy the model output\ntidy_model &lt;- tidy(rt.lm2, conf.int = TRUE)\n\n# Remove intercept\ntidy_model &lt;- tidy_model %&gt;% filter(term != \"(Intercept)\")\n\n# Create the coefficient plot\nggplot(tidy_model, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient Estimate\",\n    y = \"Predictor\",\n    title = \"Coefficient Estimates with Confidence Intervals\"\n  )\n\n\n\n\n\n\n\n\nPlot contributions of individual variable values:\n\n# Plot marginal effects\n\nlibrary(\"effects\")\n\nplot(Effect(\"Freq\", mod = rt.lm2))\n\n\n\n\n\n\n\nplot(Effect(\"POS\", mod = rt.lm2))\n\n\n\n\n\n\n\nplot(Effect(\"Length\", mod = rt.lm2))",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear and multiple regression</span>"
    ]
  },
  {
    "objectID": "Linear_regression.html#model-assumptions-and-diagnostics",
    "href": "Linear_regression.html#model-assumptions-and-diagnostics",
    "title": "11  Linear and multiple regression",
    "section": "11.5 Model assumptions and diagnostics",
    "text": "11.5 Model assumptions and diagnostics\nAs a parametric method, linear regression makes numerous assumptions about the training data. It is, therefore, essential to run further tests to rule out possible violations. Among other things, the model assumptions include:\n\nA linear relationship between the response and the quantitative predictors: The residuals should not display a clear pattern. For this reason, it is recommended to use component residual plots (e.g., crPlot() from the car library) for the visual identification of potentially non-linear trends.\nNo heteroscedasticity (i.e, non-constant variance of error terms): Visually, a violation of this assumption becomes apparent if the residuals form a funnel-like shape. It is also possible to conduct a non-constant variance test ncvTest(): If it returns \\(p\\)-values &lt; 0.05, this suggests non-constant variance.\nNo multicollinearity: Predictors should not be correlated with each other. In the model data, correlated variables have unusually high standard errors, thereby decreasing the explanatory power of both the coefficients and the model as a whole. Another diagnostic measure are variance inflation factors (VIF-scores); predictors with VIF scores &gt; 5 are potentially collinear. They can be computed using the vif() function.\nNormally distributed residuals: The residuals should follow the normal distribution. Usually, a visual inspection using qqnorm() is sufficient, but the Shapiro-Wilke test shapiro.test() can also be run on the model residuals. Note that a \\(p\\)-value below 0.05 provides evidence for non-normality.\n\n\n\n\n\n\n\nImportant\n\n\n\nBeside the points mentioned above, it is always recommend to examine the model with regard to\n\noutliers that might skew the regression estimates,\ninteractions, i.e., combined effects of predictors, and\noverfitting, which results in poor model performance outside the training data.\n\n\n\n\n\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. New York: Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nLevshina, Natalia. 2015. How to Do Linguistics with r: Data Exploration and Statistical Analysis. Amsterdam; Philadelphia: John Benjamins Publishing Company.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using r. New York; London: Routledge.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear and multiple regression</span>"
    ]
  },
  {
    "objectID": "Logistic_regression.html#preparation",
    "href": "Logistic_regression.html#preparation",
    "title": "12  Logistic Regression",
    "section": "12.1 Preparation",
    "text": "12.1 Preparation\nConsider the data from Buskin’s (n.d.) corpus-study on subject pronoun realisation:\n\n# Load libraries\nlibrary(\"tidyverse\")\nlibrary(\"rms\") # For regression modelling\n\n# Load data\ndata_pro &lt;- read_csv(\"data/Buskin_pronoun_data.csv\", sep = \",\", header = TRUE)\n\n\nTarget variable:\n\nReference (‘overt’, ‘null’)\n\nExplanatory variables:\n\nPerson (‘1.p.’, ‘2.p’, ‘3.p’ as well as the dummy pronouns ‘it’ and ‘there’)\nRegister (the text category in the International Corpus of English; ‘S1A’ are informal conversations, whereas ‘S1B’ comprises formal class lessons)\nVariety (British English ‘GB’, Singapore English ‘SING’ and Hong Kong English ‘HK’) and\nReferentiality (‘referential’ with an identifiable referent or ‘non-referential’ with no/generic reference)\n\n\n\nhead(data_pro)\n\n  Reference Person Register Variety Referentiality\n1     overt      3      S1A      GB    referential\n2     overt      3      S1A      GB    referential\n3     overt      3      S1A      GB    referential\n4     overt      3      S1A      GB    referential\n5     overt      3      S1A      GB    referential\n6     overt      3      S1A      GB    referential\n\ntable(data_pro$Reference)\n\n\n null overt \n  174  4664 \n\n\n\n12.1.1 Descriptive overview\n\n\nCode\n# Raw data for Ref x Reg x Var\ndata_pro %&gt;% \n  count(Reference, Register, Variety) %&gt;% \n  filter(Reference == \"null\") %&gt;% \n  mutate(pct = n/sum(n) * 100) -&gt; pro_stats1\n\n# Raw data for Ref x Per x Var\ndata_pro %&gt;% \n  count(Reference, Person, Variety) %&gt;% \n  filter(Reference == \"null\") %&gt;% \n  mutate(pct = n/sum(n) * 100) -&gt; pro_stats2\n\n# Raw data for Ref x Referent. x Var\ndata_pro %&gt;% \n  count(Reference, Referentiality, Variety) %&gt;% \n  filter(Reference == \"null\") %&gt;% \n  mutate(pct = n/sum(n) * 100) -&gt; pro_stats3\n\n# Plot 1\n\npro_stats1 %&gt;%\n  ggplot(aes(x = Variety, y = pct, color = Register)) +\n  geom_point(size = 3) +\n  theme_minimal() +\n  labs(\n    title = \"Null subjects by Variety and Register\",\n    y = \"Proportion of null subjects\"\n  )\n\n\n\n\n\n\n\n\n\nCode\n# Plot 2\n\npro_stats2 %&gt;%\n  ggplot(aes(x = Variety, y = pct, color = Person)) +\n  geom_point(size = 3) +\n  theme_minimal() +\n  labs(\n    title = \"Null subjects by Variety and Person\",\n    y = \"Proportion of null subjects\"\n  )\n\n\n\n\n\n\n\n\n\nCode\n# Plot 3\n\npro_stats3 %&gt;%\n  ggplot(aes(x = Variety, y = pct, color = Referentiality)) +\n  geom_point(size = 3) +\n  theme_minimal() +\n  labs(\n    title = \"Null subjects by Variety and Referentiality\",\n    y = \"Proportion of null subjects\"\n  )",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Logistic_regression.html#logistic-regression",
    "href": "Logistic_regression.html#logistic-regression",
    "title": "12  Logistic Regression",
    "section": "12.2 Logistic regression",
    "text": "12.2 Logistic regression\n\nIn contrast to linear regression, logistic regression models a qualitative response variable \\(Y\\) (here: Reference) with two outcomes as a function of the independent variables \\(X_p\\) (Register, Variety etc.). The goal is to predict a value for \\(Y\\).\nFor a given observation, the model should predict either a specific category of \\(Y\\) (e.g., Reference = overt or Reference = null) or provide a probability estimation of a particular outcome (e.g., the probability of Reference = null).\nThe probability that Reference = null given a predictor (e.g., Register) can be written as \\(P(\\text{Reference} = \\text{null} \\mid \\text{Register})\\) or more simply as \\(p(\\text{Register})\\).\nA core component of logistic regression is the logistic function. The rationale for using it is that the output of the function will always lie between \\(0\\) and \\(1\\), and it will always denote a probability.\n\n\n\n\n\n\n\n\n\n\n\n12.2.1 The logistic model\nAssuming a binary response variable \\(Y\\) with the values 1 and 0 and a single predictor \\(X\\), we can model the probability \\(P(Y = 1\\vert X) = p(X)\\) as\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}.\n\\]\nNote that \\(e \\approx 2.71828\\). This expression is equivalent to\n\\[\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0 + \\beta_1X.\n\\]\nThe fraction \\(\\frac{p(X)}{1-p(X)}\\) represents the odds, which stand for to the probability of one outcome (e.g., Reference = null) compared to the other (e.g., Reference = overt). Their logarithmic transformation are the log odds (or logits) of a model. When interpreting the output of a logistic model, note that\n\npositive log odds indicate an increase in \\(p(X)\\), whereas\nnegative log odds indicate a decrease in \\(p(X)\\).\n\nIf \\(X = \\text{Register}\\), then our model has the form:\n\\[\n\\log\\left(\\frac{P(\\text{Reference} = \\text{null} \\mid \\text{Register})}{1- P(\\text{Reference} = \\text{null} \\mid \\text{Register})}\\right) = \\beta_0 + \\beta_1\\text{Register}\n\\]\n\n\n12.2.2 Multiple logistic regression\nIf more than one predictor is included, the above equations can be expanded so as to take into account \\(p\\) slopes \\(\\beta_p\\) for \\(p\\) independent variables \\(X_p\\).\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1X_1 + ... +  \\beta_pX_p}}{1 + e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}.\n\\] Thus, the log odds correspond to the sum of \\(\\beta_pX_p\\),\n\\[\n\\begin{aligned}\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right) &= \\beta_0 + \\beta_1X_1 + ... +  \\beta_pX_p \\\\\n& = \\beta_0 + \\sum_{i=1}^{p} \\beta_i X_i\n\\end{aligned}\n\\] respectively.\n\n\n12.2.3 Odds ratios\nTo assess the strength of an effect, it is instructive to examine the odds ratios that correspond to the model coefficients. Odds ratios (OR) are defined as\n\\[\nOR(X_1) = e^{\\beta_1}.\n\\]\nEssentially, the OR describes the ratio between two odds with respect to another independent variable. This is illustrated for Reference given Register below:\n\\[\n\\text{OR}(\\text{Reference} \\mid \\text{Register}) = \\frac{\\frac{P(\\text{Reference} = \\text{null} \\mid \\text{Register} = \\text{S1A})}{P(\\text{Reference} = \\text{overt} \\mid \\text{Register} = \\text{S1A})}}{\\frac{P(\\text{Reference} = \\text{null} \\mid \\text{Register} = \\text{S1B})}{P(\\text{Reference} = \\text{overt} \\mid \\text{Register} = \\text{S1B})}}\n\\] Read as: ‘The ratio between the probability of a null vs. overt object in S1A and the probability of a null vs. overt object in S1B’.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Logistic_regression.html#workflow-in-r",
    "href": "Logistic_regression.html#workflow-in-r",
    "title": "12  Logistic Regression",
    "section": "12.3 Workflow in R",
    "text": "12.3 Workflow in R\n\n12.3.1 Step 1: Research question and hypotheses\nHow do the intra- and extra-linguistic variables suggested in the literature affect subject pronoun realisation (Definite Null Instantiation) in British English, Singapore English and Hong Kong English?\nGiven a significance level \\(\\alpha = 0.05\\), the hypotheses are: \\[\n\\begin{aligned}\nH_0: & \\quad \\text{None of the predictor coefficients deviate from 0}.\\\\\nH_1: & \\quad \\text{At least one predictor coefficient deviates from 0}.\n\\end{aligned}\n\\]\nThese can be restated mathematically as:\n\\[\n\\begin{aligned}\nH_0: & \\quad \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0 \\\\\nH_1: & \\quad \\text{At least one } \\beta_i \\neq 0 \\text{ for } i \\in \\{1, 2, \\ldots, p\\}\n\\end{aligned} \\]\n\n\n12.3.2 Step 2: Convert to factors and specify reference levels\nThe next step involves specifying reference levels for all categorical variables. This step is very important because it will directly impact the parameter estimation and, consequently, influence our interpretation of the model output.\n\nThe reference level of the response is usually chosen such that it corresponds to the unmarked or most frequent case. Since overt pronouns are much more common in the data, the reference level of the Reference variable will be set to Reference = overt. This way, the model coefficients will directly represent the probability of the null subject variant (i.e., the special case) given certain predictor configurations.\nThe predictor levels need to be specified as well. Among other things, we are interested in how the Asian Englishes pattern relative to British English. Therefore, we will define British English as the baseline for comparison.\n\nWe will use the following specifications:\n\n\n\n\n\n\n\n\nVariable\nFactor Levels\nPreferred Reference level\n\n\n\n\nRegister\nS1A, S1B\nS1A\n\n\nVariety\nGB, SING, HK\nGB\n\n\nPerson\n1, 2, 3, it, there\n3\n\n\nReferentiality\nreferential, non-referential\nreferential\n\n\n\n\n# Store \"Reference\" as factor\ndata_pro$Reference &lt;- as.factor(data_pro$Reference)\n\n## Specify reference level (the 'unmarked' case)\ndata_pro$Reference &lt;- relevel(data_pro$Reference, \"overt\")\n\n## Print levels\nlevels(data_pro$Reference)\n\n[1] \"overt\" \"null\" \n\n\nRepeat the procedure for the remaining categorical variables.\n\n\nCode\n# Store \"Register\" as factor\ndata_pro$Register &lt;- as.factor(data_pro$Register)\n\n## Specify reference level\ndata_pro$Register &lt;- relevel(data_pro$Register, \"S1A\")\n\n# Store \"Variety\" as factor\ndata_pro$Variety &lt;- as.factor(data_pro$Variety)\n\n## Specify reference level\ndata_pro$Variety &lt;- relevel(data_pro$Variety, \"GB\")\n\n# Store \"Person\" as factor\ndata_pro$Person &lt;- as.factor(data_pro$Person)\n\n## Specify reference level\ndata_pro$Person &lt;- relevel(data_pro$Person, \"3\")\n\n# Store \"Referentiality\" as factor\ndata_pro$Referentiality &lt;- as.factor(data_pro$Referentiality)\n\n## Specify reference level\ndata_pro$Referentiality &lt;- relevel(data_pro$Referentiality, \"referential\")\n\n\n\n\n12.3.3 Step 3: Fit the model\nThere are two functions that can fit logistic models in R: lrm() and glm().\n\n\n\n\n\n\nNote\n\n\n\nThe model formula below does not include Referentiality because several intermediary steps revealed it to be almost completely irrelevant for predicting Reference. In addition, the existing (and significant) interaction Variety:Person has been excluded to improve the interpretability of the model.\n\n\n\n# With lrm(); requires library(\"rms\")\n\n# Fit interaction model\nReference.lrm &lt;- lrm(Reference ~ Register + Variety + Register:Variety + Person, data = data_pro)\n\n# View model statistics\nReference.lrm\n\nLogistic Regression Model\n\nlrm(formula = Reference ~ Register + Variety + Register:Variety + \n    Person, data = data_pro)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs          4838    LR chi2     120.43      R2       0.092    C       0.729    \n overt       4664    d.f.             9     R2(9,4838)0.023    Dxy     0.458    \n null         174    Pr(&gt; chi2) &lt;0.0001    R2(9,503.2)0.199    gamma   0.488    \nmax |deriv| 4e-10                            Brier    0.034    tau-a   0.032    \n\n                            Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept                   -3.4132 0.2746 -12.43 &lt;0.0001 \nRegister=S1B                 0.0269 0.3807   0.07 0.9437  \nVariety=HK                   0.6712 0.3174   2.11 0.0345  \nVariety=SING                 1.1193 0.2959   3.78 0.0002  \nPerson=1                    -0.8807 0.1811  -4.86 &lt;0.0001 \nPerson=2                    -1.6441 0.2695  -6.10 &lt;0.0001 \nPerson=it                    0.7897 0.2978   2.65 0.0080  \nPerson=there                -2.5641 1.0095  -2.54 0.0111  \nRegister=S1B * Variety=HK    0.6035 0.4521   1.34 0.1819  \nRegister=S1B * Variety=SING -0.4753 0.4688  -1.01 0.3107  \n\n\n\n# With (glm); available in base R\n# Note the additional \"family\" argument!\nReference.glm &lt;- glm(Reference ~ Register + Variety + Register:Variety + Person, data = data_pro, family = \"binomial\")\n\n# View model statistics\nsummary(Reference.glm)\n\n\n\n\n\n\n\nStepwise variable selection\n\n\n\nWith the function drop1(), it is possible to successively remove variables from the complex model to ascertain which ones improve the model significantly (i.e., decrease the deviance and AIC scores).\n\ndrop1(Reference.glm, test = \"Chisq\")\n\n\n\n\n\n12.3.4 Step 4: Confidence intervals and odds ratios\n\n# Tidy the model output\ntidy_model &lt;- tidy(Reference.glm, conf.int = TRUE)\n\n# Remove intercept, compute odds ratios and their CIs\ntidy_model &lt;- tidy_model %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(\n    odds_ratio = exp(estimate),\n    odds.conf.low = exp(conf.low),\n    odds.conf.high = exp(conf.high)\n  )\n\n\n\n12.3.5 Step 5: Visualise the model\n\n# Create the coefficient plot\nggplot(tidy_model, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient Estimate (log-odds)\",\n    y = \"Predictor\",\n    title = \"Coefficient Estimates with Confidence Intervals\",\n    caption = \"*Note that the CIs of singificant predictors do not include 0.\"\n  )\n\n\n\n\n\n\n\n# Plot odds ratios\nggplot(tidy_model, aes(x = exp(estimate), y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = odds.conf.low, xmax = odds.conf.high), height = 0.2) +\n  geom_vline(xintercept = 1, linetype = \"dashed\", color = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient Estimate (odds ratios)\",\n    y = \"Predictor\",\n    title = \"Odds ratios with Confidence Intervals\",\n    caption = \"*Note that the CIs of singificant predictors do not include 1.\"\n  )\n\n\n\n\n\n\n\n# Plot marginal effects; y-axis = log odds of a null vs. overt subject\nplot(Effect(\"Register\", mod = Reference.glm)) \n\n\n\n\n\n\n\nplot(Effect(\"Variety\", mod = Reference.glm))\n\n\n\n\n\n\n\nplot(Effect(\"Person\", mod = Reference.glm))\n\n\n\n\n\n\n\n# Plot interactions\nplot(Effect(focal.predictors = c(\"Register\", \"Variety\"), mod = Reference.glm))\n\n\n\n\n\n\n\n\n\n\n12.3.6 Step 6: Interpret the model\nThe logistic regression model is statistically significant at \\(p &lt; 0.001\\) (\\(\\chi^2 = 120.43\\), \\(df = 9\\)) and has acceptable fit (Nagelkerke’s-\\(R^2\\) = \\(0.09\\), \\(C = 0.73\\)).\nThe model coefficients indicate that null subjects are significantly more likely in Singapore English compared to British English (Estimate = 1.12, 95% CI [0.56, 1.73], \\(p &lt; 0.001\\)). This effect is moderate with an \\(OR\\) of 3.06 (95% CI [1.75, 5.64]), suggesting that the probability of subject omission is elevated by a factor of approximately 3 in the Singaporean variety.\n…\n\n\n12.3.7 Step 7: Further model diagnostics\n\nCross-validation\n\n\n# Refit the model with additional settings\nReference.val &lt;- lrm(Reference ~ Register + Variety + Register:Variety + Person, data = data_pro, x = T, y = T)\n\n# Perform 200-fold cross-validation\nmodel.validated &lt;- validate(Reference.val, B = 200); model.validated \n\n          index.orig training    test optimism index.corrected   n\nDxy           0.4592   0.4711  0.4460   0.0251          0.4341 200\nR2            0.0923   0.1001  0.0839   0.0161          0.0761 200\nIntercept     0.0000   0.0000 -0.2638   0.2638         -0.2638 200\nSlope         1.0000   1.0000  0.9100   0.0900          0.9100 200\nEmax          0.0000   0.0000  0.0760   0.0760          0.0760 200\nD             0.0247   0.0268  0.0224   0.0044          0.0203 200\nU            -0.0004  -0.0004  0.0003  -0.0007          0.0003 200\nQ             0.0251   0.0272  0.0221   0.0051          0.0200 200\nB             0.0336   0.0336  0.0337  -0.0002          0.0338 200\ng             1.0081   1.2963  1.1566   0.1397          0.8684 200\ngp            0.0319   0.0329  0.0303   0.0026          0.0293 200\n\n# Slope optimism should be as low possible!\n\n\nMulticollinearity\n\n\n# Variable inflation factors further reveal severe multicollinearity\nvif(Reference.lrm)\n\n               Register=S1B                  Variety=HK \n                   5.818111                    4.006084 \n               Variety=SING                    Person=1 \n                   3.421687                    1.140407 \n                   Person=2                   Person=it \n                   1.089502                    1.102908 \n               Person=there   Register=S1B * Variety=HK \n                   1.007148                    6.218803 \nRegister=S1B * Variety=SING \n                   3.685075 \n\n\n\n\n\n\nBuskin, Vladimir. n.d. “Definite Null Instantiation in English(es): A Usage-based Construction Grammar Approach.” Constructions and Frames.\n\n\nHosmer, David W., and Stanley Lemeshow. 2008. Applied Logistic Regression. 2nd ed. New York: Wiley.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. New York: Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nLevshina, Natalia. 2015. How to Do Linguistics with r: Data Exploration and Statistical Analysis. Amsterdam; Philadelphia: John Benjamins Publishing Company.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Decision_trees_and_random_forests.html#introduction",
    "href": "Decision_trees_and_random_forests.html#introduction",
    "title": "13  Decision trees and random forests",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\nDecision trees and random forests are very popular non-parametric methods. As such, “they do not make explicit assumptions about the functional form of \\(f\\)’’ (James et al. 2021: 23).\nIn this unit, we will cover the conceptual basics of these methods as well as their implementation in R using the tv data from Levshina (2020) in addition to the ELP data from the unit on Linear Regression. The libraries we will need are listed below:\n\n# Load libraries\nlibrary(\"tidyverse\")\nlibrary(\"readxl\")\nlibrary(\"tree\") # for CART\nlibrary(\"randomForest\") # for traditional random forests\nlibrary(\"party\") # for Conditional Inference Trees\nlibrary(\"pdp\") # for partial dependence plots\n\n## Reaction time data\nELP &lt;- read_xlsx(\"data/ELP.xlsx\")\n\nELP$POS &lt;- as.factor(ELP$POS)\n\n## Levshina's (2020) data set on T/V forms in Russian\ntv &lt;- read.csv(\"data/Levshina_2020_tv_data.csv\", sep = \",\", header = TRUE, stringsAsFactors = TRUE)\n\nIn the tv data frame, our target variable will be T/V Form with the two outcomes ty (Russian 2.p.sg., informal) and vy (Russian 2.p.pl., polite).\n\nstr(tv)\n\nhead(tv)",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Decision trees and random forests</span>"
    ]
  },
  {
    "objectID": "Decision_trees_and_random_forests.html#decision-trees",
    "href": "Decision_trees_and_random_forests.html#decision-trees",
    "title": "13  Decision trees and random forests",
    "section": "13.2 Decision trees",
    "text": "13.2 Decision trees\nCore concepts:\n\nSegmenting the feature space: “[T]he feature space (i.e., the space spanned by all predictor variables) is recursively partitioned into a set of rectangular areas” (Strobl, Malley, and Tutz 2009: 325).\nImpurity reduction: These simplified prediction areas should consist of mostly homogeneous (i.e., ‘pure’ rather than ‘mixed’) observations.\nTree construction: The ‘decisions’ made when partitioning the training data can be visualised using tree structures. The nodes of a tree represent variables, the branches represent decision rules, and leaf nodes indicate the final outcome (e.g., a prediction).\nCART: The original computational implementation of decision trees is known as the CART (Classification and Regression Trees) algorithm developed by Breiman (1984).\n\n\n13.2.1 Classification trees\nIf we are dealing with a categorical response variable, the tree() function can be used to fit a classification tree in accordance with Breiman’s CART algorithm. For illustration, consider the tv data frame. We will model the choice of the pronoun Form based on the speaker’s and hearer’s social circle (Rel_Circle) and their difference in social class (Rel_Class).\n\n# Set random number generator for reproducibility\nset.seed(123)\n\n# Supply model formula\ntree.tv &lt;- tree(Form ~ Rel_Circle + Rel_Class, data = tv)\n\n# View tree statistics\nsummary(tree.tv)\n\n\nClassification tree:\ntree(formula = Form ~ Rel_Circle + Rel_Class, data = tv)\nNumber of terminal nodes:  9 \nResidual mean deviance:  0.974 = 213.3 / 219 \nMisclassification error rate: 0.2412 = 55 / 228 \n\n# Visualisation\nplot(tree.tv)\n\ntext(tree.tv, pretty = 3)\n\n\n\n\n\n\n\n\nAn important problem that arises during tree construction is that of split selection. When should the tree split a node into two further nodes and when not? Furthermore, when should the tree stop the splitting process entirely? In this respect, CART relies on the principle of impurity reduction: “The fundamental idea is to select each split of a subset so that the data in each of the descendent subsets are ‘purer’ than the data in the parent subset” (Breiman 1984: 23). A measure for node purity is the Gini index \\(G\\), which is defined as\n\\[\nG = \\sum_{k=1}^{K}{\\hat{p}_{mk}(1-\\hat{p}_{mk}),}\n\\]\nwhere \\(\\hat{p}_{mk}\\) measures the proportion of observations of a response level \\(k\\) in the \\(m\\)th prediction area of the training data set. Values close to 0 are indicative of high node purity, meaning that most observations belong to the same class (e.g., Form = ty). If splitting a node no longer leads to a substantial increase in purity, it becomes the terminal node, i.e., it is not split further. This terminal node returns the tree’s class prediction.\nIt is worth noting that modern CART implementations rely on different splitting criteria. For instance, Conditional Inference Trees use the \\(p\\) values of internal association tests to identify which variables warrant further subdivision of the training data. The presence or absence of correlation thus also determines whether or not a given node will be terminal (for more details, see Greenwell 2022: 122).\n\n# Fitting a conditional inference tree\nctree.tv &lt;- ctree(Form ~ ., data = tv) # dot . means 'include all predictors'\n\nplot(ctree.tv)\n\n\n\n\n\n\n\n\n\n\n13.2.2 Regression trees\nRegression trees are used for continuous response variables. Instead of providing class predictions, they return the mean value of observations in a given prediction area. The algorithm now strives to minimize the residual sum of squares (\\(RSS\\); cf. footnote 1 in 8. Linear Regression). Consider the regression tree for reaction times depending on word length, frequency and part of speech:\n\n# CART tree\ntree.rt &lt;- tree(RT ~ Length + Freq + POS, data = ELP)\n\nsummary(tree.rt)\n\n\nRegression tree:\ntree(formula = RT ~ Length + Freq + POS, data = ELP)\nVariables actually used in tree construction:\n[1] \"Freq\"   \"Length\"\nNumber of terminal nodes:  10 \nResidual mean deviance:  8629 = 7507000 / 870 \nDistribution of residuals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-220.10  -59.09  -11.17    0.00   49.66  397.40 \n\nplot(tree.rt)\n\ntext(tree.rt, pretty = 0)\n\n\n\n\n\n\n\n# Conditional inference tree\nctree.rt &lt;- ctree(RT ~ Length + Freq + POS, data = ELP) \n\nplot(ctree.rt)",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Decision trees and random forests</span>"
    ]
  },
  {
    "objectID": "Decision_trees_and_random_forests.html#random-forests",
    "href": "Decision_trees_and_random_forests.html#random-forests",
    "title": "13  Decision trees and random forests",
    "section": "13.3 Random forests",
    "text": "13.3 Random forests\nRandom forests (Breiman 2001) belong to the class of ensemble methods because they combine simpler models (e.g., individual decision trees) into a more complex and possibly more accurate model. As part of the RF algorithm, a great number of decision trees is trained on bootstrapped samples of the training data.\nSo far, random forests are essentially identical with Bagging (= bootstrap aggregation); however, an important additional characteristic of the RF algorithm is that only a random subset of the predictors is taken into consideration at each split. According to (Strobl, Malley, and Tutz 2009: 332), the resulting variability in tree structure is advantageous: “By combining the prediction of such a diverse set of trees, ensemble methods utilize the fact that classification trees are unstable, but, on average, produce the right prediction”.\n\n13.3.1 Regression forest\nFor regression tasks, random forests return the average prediction of all trees in the ensemble.\n\n# For regression\nrt.rf.reg &lt;- randomForest(RT ~ Length + Freq + POS, data = ELP,\n                                mtry = 1, # = sqrt(number of variables)\n                                ntree = 500) # number of trees\n\nrt.rf.reg\n\n\nCall:\n randomForest(formula = RT ~ Length + Freq + POS, data = ELP,      mtry = 1, ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 8972.927\n                    % Var explained: 43.64\n\n# Conditional random forest\nrt.crf.reg &lt;- cforest(RT ~ Length + Freq + POS, data = ELP, \n                    controls = cforest_unbiased(ntree = 500, mtry = 1))\n\n\n\n13.3.2 Classification forest\nFor classification, all trees cast a vote for one of the response classes. The OOB error estimate refers to the accuracy of out-of-bag (OOB) predictions. After the initial bootstrapping procedure, roughly a third of the training data remains unused. These observations, which were not used for fitting trees, can be used as a test data set. Predictions based on this internal test data set are called OOB predictions.\n\n# For classification\ntv.rf.class &lt;- randomForest(Form ~ ., data = tv,\n                            mtry = 4,\n                            ntree = 500)\n\ntv.rf.class\n\n\nCall:\n randomForest(formula = Form ~ ., data = tv, mtry = 4, ntree = 500) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n        OOB estimate of  error rate: 18.86%\nConfusion matrix:\n   ty vy class.error\nty 86 22   0.2037037\nvy 21 99   0.1750000\n\n# Conditional random forest\ntv.crf.class &lt;- cforest(Form ~ ., data = tv,\n                    controls = cforest_unbiased(ntree = 500, mtry = 4))\n\ntv.crf.class\n\n\n     Random Forest using Conditional Inference Trees\n\nNumber of trees:  500 \n\nResponse:  Form \nInputs:  Film, Rel_Age, Rel_Sex, Rel_Power, Rel_Circle, S_Class, H_Class, S_Age, H_Age, Rel_Class, Before68, Others, Office, S_Sex, H_Sex, Place \nNumber of observations:  228 \n\n\n\n\n13.3.3 Variable importance\nRandom forests allow users to assess whether or not certain predictors are useful for the model. The Gini index can be re-used to identify those variables that have led to the greatest reduction in impurity. However, this measure is biased towards predictors with many values (cf. Strobl et al. 2007).\n\n# Gini importance (Reaction times)\nvarImpPlot(rt.rf.reg)\n\n\n\n\n\n\n\n# Gini importance (Form of 2.p.)\nvarImpPlot(tv.rf.class)\n\n\n\n\n\n\n\n\nA more robust measures is (Conditional) Permutation Accuracy Importance which compares the predictive accuracy of the random forest model before and after randomly permuting the values of the predictors (cf. Strobl et al. 2008; Debeer and Strobl 2020).\n\n# Conditional permutation accuracy importance\nlibrary(\"permimp\")\n\n# Refit RF model with additional parameters\ntv.rf.class &lt;- randomForest(Form ~ .,\n                            data = tv,\n                            mtry = 4,\n                            ntree = 500,\n                            keep.inbag = TRUE,\n                            keep.forest = TRUE)\n\n# Compute CPI scores\ntv.rf.permimp &lt;- permimp(tv.rf.class, conditional = TRUE, progressBar = FALSE, threshold = .95) # Choose \"Yes\" in the console\n\n# Plot CPI scores\nplot(tv.rf.permimp, horizontal = TRUE, type = \"dot\", sort = TRUE)\n\n\n\n13.3.4 Visualising random forest models\n\nPartial dependence plots (pdp package, cf. also Hastie et al. (2017)) provide averaged predictions (\\(\\hat{y}\\)) for a given constellation of predictors.\n\n\n# Form ~ Rel_Circle\nRel_Circle.partial &lt;- pdp::partial(tv.rf.class, pred.var = \"Rel_Circle\", which.class = \"ty\")\n\nRel_Circle.partial %&gt;% \n  ggplot(aes(x = Rel_Circle, y = yhat)) +\n  geom_point(col = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    title = \"Probability of 'ty' (2.p.sg.) depending on social circle\",\n    y = \"Log odds of 'ty'\"\n  )\n\n\n\n\n\n\n\n# RT ~ POS\npos.partial &lt;- pdp::partial(rt.rf.reg, pred.var = \"POS\")\n\npos.partial %&gt;% \n  ggplot(aes(x = POS, y = yhat)) +\n  geom_point(col = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    title = \"Predicted reaction times by POS\",\n    y = \"Predicted reaction time\"\n  )\n\n\n\n\n\n\n\n# RT ~ Length\nlength.partial &lt;- pdp::partial(rt.rf.reg, pred.var = \"Length\")\n\nlength.partial %&gt;% \n  ggplot(aes(x = Length, y = yhat)) +\n  geom_line(col = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    title = \"Predicted reaction times by word length\",\n    y = \"Predicted reaction time\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nBreiman, Leo. 1984. Classification and Regression Trees. Belmont, Calif.: Wadsworth International Group.\n\n\n———. 2001. “Random Forests.” Machine Learning 45 (1): 5–32. https://doi.org/10.1023/A:1010933404324.\n\n\nDebeer, Dries, and Carolin Strobl. 2020. “Conditional Permutation Importance Revisited.” Bioinformatics 21 (1): 307.\n\n\nGreenwell, Brandon. 2022. Tree-Based Methods for Statistical Learning in r. London & New York: Taylor & Francis Group. https://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=3288358.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome H. Friedman. 2017. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York, NY: Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. New York: Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nLevshina, Natalia. 2020. “Conditional Inference Trees and Random Forests.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Thomas Gries, 611–43. Cham: Springer.\n\n\nStrobl, Carolin, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin, and Achim Zeileis. 2008. “Conditional Variable Importance for Random Forests.” BMC Bioinformatics 9 (1): 307.\n\n\nStrobl, Carolin, Anne-Laure Boulesteix, Achim Zeileis, and Torsten Hothorn. 2007. “Bias in Random Forest Variable Importance Measures: Illustrations, Sources and a Solution.” BMC Bioinformatics 8 (1): 25.\n\n\nStrobl, Carolin, James Malley, and Gerhard Tutz. 2009. “An Introduction to Recursive Partitioning: Rationale, Application, and Characteristics of Classification and Regression Trees, Bagging, and Random Forests.” Psychological Methods 14: 323–48. https://doi.org/10.1037/a0016973.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Decision trees and random forests</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "14  References",
    "section": "",
    "text": "References\n\n\nBreiman, Leo. 1984. Classification and Regression Trees.\nBelmont, Calif.: Wadsworth International Group.\n\n\n———. 2001. “Random Forests.” Machine Learning 45\n(1): 5–32. https://doi.org/10.1023/A:1010933404324.\n\n\nBuskin, Vladimir. n.d. “Definite Null Instantiation\nin English(es): A Usage-based\nConstruction Grammar Approach.” Constructions and\nFrames.\n\n\nDebeer, Dries, and Carolin Strobl. 2020. “Conditional Permutation\nImportance Revisited.” Bioinformatics 21 (1): 307.\n\n\nGreenwell, Brandon. 2022. Tree-Based Methods for Statistical\nLearning in r. London & New York: Taylor & Francis Group.\nhttps://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=3288358.\n\n\nGries, Stefan Thomas. 2013. Statistics for Linguistics with r: A\nPractical Introduction. 2nd rev. ed. Berlin; Boston: De Gruyter\nMouton.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome H. Friedman. 2017. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction. 2nd ed. New York, NY: Springer.\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022.\nIntroduction to Statistics and Data Analysis: With Exercises,\nSolutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nHosmer, David W., and Stanley Lemeshow. 2008. Applied Logistic\nRegression. 2nd ed. New York: Wiley.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning: With Applications in\nr. New York: Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nLevshina, Natalia. 2015. How to Do Linguistics with r: Data\nExploration and Statistical Analysis. Amsterdam; Philadelphia: John\nBenjamins Publishing Company.\n\n\n———. 2020. “Conditional Inference Trees and Random\nForests.” In A Practical Handbook of Corpus Linguistics,\nedited by Magali Paquot and Stefan Thomas Gries, 611–43. Cham: Springer.\n\n\nPaquot, Magali, and Tove Larsson. 2020. “Descriptive Statistics\nand Visualization with r.” In A Practical Handbook of Corpus\nLinguistics, edited by Magali Paquot and Stefan Thomas Gries,\n375–99. Cham: Springer.\n\n\nStrobl, Carolin, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin,\nand Achim Zeileis. 2008. “Conditional Variable Importance for\nRandom Forests.” BMC Bioinformatics 9 (1): 307.\n\n\nStrobl, Carolin, Anne-Laure Boulesteix, Achim Zeileis, and Torsten\nHothorn. 2007. “Bias in Random Forest Variable Importance\nMeasures: Illustrations, Sources and a Solution.” BMC\nBioinformatics 8 (1): 25.\n\n\nStrobl, Carolin, James Malley, and Gerhard Tutz. 2009. “An\nIntroduction to Recursive Partitioning: Rationale, Application, and\nCharacteristics of Classification and Regression Trees, Bagging, and\nRandom Forests.” Psychological Methods 14: 323–48. https://doi.org/10.1037/a0016973.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using\nr. New York; London: Routledge.",
    "crumbs": [
      "References",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative methods in corpus linguistics: A reader",
    "section": "",
    "text": "First steps\nThis collection of handouts provides a hands-on introduction to data analysis and statistical methods in quantitative corpus linguistics with R. It requires a working installation of both R and RStudio.\nIt is geared primarily towards students attending the classes Language Variation (BA) and Statistics for Linguistics (MA) as well as those currently working on their BA/MA/PhD theses in empirical linguistics.",
    "crumbs": [
      "Introduction to R",
      "First steps"
    ]
  },
  {
    "objectID": "index.html#installing-r",
    "href": "index.html#installing-r",
    "title": "Quantitative methods in corpus linguistics: A reader",
    "section": "Installing R",
    "text": "Installing R\nFirst, we need to download R. The link will take you to the homepage of the Comprehensive R Archive Network (CRAN) where you can download the binary distribution. Choose the one that corresponds to your operating system (Windows/MAC/Linux).\n\n\n\n\n\n\nInstallation instructions for Windows users\n\n\n\n\n\nClick “Download R for Windows” \\(\\rightarrow\\) Select “base” \\(\\rightarrow\\) Click on “Download R-4.4.1 for Windows” (or whatever most recent version is currently displayed).\nOpen the set-up file you’ve just downloaded and simply follow the instructions on screen. It’s fine to go with the default options.\nVideo tutorial on YouTube\n\n\n\n\n\n\n\n\n\nInstallation instructions for MacOS users\n\n\n\n\n\nClick “Download R for macOS” \\(\\rightarrow\\) Select the latest release for your OS\nOpen the downloaded .pkg file and follow the instructions in the installation window.\nVideo tutorial on YouTube",
    "crumbs": [
      "Introduction to R",
      "First steps"
    ]
  },
  {
    "objectID": "index.html#installing-rstudio",
    "href": "index.html#installing-rstudio",
    "title": "Quantitative methods in corpus linguistics: A reader",
    "section": "Installing RStudio",
    "text": "Installing RStudio\nYou can now download and install RStudio. RStudio is a so-called “Integrated Development Environment” (IDE), which will provide us with a variety of helpful tools to write and edit code comfortably. If R was a musical instrument, then RStudio would be the recording studio, so-to-speak.",
    "crumbs": [
      "Introduction to R",
      "First steps"
    ]
  },
  {
    "objectID": "Concordancing.html#using-the-quantedaapp",
    "href": "Concordancing.html#using-the-quantedaapp",
    "title": "5  Querying a corpus",
    "section": "5.1 Using the QuantedaApp",
    "text": "5.1 Using the QuantedaApp\n\n5.1.1 Installation\nInstallation and usage instructions are available in this GitHub repository.",
    "crumbs": [
      "Corpus linguistics with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Querying a corpus</span>"
    ]
  },
  {
    "objectID": "Concordancing.html#using-the-quanteda-package-in-r",
    "href": "Concordancing.html#using-the-quanteda-package-in-r",
    "title": "5  Querying a corpus",
    "section": "5.2 Using the quanteda package in R",
    "text": "5.2 Using the quanteda package in R\n\n5.2.1 Preparation\nIn order for R to be able to recognise the data, it is crucial to set up the working directory correctly.\n\nMake sure your R-script and the corpus (e.g., ‘ICE-GB’) are stored in the same folder.\nIn RStudio, now navigate to Session &gt; Set working directory &gt; To Source File Location. This ensures that the folder where you have placed your R-script will function as your working directory until you close RStudio again.\n\nTo see your working directory in your files pane, click on Files &gt; 'Blue wheel symbol' &gt; Go to working directory.\n\n\n5.2.2 Loading the corpus\nAfter specifying the working directory and loading the libraries we will need, we can read in the corpus files into a corpus object in R.\nFirst, simply copy-paste the following code chunk at the beginning of your R-script. Once you run it, it will load the function read_GB() into R’s working memory (it should now appear in the Environment tab!). This function will automatically handle the entire reading-in process.\nTo now get all corpus files into R, all we have to do is call the function:\n\nICE_GB &lt;- readRDS(\"ICE_GB.RDS\")\n\nIf you encounter error messages, make sure you followed steps 1 and 2 above.\n\n\n5.2.3 Concordancing\n\nlibrary(quanteda)\nlibrary(tidyverse)\nlibrary(kableExtra)\n\n# Load corpus\nICE_GB &lt;- readRDS(\"ICE_GB.RDS\")\n\n\nConcordances\n\n\n# Query the corpus\nquery1 &lt;- kwic(ICE_GB, pattern = \"provid(e|es|ing|ed)\", valuetype = \"regex\")\n\nquery1 %&gt;%\n  as_tibble() %&gt;% \n  count(keyword)\n\n# A tibble: 10 × 2\n   keyword          n\n   &lt;chr&gt;        &lt;int&gt;\n 1 Provided         5\n 2 Provident        1\n 3 Providing        1\n 4 provide        165\n 5 provided       118\n 6 providential     1\n 7 provider         1\n 8 providers        3\n 9 provides        72\n10 providing       52\n\n# Print first six lines to console\n#head(as_tibble(kwic_provide))\n\n# View output in separate window\n#View(kwic_provide)\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-003.txt\n1994\n1994\n&gt; Uhm or whatever they\nprovide\n&lt; ICE-GB:S1A-003 #82 : 1\nprovid(e|es|ing|ed)\n\n\nICE_GB/S1A-010.txt\n827\n827\nquandary about what to to\nprovide\n&lt; , , &gt; &lt;\nprovid(e|es|ing|ed)\n\n\nICE_GB/S1A-023.txt\n3230\n3230\nfrom there is Gospel Oak\nprovided\nyou know the time of\nprovid(e|es|ing|ed)\n\n\nICE_GB/S1A-024.txt\n1851\n1851\nwhat L S E is\nproviding\n&lt; ICE-GB:S1A-024 #92 : 1\nprovid(e|es|ing|ed)\n\n\nICE_GB/S1A-031.txt\n998\n998\nher own &lt; , &gt;\nproviding\nher &lt; ICE-GB:S1A-031 #47 :\nprovid(e|es|ing|ed)\n\n\nICE_GB/S1A-038.txt\n730\n730\nthis as viewing banking as\nproviding\n&lt; , , &gt; &lt;\nprovid(e|es|ing|ed)\n\n\n\n\n\n\n\n\n5.2.3.1 Increase search window\n\n# Query the corpus\nkwic_provide2 &lt;- kwic(tokens(ICE_GB_corpus),\n                     pattern = \"provide\",\n                     window = 20) # choose window size\n\n# View output in separate window\n#View(kwic_provide)",
    "crumbs": [
      "Corpus linguistics with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Querying a corpus</span>"
    ]
  },
  {
    "objectID": "Concordancing.html#regular-expressions",
    "href": "Concordancing.html#regular-expressions",
    "title": "5  Querying a corpus",
    "section": "5.3 Regular expressions",
    "text": "5.3 Regular expressions\nRegular expressions (or ‘regex’) help us find more complex patterns in strings of text. Suppose we are interested in finding all inflectional forms of the lemma PROVIDE in a corpus, i.e., provide, provides, providing and provided. Insteading of searching for all forms individually, we can construct a regular expression of the form\n\\[\n\\text{provide(s | ing | ed)?}\n\\] which can be read as ‘Match the sequence of letters &lt;provide&gt; as well as when it is optionally followed by the letters &lt;s&gt; or &lt;ing&gt; or &lt;ed&gt;’. Notice how optionality is signified by the ‘?’ operator and alternatives by ‘|’.\nTo activate regular expression in a KWIC query, simply set the valuetype argument to \"regex\":\n\n# Query the corpus\nkwic_provide3 &lt;- kwic(ICE_GB,\n                     pattern = \"provide(s|ing|ed)?\",\n                     valuetype = \"regex\", # query format\n                     window = 20)\n\n                     \n\n# View output in separate window\n#View(kwic_provide)\n\nThe number of hits has more than doubled. However, upon closer inspection, we’ll notice a number of false positives (e.g., providential).\n\n5.3.1 Example usage\n\n\n5.3.2 Exporting the results",
    "crumbs": [
      "Corpus linguistics with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Querying a corpus</span>"
    ]
  },
  {
    "objectID": "Libraries.html#working-with-packages-in-r",
    "href": "Libraries.html#working-with-packages-in-r",
    "title": "4  Libraries",
    "section": "4.1 Working with packages in R",
    "text": "4.1 Working with packages in R\nPackages expand the basic functionality of R by providing numerous quality-of-life improvements that not only considerably simplify common data wrangling tasks but which also provide frameworks for state-of-the-art methods for statistical analysis and natural language processing (NLP), among many other things.\n\n4.1.1 Installation\n\n\n\n\n\n\nHow do I install a library?\n\n\n\n\n\nNnavigate to Packages &gt; Install and verify that the pop-up window says Install from: Repository (CRAN). You can now type in the name of the package you would like to install under Packages.\nVideo tutorial on YouTube\n\n\n\nThis reader will use functions from a variety of R packages. Please install the following ones:\n\nquanteda (for the analysis of text data)\ntidyverse (a framework for data manipulation and visualisation)\nreadxl (for importing Microsoft Excel files)\nwritexl (for exporting Microsoft Excel files)\nkableExtra (for creating beautiful tables)\n\n\n\n4.1.2 Loading packages\nOnce the installation has been completed, you can proceed to load the libraries using the code below. You can ignore the warning messages.\n\nlibrary(quanteda)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(writexl)\nlibrary(kableExtra)\n\n\n\n\n\n\n\nActivating libraries\n\n\n\n\n\nWhenever you start a new R session (i.e., open RStudio), your libraries and their respective functions will be inactive. To re-activate a library, either use the library() function or simply select it in the Packages tab.\n\n\n\nIt is good practice to only activate those packages that are necessary for your analysis. While it won’t be a problem for the small set of packages as shown here, loading dozens of packages increases the risk of obtaining “homonymous” functions which have the same name but perform different operations. In this case, it might be helpful to “disambiguate” them by directly indicating which package a function is from:\n\nreadxl::read_xlsx(...)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Libraries</span>"
    ]
  },
  {
    "objectID": "Importing_exporting.html#preparation",
    "href": "Importing_exporting.html#preparation",
    "title": "5  Importing and exporting data",
    "section": "5.1 Preparation",
    "text": "5.1 Preparation\nThe first section of an R-script always specifies the libraries that are needed for executing the code to follow. In this unit, we will need readxl and writexl to aid us with importing MS Excel files.\n\nlibrary(readxl)\nlibrary(writexl)\n\nSimply copy the code lines above into your script and execute them.\n\n5.1.1 Exporting data\nAssume we’d like to save our data frame with word frequencies to a local folder on our system. Let’s briefly regenerate it:\n\nlemma &lt;- c(\"start\", \"enjoy\", \"begin\", \"help\")\n\nfrequency &lt;- c(418, 139, 337, 281)\n\ndata &lt;- data.frame(lemma, frequency)\n\nprint(data)\n\n  lemma frequency\n1 start       418\n2 enjoy       139\n3 begin       337\n4  help       281\n\n\nThere are two common formats in which tabular data can be stored on your computer:\n\nin .csv-files (‘comma-separated values’; native format of LibreOffice)\n.xls/.xlsx-files (Microsoft Excel files)\n\n\n\n\n\n\n\nExport to CSV\n\n\n\n\n\nTo save our data data frame in .csv-format, we can use the write_table() function:\n\nwrite.csv(data, \"frequency_data.csv\")\n\nThe file is now stored at the location of your current R-script. You can open this file …\n\nin LibreOffice\nin Microsoft Excel via File &gt; Import &gt; CSV file &gt; Select the file &gt; Delimited and then Next &gt; Comma and Next &gt; General and Finish.\n\nClearly, opening CSV files in MS Excel is quite cumbersome, which is why it’s better to export it as an Excel file directly.\n\n\n\n\n\n\n\n\n\nExport to Excel\n\n\n\n\n\nWe use write_xlsx() provided by the package writexl package:\n\nwrite_xlsx(data, \"frequency_data.xlsx\")\n\nThe file is now stored at the location of your current R-script. You should be able to open it in MS Excel without any issues.\n\n\n\n\n\n5.1.2 Importing data\nLet’s read the two files back into R.\n\n\n\n\n\n\nImport from CSV\n\n\n\n\n\nTo import the CSV file, we can use the read.csv() function:\n\nimported_csv &lt;- read.csv(\"frequency_data.csv\")\nprint(imported_csv)\n\n  X lemma frequency\n1 1 start       418\n2 2 enjoy       139\n3 3 begin       337\n4 4  help       281\n\n\nTo get rid of the column with the row names, we’ll use some subsetting:\n\nimported_csv &lt;- imported_csv[,-1] # delete first column\n\n\n\n\n\n\n\n\n\n\nImport from Excel\n\n\n\n\n\nFor importing the Excel file, we’ll use the read_xlsx() function from the readxl package:\n\nimported_excel &lt;- read_xlsx(\"frequency_data.xlsx\")\nprint(imported_excel)\n\n# A tibble: 4 × 2\n  lemma frequency\n  &lt;chr&gt;     &lt;dbl&gt;\n1 start       418\n2 enjoy       139\n3 begin       337\n4 help        281\n\n\n\n\n\nThat’s it! Nevertheless, remember to always check your imported data to ensure it has been read correctly, especially when working with CSV files.\n\n\n5.1.3 Exercises",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Importing and exporting data</span>"
    ]
  },
  {
    "objectID": "Summary_statistics.html#visualising-mixed-data",
    "href": "Summary_statistics.html#visualising-mixed-data",
    "title": "9  Describing continuous data",
    "section": "9.2 Visualising mixed data",
    "text": "9.2 Visualising mixed data\n\n9.2.1 A numerical and categorical variable\n\nBoxplot with geom_boxplot()\n\n\nggplot(cl.order, aes(x = ORDER, y = LEN_MC)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nDensitiy plot using the optional arguments color and/or fill\n\n\nggplot(cl.order, aes(x = LEN_MC, fill = ORDER)) +\n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nA barplot with geom_col()\n\n\nggplot(cl.order, aes(x = ORDER, y = LEN_MC)) +\n  geom_col(aes(x = ORDER, y = LEN_MC))\n\n\n\n\n\n\n\n\n\n\n9.2.2 Multivariate plots\n\nAdvanced scatterplot with four variables: LEN_MC (x), LEN_SC (y), ORDER (colour) and SUBORDTYPE (shape)\n\n\n# 4 variables\nggplot(cl.order, aes(x = LEN_MC, y = LEN_SC)) +\n  geom_point(aes(color = ORDER, shape = SUBORDTYPE))\n\n\n\n\n\n\n\n\n\nFacets\n\n\n# 5 variables\nggplot(cl.order, aes(x = LEN_MC, y = LEN_SC)) +\n  geom_point(aes(color = ORDER, shape = SUBORDTYPE)) +\n  facet_wrap(~MORETHAN2CL)\n\n\n\n\n\n\n\n\n\n\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using r. New York; London: Routledge.",
    "crumbs": [
      "Descriptive statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Describing continuous data</span>"
    ]
  },
  {
    "objectID": "Categorical_data.html#visualising-distributions",
    "href": "Categorical_data.html#visualising-distributions",
    "title": "8  Describing categorical data",
    "section": "8.3 Visualising distributions",
    "text": "8.3 Visualising distributions\n\n8.3.1 Building a ggplot\n\nA ggplot requires at minimum three elements: (1) a data frame, (2) axis labels, and (3) a plotting option (also known as “geom”). We combine them with the + sign.\n\n\n# Supply data frame\nggplot(data = cl.order,\n      # Supply axis labels\n        mapping = aes(x = LEN_MC, y = LEN_SC)) +\n      # Set plotting option (here: scatterplot)\n        geom_point()\n\n\n\n\n\n\n\n\n\n\n8.3.2 A categorical variable\n\nBarplot with geom_bar()\n\n\nggplot(cl.order, aes(x = ORDER)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n8.3.3 Two categorical variables\n\nBarplots with the fill argument\n\n\nggplot(cl.order, aes(x = ORDER, fill = SUBORDTYPE)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n8.3.4 Saving your plot\n\nSave last plot displayed in the viewer to your working directory:\n\n\nggplot(cl.order, aes(x = LEN_MC, y = LEN_SC)) +\n          geom_point()\n\nggsave(\"figures/clause_length_plot.png\")\n\n\n\n\n\nPaquot, Magali, and Tove Larsson. 2020. “Descriptive Statistics and Visualization with r.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Thomas Gries, 375–99. Cham: Springer.",
    "crumbs": [
      "Descriptive statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Describing categorical data</span>"
    ]
  },
  {
    "objectID": "Distributions.html#the-normal-distribution",
    "href": "Distributions.html#the-normal-distribution",
    "title": "10  Theoretical distributions",
    "section": "10.1 The normal distribution",
    "text": "10.1 The normal distribution\nA great number of numerical variables in the world follow the well-known normal (or Gaussian) distribution, which includes test scores, weight and height, among many others.\nIf a random variable \\(X\\) is normally distributed, it is determined by the parameters \\(\\mu\\) (the mean) and \\(\\sigma\\) (the standard deviation). Formally, we can summarise this using the notation\n\\[ X \\sim N(\\mu, \\sigma^2).\\] The probability density function (PDF) of the normal distribution has a characteristic bell-shape. The density values on the \\(y\\)-axis indicate the likelihood of encountering a specific value of \\(X\\) (cf. Winter 2020: 56; Heumann, Schomaker, and Shalabh 2022: 173-177).\n\n\n\n\n\n\n\n\n\n\n10.1.1 Bernoulli distribution\nThe Bernoulli distribution is a discrete probability distribution for random variables which have only two possible outcomes: “positive” (often coded as 1) and “negative” (often coded as 0). Examples of such variables include coin tosses (heads/tails), binary response questions (yes/no), and defect status (defective/non-defective).\nIf a random variable \\(X\\) follows a Bernoulli distribution, it is determined by the parameter \\(p\\), which is the probability of the positive case:\n\\[ X \\sim Bernoulli(p).\\] The probability mass function (PMF) of the Bernoulli distribution is given by: \\[\nP(X = x) =\n\\begin{cases}\np & \\text{if } x = 1 \\\\\n1 - p & \\text{if } x = 0\n\\end{cases}\n\\]\nwhere \\(0 \\leq p \\leq 1\\). This function shows the probability of \\(X\\) taking on the value of 1 or 0 (cf. Heumann, Schomaker, and Shalabh 2022: 162-163).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtensions\n\n\n\nA Bernoulli experiment presupposes a single trial (e.g., tossing a coin once). If we are interested in the distribution of a binary discrete variable over \\(n\\) Bernoulli trials, we can describe it in terms of the binomial distribution (Heumann, Schomaker, and Shalabh 2022: 163-166).\nCategorical variables with more than 2 outcomes and \\(n\\) Bernoulli trials can be modelled using the multinomial distribution (Heumann, Schomaker, and Shalabh 2022: 167-169).\n\n\n\n\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using r. New York; London: Routledge.",
    "crumbs": [
      "Descriptive statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Theoretical distributions</span>"
    ]
  },
  {
    "objectID": "Categorical_data.html#preparation",
    "href": "Categorical_data.html#preparation",
    "title": "8  Describing categorical data",
    "section": "8.1 Preparation",
    "text": "8.1 Preparation\nPlease download the file “Paquot_Larsson_2020_data.xlsx” (Paquot and Larsson 2020)1 and store it in the same folder as your currently active R-script.\n\n# Libraries\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\n\n# Load data from working directory\ncl.order &lt;- read_xlsx(\"Paquot_Larsson_2020_data.xlsx\")\n\nIt contains the dependent variable\n\nORDER: Does the subordinate clause come before or after the main clause? (‘sc-mc’ vs. ‘mc-sc’)\n\n… and the independent variables:\n\nSUBORDTYPE: Is the subordinate clause temporal or causal? (‘temp’ vs. ‘caus’)\nMORETHAN2CL: Are there most clauses in the sentence than just one subordinate clause and one main clause? (‘yes’ vs. ‘no’)\nLEN_MC: How many words does the main clause contain? (ratio-scaled continuous variable)\nLEN_SC: How many words does the subordinate clause contain? (ratio-scaled continuous variable)\nLENGTH_DIFF: What is the length difference in words between the main clause and subordinate clause? (ratio-scaled continuous variables)",
    "crumbs": [
      "Descriptive statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Describing categorical data</span>"
    ]
  },
  {
    "objectID": "Categorical_data.html#descriptive-measures",
    "href": "Categorical_data.html#descriptive-measures",
    "title": "8  Describing categorical data",
    "section": "8.2 Descriptive measures",
    "text": "8.2 Descriptive measures\nThe easiest way to get a general overview of the full data set is to apply the str() function to the respective data frame.\n\nstr(cl.order)\n\ntibble [403 × 8] (S3: tbl_df/tbl/data.frame)\n $ CASE       : num [1:403] 4777 1698 953 1681 4055 ...\n $ ORDER      : chr [1:403] \"sc-mc\" \"mc-sc\" \"sc-mc\" \"mc-sc\" ...\n $ SUBORDTYPE : chr [1:403] \"temp\" \"temp\" \"temp\" \"temp\" ...\n $ LEN_MC     : num [1:403] 4 7 12 6 9 9 9 4 6 4 ...\n $ LEN_SC     : num [1:403] 10 6 7 15 5 5 12 2 24 11 ...\n $ LENGTH_DIFF: num [1:403] -6 1 5 -9 4 4 -3 2 -18 -7 ...\n $ CONJ       : chr [1:403] \"als/when\" \"als/when\" \"als/when\" \"als/when\" ...\n $ MORETHAN2CL: chr [1:403] \"no\" \"no\" \"yes\" \"no\" ...\n\n\nThis shows us that the data frame has 8 columns, as the $ operators indicate ($ Case, $ ORDER, …). The column names are followed by\n\nthe data type (num for numeric and chr for character strings)\nthe number of values (`[1:403]`) and\nthe first few observations.\n\nAnother intuitive way to display the structure of a data matrix is to simply show the first few rows:\n\nhead(cl.order)\n\n# A tibble: 6 × 8\n   CASE ORDER SUBORDTYPE LEN_MC LEN_SC LENGTH_DIFF CONJ     MORETHAN2CL\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      \n1  4777 sc-mc temp            4     10          -6 als/when no         \n2  1698 mc-sc temp            7      6           1 als/when no         \n3   953 sc-mc temp           12      7           5 als/when yes        \n4  1681 mc-sc temp            6     15          -9 als/when no         \n5  4055 sc-mc temp            9      5           4 als/when yes        \n6   967 sc-mc temp            9      5           4 als/when yes        \n\n\n\n8.2.1 Frequency tables\n\n8.2.1.1 One variable\nEach categorical variable is made up of two or more categories. A simple descriptive measure is the frequency of each category. The table below indicates how often each clause order occurs in the data.\n\norder_freq1 &lt;- table(cl.order$ORDER) \n\nprint(order_freq1)\n\n\nmc-sc sc-mc \n  275   128 \n\n\nThe notation cl.order$ORDER subsets the cl.order according to the column ORDER (see data frames).\nAlternatively, you could use xtabs() to achieve the same result – perhaps with a slightly more intuitive syntax.\n\norder_freq2 &lt;- xtabs(~ ORDER, cl.order)\n\nprint(order_freq2)\n\nORDER\nmc-sc sc-mc \n  275   128 \n\n\n\n\n8.2.1.2 Two variables\nIf we are interested in the relationship between multiple categorical variables, we can cross-tabulate the frequencies of their categories. For example, what is the distribution of clause order depending on the type of subordinate clause?\n\norder_counts1 &lt;- table(cl.order$ORDER, cl.order$SUBORDTYPE)\n\nprint(order_counts1)\n\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n\n\nHere is the xtabs() alternative:\n\norder_counts2 &lt;- xtabs(~ ORDER + SUBORDTYPE, cl.order)\n\nprint(order_counts2)\n\n       SUBORDTYPE\nORDER   caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n\n\n\n\n\n\n\n\nHow do I obtain percentages?\n\n\n\n\n\nThere are two ways to convert the raw frequency counts to percentage tables:\n\nManually divide all cells by the total number of observations (which correspond to the sum of all cells) and multiply the result by 100.\n\n\npct1 &lt;- order_counts1/sum(order_counts1) * 100\n\n\nUse the prop.table() function and multiply the result by 100.\n\n\npct2 &lt;- prop.table(order_counts1) * 100",
    "crumbs": [
      "Descriptive statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Describing categorical data</span>"
    ]
  },
  {
    "objectID": "Data_frames.html#sec-df",
    "href": "Data_frames.html#sec-df",
    "title": "3  Data frames",
    "section": "3.2 Some technical details",
    "text": "3.2 Some technical details\nSince we now have two dimensions, the subsetting notation in square brackets [ ] has to reflect that. This is the general pattern:\n\\[ df[row, column] \\] Following this logic, we can get the element in the first row of the first column like so:\n\ndata[1,1]\n\n[1] \"start\"\n\n\nIf we, however, need the entire first row, we simply omit the column part. Note that the comma , still needs to be present!\n\ndata[1,]\n\n  lemma frequency\n1 start       418\n\n\nSubsetting by columns is interesting. We can either use the explicit notation with square brackets or the column operator $:\n\ndata[,1]\n\n[1] \"start\" \"enjoy\" \"begin\" \"help\" \n\ndata$lemma\n\n[1] \"start\" \"enjoy\" \"begin\" \"help\"",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "Categorical_data.html#visualising-categorical-variables",
    "href": "Categorical_data.html#visualising-categorical-variables",
    "title": "8  Describing categorical data",
    "section": "8.3 Visualising categorical variables",
    "text": "8.3 Visualising categorical variables\nThis section demonstrates both the in-built plotting functions of R (‘Base R’) as well as the more modern versions provided by the tidyverse package.\n\n8.3.1 One variable\n\nBase Rggplot2\n\n\n\nBase R barplot with barplot(); requires the counts as computed by tables() or xtabs()\n\n\nbarplot(order_freq1) # Supply the counts\n\n\n\n\n\n\n\n\n\n\n\nBarplot with geom_bar() using the raw input data\n\n\nlibrary(tidyverse)\n\nggplot(cl.order, aes(x = ORDER)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.3.2 Two variables\n\nBarplots with the fill argument\n\n\nBase RBase R (fully customised)ggplot2ggplot2 (fully customised)\n\n\n\nbarplot(order_counts2, \n        beside = TRUE,  # Make bars side-by-side\n        legend = TRUE)  # Add a legend\n\n\n\n\n\n\n\n\n\n\n\nbarplot(order_counts2, \n        beside = TRUE,  # Make bars dodged (i.e., side by side)\n        main = \"Distribution of ORDER by SUBORDTYPE (Base R)\", \n        xlab = \"ORDER\", \n        ylab = \"Frequency\", \n        col = c(\"lightblue\", \"lightgreen\"), # Customize colors\n        legend = TRUE,  # Add a legend\n        args.legend = list(title = \"SUBORDTYPE\", x = \"topright\"))\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\nggplot(cl.order, aes(x = ORDER, fill = SUBORDTYPE)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\nggplot(cl.order, aes(x = ORDER, fill = SUBORDTYPE)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    title = \"Clause order by subordinate clause type\",\n    x = \"Clause order\",\n    y = \"Frequency\",\n    fill = \"Type of subordinate clause\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow do I plot percentages?\n\n\n\n\n\nIn Base R, very much the same way as with the raw counts:\n\nbarplot(pct1, \n        beside = TRUE,  # Make bars side-by-side\n        legend = TRUE)  # Add a legend\n\n\n\n\n\n\n\n\nIn ggplot2, a few tweaks are necessary. In general, ggplot2 only works with data frames and not with table objects, so we’d have to convert it to one first:\n\npct1_df &lt;- as.data.frame(pct1)\n\nprint(pct1_df)\n\n   Var1 Var2      Freq\n1 mc-sc caus 45.657568\n2 sc-mc caus  3.722084\n3 mc-sc temp 22.580645\n4 sc-mc temp 28.039702\n\n\nNow we can plot the percentages with geom_col(). This geom (= ‘geometric object’) allows us to manually specify what should be mapped onto the y-axis:\n\nlibrary(tidyverse)\n\nggplot(pct1_df, aes(x = Var1, y = Freq, fill = Var2)) +\n  geom_col(position = \"dodge\")",
    "crumbs": [
      "Descriptive statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Describing categorical data</span>"
    ]
  },
  {
    "objectID": "Categorical_data.html#exporting-tables-to-ms-word",
    "href": "Categorical_data.html#exporting-tables-to-ms-word",
    "title": "8  Describing categorical data",
    "section": "8.4 Exporting tables to MS Word",
    "text": "8.4 Exporting tables to MS Word\nPublication-ready tables can be generated with the help of the flextable package. The full guide can be found here.\n\nlibrary(flextable)\noutput_1 &lt;- as_flextable(pct1)\nprint(output_1)\n\n\nThe rempsyc also provides a simple way of creating beautiful, export-ready tables. You can find the documentation here.\n\nlibrary(rempsyc)\n\n# Format table\noutput2 &lt;- nice_table(pct1_df)\n\nprint(output2)\n\n\n\n# Export to Microsoft Word\nprint(output2, preview = \"docx\")\n\n\n\n\n\nPaquot, Magali, and Tove Larsson. 2020. “Descriptive Statistics and Visualization with r.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Thomas Gries, 375–99. Cham: Springer.",
    "crumbs": [
      "Descriptive statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Describing categorical data</span>"
    ]
  }
]