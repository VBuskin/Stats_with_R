[
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Statistics and Data Analysis for Corpus Linguists: From Theory to Practice with R",
    "section": "Preface",
    "text": "Preface\nThis collection of handouts provides a hands-on introduction to data analysis and statistical methods in quantitative corpus linguistics with R. It is designed with accessibility in mind, assuming no prior knowledge of programming or statistics. All you need to get started is a laptop; everything else will be explained within these pages.\nPrimarily, this reader is geared towards students attending the classes Language Variation (BA) and Statistics for Linguistics (MA) at the Catholic University of Eichstätt-Ingolstadt (Germany). However, it is also meant to equip students currently working on their BA/MA/PhD theses with the tools they need to conduct empirical studies on a wide array of linguistic phenomena. The methods presented here reflect the state-of-the-art in corpus-linguistic research, providing readers with current and relevant analytical techniques.",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Statistics and Data Analysis for Corpus Linguists: From Theory to Practice with R",
    "section": "Overview",
    "text": "Overview\nWe begin by establishing the general structure of corpus-linguistic studies, accompanied by key theoretical and practical considerations. The second section introduces R as an analytical tool, covering its core functionality. With these fundamentals in place, we proceed to query corpora directly within R. Chapters four and five focus on describing discrete and continuous outputs, as well as identifying meaningful associations and differences in the data (BA). Finally, to gain more nuanced insights into potential patterns, we apply common machine learning algorithms to fit, evaluate, and interpret statistical models (MA).\nThroughout this journey, readers will develop the skills to conduct robust corpus-linguistic analyses, from basic querying to advanced statistical modeling.",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#collaborators",
    "href": "index.html#collaborators",
    "title": "Statistics and Data Analysis for Corpus Linguists: From Theory to Practice with R",
    "section": "Collaborators",
    "text": "Collaborators\nVladimir Buskin, PhD student at the Department of English Language and Linguistics, Catholic University of Eichstätt-Ingolstadt\nThomas Brunner, Assistant Professor at the Department of English Language and Linguistics, Catholic University of Eichstätt-Ingolstadt\nPhilippa Adolf, PhD student at the Department of Romance Studies, University of Vienna",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "Preface"
    ]
  },
  {
    "objectID": "Distributions.html#suggested-reading",
    "href": "Distributions.html#suggested-reading",
    "title": "17  Probability distributions",
    "section": "17.1 Suggested reading",
    "text": "17.1 Suggested reading\n\nBaguley (2012): Chapter 2\nAgresti and Kateri (2022): Chapter 2\nHeumann, Schomaker, and Shalabh (2022): Chapter 8",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "Distributions.html#continuous-distributions",
    "href": "Distributions.html#continuous-distributions",
    "title": "17  Probability distributions",
    "section": "17.2 Continuous distributions",
    "text": "17.2 Continuous distributions\n\n17.2.1 The normal distribution\nA great number of numerical variables in the world follow the well-known normal (or Gaussian) distribution, which includes test scores, weight and height, among many others. The plot below illustrates its characteristic bell-shape: Most observations are in the middle, with considerably fewer near the fringes. For example, most people are rather “average” in height; there are only few people that are extremely short or extremely tall.\n\n\n\n\n\n\n\n\n\nThe normal distribution is typically described in terms of two parameters: The population mean \\(\\mu\\) and the population standard deviation \\(\\sigma\\). If a random variable \\(X\\) is normally distributed, we typically use the notation in Equation 17.1.\n\\[ X \\sim N(\\mu, \\sigma^2).\n\\tag{17.1}\\]\nThese two parameters affect the shape of the probability density function (PDF) \\(f(x)\\), which is formally defined as\n\\[\nf(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}.\n\\tag{17.2}\\]\nIn practice, this function returns a bell curve:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick facts about the Gaussian bell curve\n\n\n\nQuite interestingly,\n\n68% all values fall within one standard deviation of the mean,\n95% within two, and\n99.7% within three.\n\n\n\nIn the plot, the \\(y\\)-axis indicates the density of population values; note that since the Gaussian distribution is a continuous distribution with technically infinite \\(x\\)-values, the probability of any given value must be 0. We can only obtain probabilities for intervals of values, which are given by\n\\[\nP(a \\leq X \\leq b) = \\int_a^b f(x)dx.\n\\tag{17.3}\\]\nWe can find the population mean of \\(X\\) with a PDF \\(f(x)\\) via\n\\[\nE(X) = \\mu = \\int_x xf(x)dx,\n\\tag{17.4}\\]\nwhere \\(E(X)\\) denotes the expected value of \\(X\\), i.e., the mean. Essentially, multiplying every value \\(x\\) by its respective probability density \\(f(x)\\) and integrating over all possible values of \\(x\\) will return \\(E(X) = \\mu\\).",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "Distributions.html#discrete-distributions",
    "href": "Distributions.html#discrete-distributions",
    "title": "17  Probability distributions",
    "section": "17.3 Discrete distributions",
    "text": "17.3 Discrete distributions\n\n17.3.1 Bernoulli distribution\nThe Bernoulli distribution is a discrete probability distribution for random variables which have only two possible outcomes: “positive” (often coded as 1) and “negative” (often coded as 0). Examples of such variables include coin tosses (heads/tails), binary response questions (yes/no), and defect status (defective/non-defective).\nIf a random variable \\(X\\) follows a Bernoulli distribution, it is determined by the parameter \\(p\\), which is the probability of the positive case:\n\\[ X \\sim Bernoulli(p).\\] The probability mass function (PMF) of the Bernoulli distribution is given by: \\[\nP(X = x) =\n\\begin{cases}\np & \\text{if } x = 1 \\\\\n1 - p & \\text{if } x = 0\n\\end{cases}\n\\]\nwhere \\(0 \\leq p \\leq 1\\). This function shows the probability of \\(X\\) taking on the value of 1 or 0 (cf. Heumann, Schomaker, and Shalabh 2022: 162-163).\n\n\n\n\n\n\n\n\n\n\n\n17.3.2 Binomial Distribution\nThe binomial distribution is a fairly straightforward extension of the Bernoulli distribution in that it models the number of successes in \\(n\\) independent Bernoulli trials, each with probability \\(p\\) of success. If a random variable \\(X\\) follows a binomial distribution with parameters \\(n\\) and \\(p\\), we write:\n\\[ X \\sim Binomial(n,p)\n\\tag{17.5}\\]\nThe probability mass function (PMF) for the binomial distribution is:\n\\[ P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\tag{17.6}\\]\nwhere:\n\n\\(n\\) is the number of trials\n\\(k\\) is the number of successes \\((0 \\leq k \\leq n)\\)\n\\(p\\) is the probability of success on each trial \\((0 \\leq p \\leq 1)\\)\n\\(\\binom{n}{k}\\) is the binomial coefficient (“n choose k”)\n\n\n\n\n\nAgresti, Alan, and Maria Kateri. 2022. Foundations of Statistics for Data Scientists: With r and Python. Boca Raton: CRC Press.\n\n\nBaguley, Thomas. 2012. Serious Stats: A Guide to Advanced Statistics for the Behavioral Sciences. Houndmills, Basingstoke: Palgrave Macmillan.\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "Distributions.html#sampling-distributions",
    "href": "Distributions.html#sampling-distributions",
    "title": "17  Probability distributions",
    "section": "17.4 Sampling Distributions",
    "text": "17.4 Sampling Distributions\nSampling distributions describe the distribution of statistics calculated from repeated random samples. Here are the most common ones:\n\n17.4.1 Chi-squared Distribution\nThe chi-squared distribution with \\(k\\) degrees of freedom, denoted \\(\\chi^2(k)\\), is the distribution of the sum of squares of \\(k\\) independent standard normal random variables. Its probability density function (PDF) is:\n\\[ f(x) = \\frac{x^{k/2-1}e^{-x/2}}{2^{k/2}\\Gamma(k/2)} \\]\nwhere:\n\n\\(x &gt; 0\\)\n\\(k\\) is the degrees of freedom\n\\(\\Gamma\\) is the gamma function\n\n\n\n17.4.2 Student’s t-Distribution\nThe Student’s t-distribution with \\(\\nu\\) degrees of freedom has the following PDF:\n\\[ f(x) = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\sqrt{\\nu\\pi}\\Gamma(\\frac{\\nu}{2})}\\left(1+\\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}} \\]\nwhere:\n\n\\(-\\infty &lt; x &lt; \\infty\\)\n\\(\\nu &gt; 0\\) is the degrees of freedom\n\\(\\Gamma\\) is the gamma function\n\nKey properties:\n\nAs \\(\\nu \\to \\infty\\), the t-distribution approaches the standard normal distribution\nThe distribution is symmetric about 0\nThe variance is \\(\\frac{\\nu}{\\nu-2}\\) for \\(\\nu &gt; 2\\)\n\n\n\n17.4.3 F-Distribution\nThe F-distribution with parameters \\(d_1\\) and \\(d_2\\) (degrees of freedom) has the following PDF:\n\\[ f(x) = \\frac{\\sqrt{\\frac{(d_1x)^{d_1}d_2^{d_2}}{(d_1x+d_2)^{d_1+d_2}}}}{x\\beta(\\frac{d_1}{2},\\frac{d_2}{2})} \\]\nwhere:\n\n\\(x &gt; 0\\)\n\\(d_1, d_2 &gt; 0\\) are the degrees of freedom\n\\(\\beta\\) is the beta function\n\nThe F-distribution is commonly used in:\n\nAnalysis of Variance (ANOVA)\nTesting equality of variances\nModel comparison in regression analysis\n\nEach of these sampling distributions plays a crucial role in statistical inference:\n\nChi-squared: Used for goodness-of-fit tests and tests of independence\nt-distribution: Used for inference about means when population variance is unknown\nF-distribution: Used for comparing variances and in ANOVA procedures\n\n\n\n\n\nAgresti, Alan, and Maria Kateri. 2022. Foundations of Statistics for Data Scientists: With r and Python. Boca Raton: CRC Press.\n\n\nBaguley, Thomas. 2012. Serious Stats: A Guide to Advanced Statistics for the Behavioral Sciences. Houndmills, Basingstoke: Palgrave Macmillan.\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "Categorical_data.html#preparation",
    "href": "Categorical_data.html#preparation",
    "title": "15  Categorical data",
    "section": "15.1 Preparation",
    "text": "15.1 Preparation\nPlease download the file Paquot_Larsson_2020_data.xlsx (Paquot and Larsson 2020)1 and store it in the same folder as your currently active R-script. Then run the code lines below:\n1 The original supplementary materials can be downloaded from the publisher’s website [Last accessed April 28, 2024].\n# Libraries\nlibrary(\"readxl\") # for loading Excel data\nlibrary(\"tidyverse\") # data manipulation and visualisation\n\n# Load data from working directory\ncl.order &lt;- read_xlsx(\"Paquot_Larsson_2020_data.xlsx\")",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Categorical data</span>"
    ]
  },
  {
    "objectID": "Categorical_data.html#frequency-tables",
    "href": "Categorical_data.html#frequency-tables",
    "title": "15  Categorical data",
    "section": "15.2 Frequency tables",
    "text": "15.2 Frequency tables\n\n15.2.1 One variable\nEach categorical variable in the data frame is made up of two or more categories. A simple descriptive measure is the frequency of each category. The table below indicates how often each clause order occurs in the ORDER column.\n\norder_freq1 &lt;- table(cl.order$ORDER) \n\nprint(order_freq1)\n\n\nmc-sc sc-mc \n  275   128 \n\n\nThe notation cl.order$ORDER subsets the cl.order according to the column ORDER (see data frames).\nAlternatively, you could use xtabs() to achieve the same result – perhaps with a slightly more intuitive syntax.\n\norder_freq2 &lt;- xtabs(~ ORDER, cl.order)\n\nprint(order_freq2)\n\nORDER\nmc-sc sc-mc \n  275   128 \n\n\n\n\n15.2.2 Two or more variables\nIf we are interested in the relationship between multiple categorical variables, we can cross-tabulate the frequencies of their categories. For example, what is the distribution of clause order depending on the type of subordinate clause? The output is also referred to as a contingency table.\n\norder_counts1 &lt;- table(cl.order$ORDER, cl.order$SUBORDTYPE)\n\nprint(order_counts1)\n\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n\n\nHere is the xtabs() alternative:\n\norder_counts2 &lt;- xtabs(~ ORDER + SUBORDTYPE, cl.order)\n\nprint(order_counts2)\n\n       SUBORDTYPE\nORDER   caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n\n\n\n\n\n\n\n\nHow do I obtain percentage tables?\n\n\n\n\n\nThere are two ways to convert the raw frequency counts to percentage tables:\n\nManually divide all cells by the total number of observations (which correspond to the sum of all cells) and multiply the result by 100.\n\n\npct1 &lt;- order_counts1/sum(order_counts1) * 100\n\n\nUse the prop.table() function and multiply the result by 100.\n\n\npct2 &lt;- prop.table(order_counts1) * 100",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Categorical data</span>"
    ]
  },
  {
    "objectID": "Categorical_data.html#plotting-categorical-data",
    "href": "Categorical_data.html#plotting-categorical-data",
    "title": "15  Categorical data",
    "section": "15.3 Plotting categorical data",
    "text": "15.3 Plotting categorical data\nThis section demonstrates both the in-built plotting functions of R (‘Base R’) as well as the more modern versions provided by the tidyverse package.\n\n15.3.1 Moscaiplot\nA straightforward way to visualise a contingency table is the mosaicplot:\n\nmosaicplot(order_counts2, color = TRUE)\n\n\n\n\n\n\n\n\n\n\n15.3.2 Barplots\nThe workhorse of categorical data analysis is the barplot. Base R functions usually require a table object as input, whereas ggplot2 can operate on the raw dataset.\n\n\n15.3.3 One variable\n\nBase Rggplot2\n\n\n\nBase R barplot with barplot(); requires the counts as computed by tables() or xtabs()\n\n\n# Generate cross-table\norder_freq1 &lt;- table(cl.order$ORDER)\n\n# Create barplot\nbarplot(order_freq1)\n\n\n\n\n\n\n\n\n\n\n\nBarplot with geom_bar() using the raw input data\n\n\n# Load library  (if not already loaded)\nlibrary(tidyverse)\n\n# Create barplot\nggplot(cl.order, aes(x = ORDER)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.3.4 Two variables\nBivariate barplots can be obtained by either supplying a contingency table (Base R) or by mapping the second variable onto the fill argument using the raw data.\n\nBase RBase R (fully customised)ggplot2ggplot2 (fully customised)\n\n\n\n# Generate cross-table\norder_counts2 &lt;- xtabs(~ ORDER + SUBORDTYPE, cl.order)\n\n# Create barplot\nbarplot(order_counts2, \n        beside = TRUE,  # Make bars side-by-side\n        legend = TRUE)  # Add a legend\n\n\n\n\n\n\n\n\n\n\n\n# Generate cross-table\norder_counts2 &lt;- xtabs(~ ORDER + SUBORDTYPE, cl.order)\n\n# Create barplot\nbarplot(order_counts2, \n        beside = TRUE,  # Make bars dodged (i.e., side by side)\n        main = \"Distribution of ORDER by SUBORDTYPE (Base R)\", \n        xlab = \"ORDER\", \n        ylab = \"Frequency\", \n        col = c(\"lightblue\", \"lightgreen\"), # Customize colors\n        legend = TRUE,  # Add a legend\n        args.legend = list(title = \"SUBORDTYPE\", x = \"topright\"))\n\n\n\n\n\n\n\n\n\n\n\n# Load library (if not already loaded)\nlibrary(tidyverse)\n\n# Create barplot\nggplot(cl.order, aes(x = ORDER, fill = SUBORDTYPE)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\n# Load library (if not already loaded)\nlibrary(tidyverse)\n\nggplot(cl.order, aes(x = ORDER, fill = SUBORDTYPE)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    title = \"Clause order by subordinate clause type\",\n    x = \"Clause order\",\n    y = \"Frequency\",\n    fill = \"Type of subordinate clause\"\n  ) +\n  theme_bw()",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Categorical data</span>"
    ]
  },
  {
    "objectID": "Categorical_data.html#plotting-percentages",
    "href": "Categorical_data.html#plotting-percentages",
    "title": "15  Categorical data",
    "section": "15.4 Plotting percentages",
    "text": "15.4 Plotting percentages\n\n\n\n\n\n\nHow do I plot percentages in Base R?\n\n\n\n\n\nIn very much the same way as with the raw counts:\n\n# Create barplot\nbarplot(pct1, \n        beside = TRUE,  # Make bars side-by-side\n        legend = TRUE)  # Add a legend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow do I plot percentages with ggplot2?\n\n\n\n\n\nHere, a few tweaks are necessary. In general, the ggplot() function prefers to works with data frames rather than cross-tables, so we’ll have to coerce it into one first:\n\n# Convert pct1 to a data frame\npct1_df &lt;- as.data.frame(pct1)\n\ncolnames(pct1_df) &lt;- c(\"ORDER\", \"SUBORDTYPE\", \"pct\")\n\n# Print contents of the new data frame \"pct1_df\"\nprint(pct1_df)\n\n  ORDER SUBORDTYPE       pct\n1 mc-sc       caus 45.657568\n2 sc-mc       caus  3.722084\n3 mc-sc       temp 22.580645\n4 sc-mc       temp 28.039702\n\n\nNow we can plot the percentages with geom_col(). This geom (= ‘geometric object’) allows us to manually specify what should be mapped onto the y-axis:\n\n# Load library  (if not already loaded)\nlibrary(tidyverse)\n\n# Create barplot with user-defined y-axis (requires geom_col())\nggplot(pct1_df, aes(x = ORDER, y = pct, fill = SUBORDTYPE)) +\n  geom_col(position = \"dodge\")",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Categorical data</span>"
    ]
  },
  {
    "objectID": "Categorical_data.html#exporting-tables-to-ms-word",
    "href": "Categorical_data.html#exporting-tables-to-ms-word",
    "title": "15  Categorical data",
    "section": "15.6 Exporting tables to MS Word",
    "text": "15.6 Exporting tables to MS Word\nThe flextable package greatly facilitates generating publication-ready tables in R. It provides the best output if used on a table created by xtabs(). The full guide to this package can be found here.\n\n\nShow the code\n# Load library after installation\nlibrary(flextable)\n\n# Create a table\ntab1 &lt;- xtabs(~ ORDER + SUBORDTYPE, cl.order)\n\n# Convert a table to a flextable with as_flextable()\noutput_1 &lt;- as_flextable(tab1)\n\n# Print output\nprint(output_1)\n\n\n\nUnfortunately, the output cannot really be customised. However, if you’d like to add some further options, the crosstable package provides a remedy. In fact, it is even easier to use as it doesn’t require you to compute any tables beforehand.\n\n\nShow the code\n# Required libraries\nlibrary(crosstable)\nlibrary(flextable)\n\n# Create the cross table\noutput2 &lt;- crosstable(cl.order, ORDER, by = SUBORDTYPE, \n                 total = \"both\",\n                 percent_digits = 2)\n\nas_flextable(output2)\n\n\nlabelvariableSUBORDTYPETotalcaustempORDERmc-sc184 (66.91%)91 (33.09%)275 (68.24%)sc-mc15 (11.72%)113 (88.28%)128 (31.76%)Total199 (49.38%)204 (50.62%)403 (100.00%)\n\n\n\n\n\n\nPaquot, Magali, and Tove Larsson. 2020. “Descriptive Statistics and Visualization with r.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Thomas Gries, 375–99. Cham: Springer.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Categorical data</span>"
    ]
  },
  {
    "objectID": "Concordancing.html#suggested-reading",
    "href": "Concordancing.html#suggested-reading",
    "title": "11  Concordancing",
    "section": "11.1 Suggested reading",
    "text": "11.1 Suggested reading\nIn-depth introduction to concordancing with R:\n\nSchweinberger (2024)\n\nNaturale Language Processing (NLP) with quanteda:\n\nBenoit et al. (2018)\nOnline reference\n\nOn corpus-linguistic theory:\n\nWulff and Baker (2020)\nLange and Leuckert (2020)\nMcEnery, Xiao, and Yukio (2006)",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Concordancing</span>"
    ]
  },
  {
    "objectID": "Concordancing.html#preparation",
    "href": "Concordancing.html#preparation",
    "title": "11  Concordancing",
    "section": "11.2 Preparation",
    "text": "11.2 Preparation\n\n\n\n\n\n\nScript\n\n\n\nYou can find the full R script associated with this unit here.\n\n\n\n\n\n\n\n\nWorking directory\n\n\n\nIn order for R to be able to recognise the data, it is crucial to set up the working directory correctly.\n\nMake sure your R-script and the corpus (e.g., ‘ICE-GB’) are stored in the same folder on your computer.\nIn RStudio, go to the Files pane (usually in the bottom-right corner) and navigate to the location of your script. Alternatively, you can click on the three dots ... and use the file browser instead.\nOnce you’re in the correct folder, click on the blue ⚙️ icon.\nSelect Set As Working Directory. This action will update your working directory to the folder where the file is located.\n\n\n\nIn addition, make sure you have installed quanteda. Load it at the beginning of your script:\n\nlibrary(quanteda) # Package for Natural Language Processing in R\nlibrary(lattice) # for dotplots\n\nTo load a corpus object into R, place it in your working directory and read it into your working environment with readRDS().1\n1 The ICE-GB.RDS file you’ve been provided with has been pre-processed and saved in this specific format for practical reasons.\n# Load corpus from directory\nICE_GB &lt;- readRDS(\"ICE_GB.RDS\")\n\nIf you encounter any error messages at this stage, ensure you followed steps 1 and 2 in the callout box above.",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Concordancing</span>"
    ]
  },
  {
    "objectID": "Concordancing.html#concordancing",
    "href": "Concordancing.html#concordancing",
    "title": "11  Concordancing",
    "section": "11.3 Concordancing",
    "text": "11.3 Concordancing\nA core task in corpus-linguistic research involves finding occurrences of a single word or multi-word sequence in the corpus. Lange & Leuckert (2020: 55) explain that specialised software typically “provide[s] the surrounding context as well as the name of the file in which the word could be identified.” Inspecting the context is particularly important in comparative research, as it may be indicative of distinct usage patterns.\n\n11.3.1 Simple queries\nTo obtain such a keyword in context (KWIC) in R, we use the kwic() function. We supply the corpus as well as the keyword we’re interested in:\n\nquery1 &lt;- kwic(ICE_GB, \"eat\")\n\nThe output in query1 contains concordance lines that list all occurrences of the keyword, including the document, context to the left, the keyword itself, and the context to the right. The final column reiterates our search expression.\n\nhead(query1)\n\nKeyword-in-context with 6 matches.                                                            \n  [ICE_GB/S1A-006.txt, 785]           So I' d rather | eat |\n [ICE_GB/S1A-009.txt, 1198]              I must &lt;, &gt; | eat |\n  [ICE_GB/S1A-010.txt, 958]         to &lt;, &gt; actually | eat |\n  [ICE_GB/S1A-018.txt, 455] order one first and then | eat |\n  [ICE_GB/S1A-018.txt, 498]  A &gt; The bargain hunting | eat |\n [ICE_GB/S1A-023.txt, 1853]       B &gt; Oh name please | eat |\n                            \n beforehand just to avoid uh\n them &lt; ICE-GB:S1A-009#71:  \n it for one' s              \n it and then sort of        \n &lt; ICE-GB:S1A-018#29: 1     \n something &lt;,, &gt;            \n\n\nFor a full screen display of the KWIC data frame, try View():\n\nView(query1)\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-006.txt\n785\n785\nSo I ' d rather\neat\nbeforehand just to avoid uh\neat\n\n\nICE_GB/S1A-009.txt\n1198\n1198\nI must &lt; , &gt;\neat\nthem &lt; ICE-GB:S1A-009 #71 :\neat\n\n\nICE_GB/S1A-010.txt\n958\n958\nto &lt; , &gt; actually\neat\nit for one ' s\neat\n\n\nICE_GB/S1A-018.txt\n455\n455\norder one first and then\neat\nit and then sort of\neat\n\n\nICE_GB/S1A-018.txt\n498\n498\nA &gt; The bargain hunting\neat\n&lt; ICE-GB:S1A-018 #29 : 1\neat\n\n\nICE_GB/S1A-023.txt\n1853\n1853\nB &gt; Oh name please\neat\nsomething &lt; , , &gt;\neat\n\n\n\n\n\n\n\n\n\n11.3.2 Multi-word queries\nIf the search expression exceeds a single word, we need to mark it as a multi-word sequence by means of the phrase() function. For instance, if we were interested in the pattern eat a, we’d have to adjust the code as follows:\n\nquery2 &lt;- kwic(ICE_GB, phrase(\"eat a\"))\n\n\nView(query2)\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-059.txt\n2230\n2231\n1 : B &gt; I\neat a\n&lt; , &gt; very balanced\neat a\n\n\nICE_GB/W2B-014.txt\n1045\n1046\n: 1 &gt; We can't\neat a\nlot of Welsh or Scottish\neat a\n\n\nICE_GB/W2B-022.txt\n589\n590\nhave few labour-saving devices ,\neat a\ndiet low in protein ,\neat a\n\n\n\n\n\n\n\n\n\n11.3.3 Multiple simultaneous queries\nA very powerful advantage of quanteda over traditional corpus software is that we can query a corpus for a multitude of keywords at the same time. Say, we need our output to contain hits for eat, drink as well as sleep. Instead of a single keyword, we supply a character vector containing the strings of interest.\n\nquery3 &lt;- kwic(ICE_GB, c(\"eat\", \"drink\", \"sleep\"))\n\n\nView(query3)\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-006.txt\n785\n785\nSo I ' d rather\neat\nbeforehand just to avoid uh\neat\n\n\nICE_GB/S1A-009.txt\n869\n869\n: A &gt; Do you\ndrink\nquite a lot of it\ndrink\n\n\nICE_GB/S1A-009.txt\n1198\n1198\nI must &lt; , &gt;\neat\nthem &lt; ICE-GB:S1A-009 #71 :\neat\n\n\nICE_GB/S1A-010.txt\n958\n958\nto &lt; , &gt; actually\neat\nit for one ' s\neat\n\n\nICE_GB/S1A-014.txt\n3262\n3262\nyou were advised not to\ndrink\nwater in Leningrad because they\ndrink\n\n\nICE_GB/S1A-016.txt\n3290\n3290\n&gt; I couldn't I couldn't\nsleep\nif I didn't read &lt;\nsleep\n\n\n\n\n\n\n\n\n\n11.3.4 Window size\nSome studies require more detailed examination of the preceding or following context of the keyword. We can easily adjust the window size to suit our needs:\n\nquery4 &lt;- kwic(ICE_GB, \"eat\", window = 20) \n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-006.txt\n785\n785\n#49 : 1 : A &gt; Yeah &lt; ICE-GB:S1A-006 #50 : 1 : A &gt; So I ' d rather\neat\nbeforehand just to avoid uh &lt; , , &gt; any problems there &lt; ICE-GB:S1A-006 #51 : 1 : B &gt;\neat\n\n\nICE_GB/S1A-009.txt\n1198\n1198\n&lt; , &gt; in in the summer &lt; ICE-GB:S1A-009 #70 : 1 : A &gt; I must &lt; , &gt;\neat\nthem &lt; ICE-GB:S1A-009 #71 : 1 : A &gt; Yes &lt; ICE-GB:S1A-009 #72 : 1 : B &gt; You ought\neat\n\n\nICE_GB/S1A-010.txt\n958\n958\n1 : B &gt; You know I mean it would seem to be squandering it to &lt; , &gt; actually\neat\nit for one ' s own enjoyment &lt; , , &gt; &lt; ICE-GB:S1A-010 #49 : 1 : A &gt; Mm\neat\n\n\nICE_GB/S1A-018.txt\n455\n455\ns so &lt; ICE-GB:S1A-018 #27 : 1 : A &gt; What you should do is order one first and then\neat\nit and then sort of carry on from there &lt; laughter &gt; &lt; , &gt; by which time you wouldn't\neat\n\n\nICE_GB/S1A-018.txt\n498\n498\nsecond anyway so &lt; laugh &gt; &lt; , &gt; &lt; ICE-GB:S1A-018 #28 : 1 : A &gt; The bargain hunting\neat\n&lt; ICE-GB:S1A-018 #29 : 1 : B &gt; So all right what did I have &lt; ICE-GB:S1A-018 #30 : 1\neat\n\n\nICE_GB/S1A-023.txt\n1853\n1853\n&gt; I can't bear it &lt; , , &gt; &lt; ICE-GB:S1A-023 #121 : 1 : B &gt; Oh name please\neat\nsomething &lt; , , &gt; &lt; ICE-GB:S1A-023 #122 : 1 : A &gt; Oh actually Dad asked me if &lt;\neat\n\n\n\n\n\n\n\n\n\n11.3.5 Saving your output\nYou can store your results in a spreadsheet file just as described in the unit on importing and exporting data.\n\nMicrosoft Excel (.xlsx)\n\n\nlibrary(writexl) # required for writing files to MS Excel\n\nwrite_xlsx(query1, \"myresults1.xlsx\")\n\n\nLibreOffice (.csv)\n\n\nwrite.csv(query1, \"myresults1.csv\")\n\nAs soon as you have annotated your data, you can load .xlsx files back into R with read_xlsx() from the readxl package and .csv files using the Base R function read.csv().",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Concordancing</span>"
    ]
  },
  {
    "objectID": "Concordancing.html#characterising-the-output",
    "href": "Concordancing.html#characterising-the-output",
    "title": "11  Concordancing",
    "section": "11.4 Characterising the output",
    "text": "11.4 Characterising the output\nRecall our initial query of the eat, whose output we stored in query1:\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-006.txt\n785\n785\nSo I ' d rather\neat\nbeforehand just to avoid uh\neat\n\n\nICE_GB/S1A-009.txt\n1198\n1198\nI must &lt; , &gt;\neat\nthem &lt; ICE-GB:S1A-009 #71 :\neat\n\n\nICE_GB/S1A-010.txt\n958\n958\nto &lt; , &gt; actually\neat\nit for one ' s\neat\n\n\nICE_GB/S1A-018.txt\n455\n455\norder one first and then\neat\nit and then sort of\neat\n\n\nICE_GB/S1A-018.txt\n498\n498\nA &gt; The bargain hunting\neat\n&lt; ICE-GB:S1A-018 #29 : 1\neat\n\n\nICE_GB/S1A-023.txt\n1853\n1853\nB &gt; Oh name please\neat\nsomething &lt; , , &gt;\neat\n\n\n\n\n\n\n\nFirst, we may be interested in obtaining some general information on our results, such as …\n\n… how many tokens (= individual hits) does the query return?\n\nThe nrow() function counts the number of rows in a data frame — these always correspond to the number of observations in our sample (here: 53).\n\nnrow(query1)\n\n[1] 53\n\n\n\n… how many types (= distinct hits) does the query return?\n\nApparently, there are 52 counts of eat in lower case and 1 in upper case. Their sum corresponds to our 53 observations in total.\n\ntable(query1$keyword)\n\n\neat Eat \n 52   1 \n\n\n\n… how is the keyword distributed across corpus files?\n\nThis question relates to the notion of dispersion: Is a keyword spread relatively evenly across corpus files or does it only occur in specific ones?\n\n# Frequency of keyword by docname\nquery_distrib &lt;- table(query1$docname, query1$keyword)\n\n# Show first few rows\nhead(query_distrib)\n\n                    \n                     eat Eat\n  ICE_GB/S1A-006.txt   1   0\n  ICE_GB/S1A-009.txt   1   0\n  ICE_GB/S1A-010.txt   1   0\n  ICE_GB/S1A-018.txt   2   0\n  ICE_GB/S1A-023.txt   1   0\n  ICE_GB/S1A-025.txt   1   0\n\n# Create a simple dot plot\ndotplot(query_distrib, auto.key = list(columns = 2, title = \"Tokens\", cex.title = 1))\n\n\n\n\n\n\n\n# Create a fancy plot (requires tidyverse)\nggplot(query1, aes(x = keyword)) + \n  geom_bar() +\n  facet_wrap(~docname)\n\n\n\n\n\n\n\n\nIt seems that eat occurs at least once in most text categories (both spoken and written), but seems to be much more common in face-to-face conversations (S1A). This is not surprising: It is certainly more common to discuss food in a casual chat with friends than in an academic essay (unless, of course, its main subject matter is food). Dispersion measures can thus be viewed as indicators of contextual preferences associated with lexemes or more grammatical patterns.\n\n\n\n\n\n\nAdvanced: More on dispersion\n\n\n\n\n\nThe empirical study of dispersion has attracted a lot of attention in recent years Gries (2020). A reason for this is the necessity of finding a dispersion measure that is minimally correlated with token frequency. One such measure is the Kullback-Leibler divergence \\(KLD\\), which comes from the field of information theory and is closely related to entropy.\nMathematically, \\(KLD\\) measures the difference between two probability distributions \\(p\\) and \\(q\\).\n\\[ KLD(p \\parallel q) = \\sum\\limits_{x \\in X} p(x) \\log \\frac{p(x)}{q(x)}\n\\tag{11.1}\\]\nLet \\(f\\) denote the overall frequency of a keyword in the corpus, \\(v\\) its frequency in each corpus part, \\(s\\) the sizes of each corpus part (as fractions) and \\(n\\) the total number of corpus parts. We thus compare the posterior (= “actual”) distribution of keywords \\(\\frac{v_i}{f}\\) for \\(i = 1, ..., n\\) with their prior distribution, which assumes all words are spread evenly across corpus parts (hence the division by \\(s_i\\)).\n\\[ KLD = \\sum\\limits_{i=1}^n \\frac{v_i}{f} \\times \\log_2\\left({\\frac{v_i}{f} \\times \\frac{1}{s_i}}\\right)\n\\tag{11.2}\\]\nIn R, let’s calculate the dispersion of the verbs eat, drink, and sleep from query3.\n\n# Let's filter out the upper-case variants:\nquery3_reduced &lt;- query3[query3$keyword %in% c(\"eat\", \"drink\", \"sleep\"),]\ntable(query3_reduced$keyword)\n\n\ndrink   eat sleep \n   48    52    41 \n\n# Extract text categories\nquery_registers &lt;- separate_wider_delim(query3_reduced, cols = docname, delim = \"-\", names = c(\"Text_category\", \"File_number\"))\n\n# Get separate data frames for each verb\neat &lt;- filter(query_registers, keyword == \"eat\")\ndrink &lt;- filter(query_registers, keyword == \"drink\")\nsleep &lt;- filter(query_registers, keyword == \"sleep\")\n\n## Get frequency distribution across files\nv_eat &lt;- table(eat$Text_category)\nv_drink &lt;- table(drink$Text_category)\nv_sleep &lt;- table(sleep$Text_category)\n\n## Get total frequencies\nf_eat &lt;- nrow(eat)\nf_drink &lt;- nrow(drink)\nf_sleep &lt;- nrow(sleep)\n\n# The next step is a little trickier. First we need to find out how many distinct corpus parts there are in the ICE corpus.\n\n## Check ICE-corpus structure and convert to data frame\nICE_GB_str &lt;- as.data.frame(summary(ICE_GB))\n\n## Separate files from text categores\nICE_GB_texts &lt;- separate_wider_delim(ICE_GB_str, cols = Var1, delim = \"-\", names = c(\"Text_category\", \"File\"))\n\n## Get number of distinct text categories\nn &lt;- length(unique(ICE_GB_texts$Text_category))\n\n## Get proportions of distinct text categories (s)\ns &lt;- table(ICE_GB_texts$Text_category)/sum(table(ICE_GB_texts$Text_category))\n\n## Unfortunately not all of these corpus parts are represented in our queries. We need to correct the proportions in s for the missing ones!\n\n## Store unique ICE text categories \nICE_unique_texts &lt;- unique(ICE_GB_texts$Text_category)\n\n## Make sure only those text proportions are included where the keywords actually occur\ns_eat &lt;- s[match(names(v_eat), ICE_unique_texts)]\ns_drink &lt;- s[match(names(v_drink), ICE_unique_texts)]\ns_sleep &lt;- s[match(names(v_sleep), ICE_unique_texts)]\n\n# Compute KLD for each verb\nkld_eat &lt;- sum(v_eat/f_eat * log2(v_eat/f_eat * 1/s_eat)); kld_eat\n\n[1] 0.6747268\n\nkld_drink &lt;- sum(v_drink/f_drink * log2(v_drink/f_drink * 1/s_drink)); kld_drink\n\n[1] 0.8463608\n\nkld_sleep &lt;- sum(v_sleep/f_sleep * log2(v_sleep/f_sleep * 1/s_sleep)); kld_sleep\n\n[1] 0.7047421\n\n# Plot\nkld_df &lt;- data.frame(kld_eat, kld_drink, kld_sleep)\n\nbarplot(as.numeric(kld_df), names.arg = names(kld_df), col = \"steelblue\",\n        xlab = \"Variable\", ylab = \"KLD Value (= deviance from even distribution)\", main = \"Dispersion of 'eat', 'drink', and 'sleep'\")\n\n\n\n\n\n\n\n\nThe plot indicates that drink is the most unevenly distributed verb out of the three considered (high KDL \\(\\sim\\) low dispersion), whereas eat appears to be slightly more evenly distributed across corpus files. The verb sleep assumes an intermediary position.",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Concordancing</span>"
    ]
  },
  {
    "objectID": "Concordancing.html#i-need-a-proper-user-interface-some-alternatives",
    "href": "Concordancing.html#i-need-a-proper-user-interface-some-alternatives",
    "title": "11  Concordancing",
    "section": "11.5 “I need a proper user interface”: Some alternatives",
    "text": "11.5 “I need a proper user interface”: Some alternatives\nThere is a wide variety of concordancing software available, both free and paid. Among the most popular options are AntConc (Anthony 2020) and SketchEngine (Kilgarriff et al. 2004). However, as Schweinberger (2024) notes, the exact processes these tools use to generate output are not always fully transparent, making them something of a “black box.” In contrast, programming languages like R or Python allow researchers to document each step of their analysis clearly, providing full transparency from start to finish.\nThe following apps attempt to reconcile the need for an intuitive user interface with transparent data handling. The full source code is documented in the respective GitHub repositories.\n\nQuantedaApp is an interface for the R package quanteda (Benoit et al. 2018).\nPyConc is an interface for the Python package spaCy (Honnibal and Montani 2017).\n\n\n\n\n\nAnthony, Lawrence. 2020. AntConc (Version 3.5.9). Tokyo, Japan: Waseda University. https://www.laurenceanthony.net/software.\n\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Müller, and Akitaka Matsuo. 2018. “Quanteda: An r Package for the Quantitative Analysis of Textual Data.” Journal of Open Source Software 3 (30): 774. https://doi.org/10.21105/joss.00774.\n\n\nGries, Stefan Thomas. 2020. “Analyzing Dispersion.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Thomas Gries, 99–118. Cham: Springer.\n\n\nHonnibal, Matthew, and Ines Montani. 2017. “spaCy 2: Natural Language Understanding with Bloom Embeddings, Convolutional Neural Networks and Incremental Parsing.”\n\n\nKilgarriff, Adam, Pavel Rychly, Pavel Smrz, and David Tugwell. 2004. “ITRI-04-08 the Sketch Engine.” Information Technology 105: 116.\n\n\nLange, Claudia, and Sven Leuckert. 2020. Corpus Linguistics for World Englishes: A Guide for Research. New York: Taylor; Francis.\n\n\nMcEnery, Tony, Richard Xiao, and Tono Yukio. 2006. Corpus-Based Language Studies: An Advanced Resource Book. London: Routledge.\n\n\nSchweinberger, Martin. 2024. Concordancing with r. 2024.05.07 ed. Brisbane: The Language Technology; Data Analysis Laboratory (LADAL). https://ladal.edu.au/kwics.html.\n\n\nSönning, Lukas. 2024. “Evaluation of Keyness Metrics: Performance and Reliability,” Corpus Linguistics and Linguistic Theory, 20 (2): 263–88. https://doi.org/10.1515/cllt-2022-0116.\n\n\nWulff, Stefanie, and Paul Baker. 2020. “Analyzing Concordances.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Th. Gries, 161–79. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-46216-1_8.",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Concordancing</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "30  References",
    "section": "",
    "text": "References\n\n\nAgresti, Alan, and Maria Kateri. 2022. Foundations of Statistics for\nData Scientists: With r and Python. Boca Raton: CRC\nPress.\n\n\nAnthony, Lawrence. 2020. AntConc (Version 3.5.9).\nTokyo, Japan: Waseda University. https://www.laurenceanthony.net/software.\n\n\nBaayen, R. Harald, and Maja Linke. 2020. “Generalized\nAdditive Mixed Models.” In\nA Practical Handbook of\nCorpus Linguistics, edited by Magali\nPaquot and Stefan Thomas Gries, 563–91. Cham: Springer.\n\n\nBaguley, Thomas. 2012. Serious Stats: A\nGuide to Advanced Statistics for\nthe Behavioral Sciences. Houndmills,\nBasingstoke: Palgrave Macmillan.\n\n\nBalota, David A., Melvin J. Yap, Keith A. Hutchison, Michael J. Cortese,\nBrett Kessler, Bjorn Loftis, James H. Neely, Douglas L. Nelson, Greg B.\nSimpson, and Rebecca Treiman. 2007. “The English\nLexicon Project.” Behavior Research\nMethods 39 (3): 445–59. https://doi.org/10.3758/BF03193014.\n\n\nBarbieri, Federica. 2007. “Older Men and Younger Women: A\nCorpus-Based Study of Quotative Use in American English.”\nEnglish World-Wide 28 (1): 23–45. https://doi.org/10.1075/eww.28.1.03bar.\n\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng,\nStefan Müller, and Akitaka Matsuo. 2018. “Quanteda: An r Package\nfor the Quantitative Analysis of Textual Data.” Journal of\nOpen Source Software 3 (30): 774. https://doi.org/10.21105/joss.00774.\n\n\nBooth, Wayne C., Gregory G. Colomb, and Joseph M. Williams. 2008.\nThe Craft of Research. 3rd ed. Chicago: The University of\nChicago Press.\n\n\nBreiman, Leo. 1984. Classification and Regression Trees.\nBelmont, Calif.: Wadsworth International Group.\n\n\n———. 2001. “Random Forests.” Machine Learning 45\n(1): 5–32. https://doi.org/10.1023/A:1010933404324.\n\n\nBuskin, Vladimir. n.d. “Definite Null Instantiation\nin English(es): A Usage-based\nConstruction Grammar Approach.” Constructions and\nFrames.\n\n\nCiaccio, Laura Anna, and João Veríssimo. 2022. “Investigating\nVariability in Morphological Processing with Bayesian\nDistributional Models.” Psychonomic Bulletin &\nReview 29 (6): 2264–74. https://doi.org/10.3758/s13423-022-02109-w.\n\n\nDebeer, Dries, and Carolin Strobl. 2020. “Conditional Permutation\nImportance Revisited.” Bioinformatics 21 (1): 307.\n\n\nGao, Chuanji, Svetlana V. Shinkareva, and Rutvik H. Desai. 2022.\n“SCOPE: The South\nCarolina Psycholinguistic Metabase.” Behavior\nResearch Methods 55 (6): 2853–84. https://doi.org/10.3758/s13428-022-01934-0.\n\n\nGardner, Matt Hunt et al. 2021. “Variation Isn’t That Hard:\nMorphosyntactic Choice Does Not Predict Production Difficulty.”\nPloS One 16 (6): e0252602–2.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis\nUsing Regression and\nMultilevel/Hierarchical Models.\nAnalytical Methods for Social Research. Cambridge: Cambridge University\nPress.\n\n\nGlass, Lelia. 2021. “English Verbs Can Omit Their Objects When\nThey Describe Routines.” English Language and\nLinguistics 26 (1): 49–73. https://doi.org/10.1017/S1360674321000022.\n\n\nGoldberg, Adele E. 2001. “Patient Arguments of Causative Verbs Can\nBe Omitted: The Role of Information Structure in Argument\nDistribution.” Language Sciences 23 (4): 503–24.\n\n\nGreenwell, Brandon. 2022. Tree-Based Methods for Statistical\nLearning in r. London & New York: Taylor & Francis Group.\nhttps://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=3288358.\n\n\nGreenwell, Brandon M. 2017. “Pdp: An r Package for Constructing\nPartial Dependence Plots.” The R Journal 9 (1): 421–36.\n\n\nGries, Stefan Thomas. 2013. Statistics for Linguistics with r: A\nPractical Introduction. 2nd rev. ed. Berlin; Boston: De Gruyter\nMouton.\n\n\n———. 2021. Statistics for Linguistics with r: A Practical\nIntroduction. 3rd rev. ed. Berlin; Boston: De Gruyter Mouton.\n\n\nGries, Stefan, and Sandra Deshors. 2014. “Using Regressions to\nExplore Deviations Between Interlanguage and Native Language.”\nCorpora 9: 109–36.\n\n\nHastie, Trevor, and Robert Tibshirani. 1991. Generalized\nAdditive Models. London: Chapman &\nHall.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome H. Friedman. 2017. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction. 2nd ed. New York, NY: Springer.\n\n\nHazen, Kirk. 2015. An Introduction to Language. Chichester:\nWiley Blackwell.\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022.\nIntroduction to Statistics and Data Analysis: With Exercises,\nSolutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nHonnibal, Matthew, and Ines Montani. 2017. “spaCy 2: Natural Language Understanding with\nBloom Embeddings, Convolutional Neural Networks and\nIncremental Parsing.”\n\n\nHosmer, David W., and Stanley Lemeshow. 2008. Applied Logistic\nRegression. 2nd ed. New York: Wiley.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning: With Applications in\nr. New York: Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nKiesling, Scott F. 2011. Linguistic Variation and Change.\nEdinburgh: Edinburgh University Press.\n\n\nKilgarriff, Adam, Pavel Rychly, Pavel Smrz, and David Tugwell. 2004.\n“ITRI-04-08 the Sketch Engine.” Information\nTechnology 105: 116.\n\n\nLabov, William. 1972. Sociolinguistic Patterns. Philadelphia:\nUniversity of Pennsylvania Press.\n\n\nLange, Claudia, and Sven Leuckert. 2020. Corpus Linguistics for\nWorld Englishes: A Guide for Research. New York: Taylor; Francis.\n\n\nLevshina, Natalia. 2015. How to Do Linguistics with r: Data\nExploration and Statistical Analysis. Amsterdam; Philadelphia: John\nBenjamins Publishing Company.\n\n\n———. 2020. “Conditional Inference Trees and Random\nForests.” In A Practical Handbook of Corpus Linguistics,\nedited by Magali Paquot and Stefan Thomas Gries, 611–43. Cham: Springer.\n\n\nMair, Patrick. 2018. Modern Psychometrics with\nR. Cham: Springer. https://doi.org/10.1007/978-3-319-93177-7.\n\n\nMcEnery, Tony, Richard Xiao, and Tono Yukio. 2006. Corpus-Based\nLanguage Studies: An Advanced Resource Book. London: Routledge.\n\n\nMesthrie, Rajend. 2011. Introducing Sociolinguistics. 2nd ed.\nEdinburgh: Edinburgh University Press.\n\n\nMeyerhoff, Miriam. 2009. Introducing Sociolinguistics. London:\nRoutledge.\n\n\nO’Connell, Ann A. 2006. Logistic Regression\nModels for Ordinal Response\nVariables. Vol. 146. Thousand Oaks, Calif.: Sage.\n\n\nPaquot, Magali, and Tove Larsson. 2020. “Descriptive Statistics\nand Visualization with r.” In A Practical Handbook of Corpus\nLinguistics, edited by Magali Paquot and Stefan Thomas Gries,\n375–99. Cham: Springer.\n\n\nPowers, Daniel A., and Yu Xie. 2008. Statistical\nMethods for Categorical Data\nAnalysis. 2. ed. Bingley: Emerald.\n\n\nResnik, Philip. 1996. “Selectional Constraints: An\nInformation-Theoretic Model and Its Computational Realization” 61\n(1): 127–59.\n\n\nSchäfer, Roland. 2020. “Mixed-Effects Regression Modeling.”\nIn A Practical Handbook of Corpus Linguistics, edited by Magali\nPaquot and Stefan Thomas Gries, 535–61. Cham: Springer.\n\n\nSchweinberger, Martin. 2024. Concordancing with r. 2024.05.07\ned. Brisbane: The Language Technology; Data Analysis Laboratory (LADAL).\nhttps://ladal.edu.au/kwics.html.\n\n\nSönning, Lukas. 2024. “Evaluation of Keyness Metrics: Performance\nand Reliability,” Corpus Linguistics and\nLinguistic Theory, 20 (2): 263–88. https://doi.org/10.1515/cllt-2022-0116.\n\n\nStrobl, Carolin, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin,\nand Achim Zeileis. 2008. “Conditional Variable Importance for\nRandom Forests.” BMC Bioinformatics 9 (1): 307.\n\n\nStrobl, Carolin, Anne-Laure Boulesteix, Achim Zeileis, and Torsten\nHothorn. 2007. “Bias in Random Forest Variable Importance\nMeasures: Illustrations, Sources and a Solution.” BMC\nBioinformatics 8 (1): 25.\n\n\nStrobl, Carolin, James Malley, and Gerhard Tutz. 2009. “An\nIntroduction to Recursive Partitioning: Rationale, Application, and\nCharacteristics of Classification and Regression Trees, Bagging, and\nRandom Forests.” Psychological Methods 14: 323–48. https://doi.org/10.1037/a0016973.\n\n\nUnuabonah, Foluke Olayinka, and Ulrike Gut. 2018. “Commentary\nPragmatic Markers in Nigerian English.”\nEnglish World-Wide 39 (2): 190–213.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using\nr. New York; London: Routledge.\n\n\nWood, Simon N. 2006. Generalized Additive\nModels: An Introduction with\nR. Boca Raton: Chapman & Hall/CRC.\n\n\nWulff, Stefanie, and Paul Baker. 2020. “Analyzing\nConcordances.” In A Practical\nHandbook of Corpus\nLinguistics, edited by Magali Paquot and Stefan Th.\nGries, 161–79. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-46216-1_8.",
    "crumbs": [
      "References",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "Research_questions.html#what-makes-a-good-research-question",
    "href": "Research_questions.html#what-makes-a-good-research-question",
    "title": "2  Research questions",
    "section": "2.1 What makes a good research question?",
    "text": "2.1 What makes a good research question?\n\n2.1.1 The structure of research questions\nExample: I am studying the pronunciation of [r] in New York because I want to find out whether different social groups differ with respect to their pronunciation, in order to help my readers understand how social factors influence our linguistic behavior.\n\nTopic: I am studying...\nIndirect question: Because I want to find out what, why, how...\n(General) linguistic significance: ... in order to help the readers understand how, why, or whether...\n\n\nCf. Booth, Colomb, and Williams (2008): 45–48\n\n\n\n2.1.2 Hints for good research questions\n\n\n\n\n\n\nTip\n\n\n\n\nA research question must be simple, specific, and doable. It is better to make a small contribution to a particular problem than to aim at covering a whole subfield of sociolinguistics.\nKeep a research question stable in your paper, even if you come across interesting new ideas as you work on it.\nThe research question must be an explicit part of your paper. Mention it in the introduction and keep referring back to it from time to time.\nKeep in mind that the research question will determine both methodology and data.\n\n\nCf. Hazen (2015): 9–10\n\n\n\n\n\n2.1.3 Hypotheses\nResearch questions must be complemented by a set of falsifiable hypotheses. These will be covered in-depth in the chapter on hypothesis testing. In short,\n\nHypothesis \\(H_1\\) predicts a specific relationship between a dependent variable and an independent variable. If \\(H_1\\) holds, studies often speak of a correlation (or association) between variables.\nits opposite, Hypothesis \\(H_0\\), describes the state of affairs where the predicted relationship between the variables does not hold.\n\nThe predictions made by the hypotheses are based on previous research. Here is an example:\nResearch question: I am studying the pronunciation of [r] in New York because I want to find out whether different social groups differ with respect to their pronunciation, in order to help my readers understand how social factors influence our linguistic behavior.\n\nHypothesis \\(H_1\\): There is a difference in the use of postvocalic /r/ between upper-class and lower-class speakers.\nHypothesis \\(H_0\\): There is no difference in the use of postvocalic /r/ between upper-class and lower-class speakers.\n\n\n\n2.1.4 Exercises\n\nExercise 2.1 Assess the following research questions!\n\nI will try to find the specific linguistic features which a Shetlander uses when speaking Scottish standard English, i.e., Shetland accent in Scottish Standard English.\nIn this paper, I am going to analyze the use of American and Indian dialect features among Indian immigrants to the US because I want to find out whether they correlate with their willingness to integrate into American society; this will show the degree to which linguistic adaptation depends on cultural orientation.\nIn this paper, I am going to address the question of how gender influences language.\nThe study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all.\nIn this paper, adverts by Coca-Cola and Pepsi will be analyzed for the use of verb forms.\n\n\n\nExercise 2.2 Develop valid research questions on the basis of the following linguistic variables:\n\nSpeakers in bilingual conversations are especially likely to code-switch when there is a significant change of topic.\nWomen use more tentative language in conversations than men.\n\n\n\n\n\n\nBooth, Wayne C., Gregory G. Colomb, and Joseph M. Williams. 2008. The Craft of Research. 3rd ed. Chicago: The University of Chicago Press.\n\n\nHazen, Kirk. 2015. An Introduction to Language. Chichester: Wiley Blackwell.",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Research questions</span>"
    ]
  },
  {
    "objectID": "Hypothesis_testing.html#preparation",
    "href": "Hypothesis_testing.html#preparation",
    "title": "18  Hypothesis testing",
    "section": "18.2 Preparation",
    "text": "18.2 Preparation",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "Hypothesis_testing.html#sec-hyp",
    "href": "Hypothesis_testing.html#sec-hyp",
    "title": "18  Hypothesis testing",
    "section": "18.2 Hypothesis testing",
    "text": "18.2 Hypothesis testing\nThe null hypothesis significance testing (NHST) framework offers researchers a convenient way of testing theoretical assumptions about a population of interest (e.g., a speech community). This involves setting up a set of falsifiable statistical hypotheses that predict the presence or absence of certain patterns in the data. These are known as the null hypothesis \\(H_0\\) and the alternative hypothesis \\(H_1\\) (or \\(H_a\\)). They are set up before seeing the data and justified by previous research.\n\nGiven two categorical variables \\(X\\) and \\(Y\\), we assume under \\(H_0\\) that both variables are independent from each other. This hypothesis describes the “default state of the world” (James et al. 2021: 555), i.e., what we would usually expect to see. There is no association between the variables of interest and, therefore, no effect.\nBy contrast, the alternative hypothesis \\(H_1\\) claims that \\(X\\) and \\(Y\\) are not independent, i.e., that \\(H_0\\) does not hold. \\(X\\) and \\(Y\\) then appear to be correlated in some way, i.e., there is some kind of effect.\n\nIn the subsequent sections, we will consider two scenarios:\n\nWe are interested in finding out whether English clause ORDER (‘sc-mc’ or ‘mc-sc’) depends on the type of the subordinate clause (SUBORDTYPE), which can be either temporal (‘temp’) or causal (‘caus’).\n\nOur hypotheses are:\n\n\\(H_0:\\) The variables ORDER and SUBORDTYPE are independent.\n\\(H_1:\\) The variables ORDER and SUBORDTYPE are not independent.\n\n\nAs part of a phonetic study, we compare the base frequencies of the F1 formants for male and female speakers of Apache. We forward the following hypotheses:\n\n\n\\(H_0:\\) mean F1 frequency of men \\(=\\) mean F1 frequency of women.\n\\(H_1:\\) mean F1 frequency of men \\(\\ne\\) mean F1 frequency of women.\n\nBased on our data, we can decide to either accept or reject \\(H_0\\). Rejecting \\(H_0\\) can be viewed as evidence in favour of \\(H_1\\) and thus marks a potential ‘discovery’ in the data. However, there is always a chance that we accept or reject the wrong hypothesis; the four possible constellations are summarised in the table below (cf. Heumann, Schomaker, and Shalabh 2022: 223):\n\n\n\n\n\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is not true\n\n\n\n\n\\(H_0\\) is not rejected\n\\(\\color{green}{\\text{Correct decision}}\\)\n\\(\\color{red}{\\text{Type II } (\\beta)\\text{-error}}\\)\n\n\n\\(H_0\\) is rejected\n\\(\\color{red}{\\text{Type I } (\\alpha)\\text{-error}}\\)\n\\(\\color{green}{\\text{Correct decision}}\\)\n\n\n\nThe probability of a Type I error, which refers to the rejection of \\(H_0\\) although it is true, is called the significance level \\(\\alpha\\), which has a conventional value of \\(0.05\\) (i.e., a 5% chance of committing a Type I error). Nevertheless, it is always recommended to state explicitly the \\(\\alpha\\)-level used for rejecting/accepting \\(H_0\\).",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "Hypothesis_testing.html#constructing-the-critical-region",
    "href": "Hypothesis_testing.html#constructing-the-critical-region",
    "title": "18  Hypothesis testing",
    "section": "18.3 Constructing the critical region",
    "text": "18.3 Constructing the critical region\nAn important question remains: How great should the difference be for us to reject \\(H_0\\)? The \\(p\\)-value measures the probability of encountering a specific value of a test statistic (\\(\\chi^2\\)-score, \\(t\\), \\(F\\) etc.) on the condition that \\(H_0\\) is true.\n\n\n\n\n\n\nA more precise definition of \\(p\\)-values\n\n\n\n“The \\(P\\)-value is the probability, presuming that \\(H_0\\) is true, that the test statistic equals the observed value or a value even more extreme in the direction predicted by \\(H_a\\) (Agresti and Kateri 2022: 163)”.\n\n\nFor example, a \\(p\\)-value of \\(0.02\\) means that we would see a particular test statistic only 2% of the time if \\(X\\) and \\(Y\\) were unrelated (or if there was no difference between \\(\\bar{x}\\) and \\(\\bar{y}\\), respectively). Since our significance level \\(\\alpha\\) is set to \\(0.05\\), we only reject the null hypothesis if this conditional probability is lower than 5%.\nWe obtain \\(p\\)-values by consulting the probability density functions of the underlying sampling distributions:\n\nProbability density function for the \\(\\chi^2\\)-distribution with \\(df = 1\\)\n\n\n\nCode\n# Generate random samples from a chi-squared distribution with 1 degree of freedom\nx &lt;- rchisq(100000, df = 1)\n\n# Create histogram\nhist(x,\n     breaks = \"Scott\",\n     freq = FALSE,\n     xlim = c(0, 20),\n     ylim = c(0, 0.2),\n     ylab = \"Probability density of observing a specific score\",\n     xlab = \"Chi-squared score\",\n     main = \"Histogram for a chi-squared distribution with 1 degree of freedom (df)\",\n     cex.main = 0.9)\n\n# Overlay PDF\ncurve(dchisq(x, df = 1), from = 0, to = 150, n = 5000, col = \"steelblue\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\n\n\nProbability density function for the \\(t\\)-distribution with \\(df = 112.19\\)\n\n\n\nCode\n# Given t-statistic and degrees of freedom\nt_statistic &lt;- 2.4416\ndf &lt;- 112.19\n\n# Generate random samples from a t-distribution with the given degrees of freedom\nx &lt;- rt(100000, df = df)\n\n# Create histogram\nhist(x,\n     breaks = \"Scott\",\n     freq = FALSE,\n     xlim = c(-5, 5),\n     ylim = c(0, 0.4),\n     ylab = \"Probability density of observing a specific score\",\n     xlab = \"t-score\",\n     main = \"Histogram for a t-distribution with 112.19 degrees of freedom\",\n     cex.main = 0.9)\n\n# Overlay PDF\ncurve(dt(x, df = df), from = -5, to = 5, n = 5000, col = \"steelblue\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgresti, Alan, and Maria Kateri. 2022. Foundations of Statistics for Data Scientists: With r and Python. Boca Raton: CRC Press.\n\n\nBaguley, Thomas. 2012. Serious Stats: A Guide to Advanced Statistics for the Behavioral Sciences. Houndmills, Basingstoke: Palgrave Macmillan.\n\n\nGries, Stefan Thomas. 2021. Statistics for Linguistics with r: A Practical Introduction. 3rd rev. ed. Berlin; Boston: De Gruyter Mouton.\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. New York: Springer. https://doi.org/10.1007/978-1-0716-1418-1.",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "Hypothesis_testing.html#suggested-reading",
    "href": "Hypothesis_testing.html#suggested-reading",
    "title": "18  Hypothesis testing",
    "section": "18.1 Suggested reading",
    "text": "18.1 Suggested reading\nFor linguists:\n\nGries (2021): Chapter 1.3.2\n\nGeneral:\n\nBaguley (2012): Chapter 4\nAgresti and Kateri (2022): Chapter 5",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "Linguistic_variables.html#what-is-a-linguistic-variable",
    "href": "Linguistic_variables.html#what-is-a-linguistic-variable",
    "title": "3  Linguistic variables",
    "section": "3.1 What is a linguistic variable?",
    "text": "3.1 What is a linguistic variable?\n\nThe classical view: Labov (1972) defines a linguistic variable as “two ways of saying the same thing.”\nA restriction: Meyerhoff (2009: 11) summarized, “In sum, a sociolinguistic variable can be defined as a linguistic variable that is constrained by social or non-linguistic factors…”\nA more open view: Kiesling (2011) argued, “Given the variability of what counts as a variable, we must define what counts as a variable more broadly than ‘two or more ways of saying the same thing’. We will simply say that a linguistic variable is a choice or option about speaking in a speech community… Note that this definition does not in any way require us to state that the meaning be the same, although there should be some kind of equivalence noted.”\n\n\n\n\n\n\n\nDiscussion\n\n\n\nWhich of the following variables are good sociolinguistic variables, and which of them are poor?\n\n/fɔːθ flɔː/ vs. /fɔːrθ flɔːr/\nThis enables him to preside over the process which I have described vs. This enables him to preside over the process that I have described vs. This enables him to preside over the process ∅ I have described.\nThe pair found the briefcase on a bus station bench at Bath central bus station. vs. The briefcase was found on a bus station bench at Bath central bus station by the pair.\nArt is after all the subject of attention for both critic and historian, even though the functions and methods of the two sorts of writer have drawn apart. vs. Art histories often make an attempt to keep to chronology, although the difficulties include the crucial fact that in art there is no clear sequence of events. vs. Many of his readers approved his sensitive and appreciative understanding of paintings, though without sharing his political views.\n/pleɪŋ/ vs. /pleɪn/\n[tʰ] in /tɔp/ vs. [t] in stop.",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linguistic variables</span>"
    ]
  },
  {
    "objectID": "Linguistic_variables.html#the-principle-of-accountability",
    "href": "Linguistic_variables.html#the-principle-of-accountability",
    "title": "3  Linguistic variables",
    "section": "3.2 The principle of accountability",
    "text": "3.2 The principle of accountability\n\n\n\n\n\n\nTask\n\n\n\nTwo linguists aim to study the preference for passives among men and women. They extract all the passives from 500,000 words of male speech and all passives from 500,000 words of female speech and report the results. What’s wrong?",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linguistic variables</span>"
    ]
  },
  {
    "objectID": "Linguistic_variables.html#subtypes-of-variables",
    "href": "Linguistic_variables.html#subtypes-of-variables",
    "title": "3  Linguistic variables",
    "section": "3.3 Subtypes of variables",
    "text": "3.3 Subtypes of variables\n\n3.3.1 Linguistic perspective\n\nphonetic/phonological\nmorphological\nsyntactic\npragmatic\n\n\n\n3.3.2 Sociolinguistic perspective\nSociolinguistic variables also differ with regard to their salience in society.\n\nStereotypes are strongly socially marked and part of popular discourse about language.\n\nh-dropping in Cockney\nCanadian eh at the end of sentences\nAustralian dinkum: I was fair dinkum about my interest in their culture ‘authentic, genuine’\n\nMarkers show both social and style stratification; all members of a society react similarly in taking care to avoid the pattern in formal registers.\n\n(r)\n(th)\n\nIndicators differentiate social groups. However, people are not aware of them and therefore do not avoid them in formal registers.\n\nSame vowel in God and Guard in New York City\n\n\n\nCf. Mesthrie (2011).",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linguistic variables</span>"
    ]
  },
  {
    "objectID": "Linguistic_variables.html#statistical-perspective",
    "href": "Linguistic_variables.html#statistical-perspective",
    "title": "3  Linguistic variables",
    "section": "3.4 Statistical perspective",
    "text": "3.4 Statistical perspective\n\n3.4.1 Dependent and independent variables\nTask: Decide which of the variables are independent, and which are dependent variables!\n\n\n\n\nKenyan speakers\nSingaporean speakers\n\n\n\n\nAverage sentence length\n10.5\n9.8\n\n\n\n\n\n\n\nmale\nfemale\n\n\n\n\nactually\n65\n98\n\n\npossibly\n55\n77\n\n\nreally\n54\n55\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: English is the most important language today.\nstrongly agree\nagree\nneutral\ndisagree\nstrongly disagree\n\n\n\n\nN\n12\n33\n58\n12\n8\n\n\n\n\n\n3.4.2 Interval, ordinal, and nominal\n\nCOMPLEXITY (ordinal)\n\nthe book (low)\nthe brown book (middle)\nthe book I had bought in Europe (high)\n\nLENGTH (interval)\n\nbook (1)\nthe book (2)\nthe book I had bought in Europe (7)\n\nANIMACY (nominal)\n\nHe picked up the book.\nHe picked his dad up.\n\n\n\nCf. Gries, Stefan Th. Statistics for Linguistics with R. De Gruyter Mouton, 2013, p. 9.\n\nTask: Now, go back to Tables 1–3 and decide which of the variables are interval variables, ordinal or nominal variables!",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linguistic variables</span>"
    ]
  },
  {
    "objectID": "Linguistic_variables.html#many-morphosyntactic-variables-in-english",
    "href": "Linguistic_variables.html#many-morphosyntactic-variables-in-english",
    "title": "3  Linguistic variables",
    "section": "3.4 Many morphosyntactic variables in English",
    "text": "3.4 Many morphosyntactic variables in English\n\n\n\n\n\n\n\nVariable\nExample\n\n\n\n\nIndefinite Pronouns\neverybody vs. everyone\n\n\nCase and order of coordinated pronouns\nmy husband and I vs. my husband and me vs. me and my husband\n\n\nthat vs. zero complementation\nI don’t think that/Ø it’s a problem.\n\n\nthat vs. gerundial complementation\nremember that vs. remember V-ing; try to vs. try and vs. try V-ing\n\n\nParticle placement\nset the computer up vs. set up the computer\n\n\nThe dative alternation\ngive the book to John vs. give John the book\n\n\nThe genitive alternation\nJohn’s house vs. the house of John\n\n\nRelativization strategies\nwh-word vs. that vs. Ø\n\n\nAnalytic vs. synthetic comparatives\nwarmer vs. more scary\n\n\nPlural existentials\nthere are some places vs. there’s some places\n\n\nFuture temporal reference\nwill vs. going to vs. progressive etc.\n\n\nDeontic modality\nmust vs. have to vs. need to vs. got to etc.\n\n\nStative possession\nhave vs. have got vs. got\n\n\nQuotatives\nsay vs. be like vs. go etc.\n\n\nnot vs. no\nnot anybody vs. nobody; not anyone vs. no one; not anything vs. nothing\n\n\nNOT vs. AUX contraction\nthat’s not vs. that isn’t etc.\n\n\n\n\nCf. Gardner et al. (2021).\n\n\n\n\n\nGardner, Matt Hunt et al. 2021. “Variation Isn’t That Hard: Morphosyntactic Choice Does Not Predict Production Difficulty.” PloS One 16 (6): e0252602–2.\n\n\nKiesling, Scott F. 2011. Linguistic Variation and Change. Edinburgh: Edinburgh University Press.\n\n\nLabov, William. 1972. Sociolinguistic Patterns. Philadelphia: University of Pennsylvania Press.\n\n\nMesthrie, Rajend. 2011. Introducing Sociolinguistics. 2nd ed. Edinburgh: Edinburgh University Press.\n\n\nMeyerhoff, Miriam. 2009. Introducing Sociolinguistics. London: Routledge.",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linguistic variables</span>"
    ]
  },
  {
    "objectID": "PCA.html#recommended-reading",
    "href": "PCA.html#recommended-reading",
    "title": "27  Principal Components Analysis",
    "section": "27.1 Recommended reading",
    "text": "27.1 Recommended reading\nFor linguists:\n\nLevshina (2015: Chapter 18)\n\nGeneral:\n\nMair (2018: Chapter 6)",
    "crumbs": [
      "Multivariate Data Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "PCA.html#sec-pca-prep",
    "href": "PCA.html#sec-pca-prep",
    "title": "27  Principal Components Analysis",
    "section": "27.2 Preparation",
    "text": "27.2 Preparation\nThis unit relies on psycholinguistic data from the South Carolina Psycholinguistic Metabase (Gao, Shinkareva, and Desai 2022).1 Detailed descriptions of the variables can be found here.\n1 One exception is the variable Resnik_strength [Resnik (1996)], which was computed manually and appended to the data frame.The data frame scope_sem_df contains semantic ratings for a sample of 1,702 transitive verbs. Note that all columns have been standardised (cf. ?scale() for details).\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(lattice)\nlibrary(corrplot)\nlibrary(psych)\nlibrary(GPArotation)\nlibrary(gridExtra)\n\n# Load data\nscope_sem_df &lt;- readRDS(\"scope_sem.RDS\")\n\n# Select subset\nscope_sem_sub &lt;- scope_sem_df[,1:11]\n\n# Overview\nglimpse(scope_sem_sub)\n\nRows: 1,702\nColumns: 11\n$ Verb               &lt;chr&gt; \"abstain\", \"abstract\", \"abuse\", \"accelerate\", \"acce…\n$ Resnik_strength    &lt;dbl&gt; 0.40909889, 0.18206692, 0.12473608, -0.76972217, -1…\n$ Conc_Brys          &lt;dbl&gt; -0.94444378, -1.92983639, -0.59478833, 0.22107437, …\n$ Nsenses_WordNet    &lt;dbl&gt; -0.68843996, 0.27755219, 0.00155443, -0.68843996, 0…\n$ Nmeanings_Websters &lt;dbl&gt; -0.95559835, 0.73781281, 0.73781281, -0.27823388, 0…\n$ Visual_Lanc        &lt;dbl&gt; -2.2545455, 0.6103733, 1.3354358, -0.4342084, -0.34…\n$ Auditory_Lanc      &lt;dbl&gt; -0.84225787, -0.35605108, 1.54797548, 0.18795651, 1…\n$ Haptic_Lanc        &lt;dbl&gt; -0.75523987, -0.29089287, 1.25099360, -0.18911818, …\n$ Olfactory_Lanc     &lt;dbl&gt; -0.14444936, -0.37350419, -0.53335522, -0.37350419,…\n$ Gustatory_Lanc     &lt;dbl&gt; 0.27698988, -0.10105698, -0.36148925, -0.52110903, …\n$ Interoceptive_Lanc &lt;dbl&gt; 1.08153427, -0.06560311, 1.64313895, 1.45452985, 0.…",
    "crumbs": [
      "Multivariate Data Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "PCA.html#descriptive-overview",
    "href": "PCA.html#descriptive-overview",
    "title": "27  Principal Components Analysis",
    "section": "27.3 Descriptive overview",
    "text": "27.3 Descriptive overview\nA popular descriptive measure for associations between continuous variables \\(x\\) and \\(y\\) is the Pearson product-moment correlation coefficient (or simply Pearson’s \\(r\\); cf. Equation 27.1). It varies on a scale from \\(-1\\) to \\(1\\) and indicates the extent to which two variables form a straight-line relationship (Heumann, Schomaker, and Shalabh 2022: 153-154). One of its core components is the covariance between \\(x\\) and \\(y\\) which “measures the average tendency of two variables to covary (change together)” (Baguley 2012: 206).\n\\[\nr_{xy} = \\frac{Cov(x, y)}{\\sqrt{Var(x)}\\sqrt{Var(y)}}= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}.\n\\tag{27.1}\\]\nIn R, we can compute Pearson’s \\(r\\) by using the cor() function.\n\n# Check correlation between number of senses and concreteness\ncor(scope_sem_sub[,-1]$Nsenses_WordNet, scope_sem_sub[,-1]$Conc_Brys) # low\n\n[1] 0.2351554\n\n# Check correlation between haptic experience and concreteness\ncor(scope_sem_sub[,-1]$Haptic_Lanc, scope_sem_sub[,-1]$Conc_Brys) # high\n\n[1] 0.5676945\n\n\nIf the data frame consists of numeric columns only (i.e., if it is a matrix), we can apply cor() to the full dataset and obtain the correlation matrix (also known as covariance matrix).\n\n# Generate correlation matrix\ncor_mat1 &lt;- cor(scope_sem_sub[,-1])\n\nhead(cor_mat1)\n\n                   Resnik_strength  Conc_Brys Nsenses_WordNet\nResnik_strength         1.00000000  0.1166670     -0.37442983\nConc_Brys               0.11666697  1.0000000      0.23515537\nNsenses_WordNet        -0.37442983  0.2351554      1.00000000\nNmeanings_Websters     -0.34225250  0.2023356      0.68509560\nVisual_Lanc             0.05471417  0.5519836      0.17154846\nAuditory_Lanc          -0.11162700 -0.2683646     -0.02960745\n                   Nmeanings_Websters Visual_Lanc Auditory_Lanc  Haptic_Lanc\nResnik_strength            -0.3422525  0.05471417   -0.11162700  0.008260683\nConc_Brys                   0.2023356  0.55198358   -0.26836458  0.567694470\nNsenses_WordNet             0.6850956  0.17154846   -0.02960745  0.239470104\nNmeanings_Websters          1.0000000  0.14597243   -0.04656650  0.193226862\nVisual_Lanc                 0.1459724  1.00000000   -0.11674896  0.404536416\nAuditory_Lanc              -0.0465665 -0.11674896    1.00000000 -0.289586292\n                   Olfactory_Lanc Gustatory_Lanc Interoceptive_Lanc\nResnik_strength        0.05131717    0.015087346      -9.459551e-02\nConc_Brys              0.21354305    0.123459754      -3.257702e-01\nNsenses_WordNet       -0.03353627   -0.018262178      -1.392934e-02\nNmeanings_Websters    -0.01898766    0.001409646      -4.460375e-05\nVisual_Lanc            0.15319007    0.055176064      -3.424087e-01\nAuditory_Lanc         -0.06123191   -0.047086877       1.799479e-01\n\n# Plot correlation matrix\ncorrplot(cor_mat1, col = topo.colors(200), tl.col = \"darkgrey\", number.cex = 0.5, tl.cex = 0.5)\n\n\n\n\n\n\n\n\nSince the upper triangle mirrors the lower one, it is enough to only examine one of them. The diagonal values are not particularly insightful and can be ignored.\n\n# Levelplot\nseq1 &lt;- seq(-1, 1, by = 0.01)\n\nlevelplot(cor_mat1, aspect = \"fill\", col.regions = topo.colors(length(seq1)),\n          at = seq1, scales = list(x = list(rot = 45)),\n          xlab = \"\", ylab = \"\")\n\n\n\n\n\n\n\n\nNeedless to say, the above correlation matrices are hard to interpret – even more so if the number of variables were to increase further.\nPrincipal Components Analysis offers a technique to break down a high-dimensional dataset into a much smaller set of “meta-variables”, i.e., principle components (PCs) which capture the bulk of the variance in the data. This is also known as dimension reduction, which allows researchers to see overarching patterns in the data and re-use the output for further analysis (e.g., clustering or predictive modelling).",
    "crumbs": [
      "Multivariate Data Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "PCA.html#basics-of-pca",
    "href": "PCA.html#basics-of-pca",
    "title": "27  Principal Components Analysis",
    "section": "27.4 Basics of PCA",
    "text": "27.4 Basics of PCA\nPCA “repackages” large sets of variables by forming uncorrelated linear combinations of them, yielding \\(k\\) principal components \\(Z_1, ..., Z_k\\) (PCs hf.) of the dataset (for \\(1, ..., k\\)). PCs are ordered such that the first PC explains the most variance in the data, with each subsequent PC explaining the maximum remaining variance while being uncorrelated with previous PCs.\nEach PC comprises a set of loadings (or weights) \\(w_{nm}\\), which are comparable to the coefficients of regression equations. For instance, the first PC has the general form shown in Equation 27.2, where \\(x_m\\) stand for continuous input variables in the \\(n \\times m\\) data matrix \\(\\mathbf{X}\\).\n\\[\nZ_{1} = w_{11}\n\\begin{pmatrix}\nx_{11} \\\\\nx_{21} \\\\\n\\vdots \\\\\nx_{n1}\n\\end{pmatrix}\n+ w_{21}\n\\begin{pmatrix}\nx_{12} \\\\\nx_{22} \\\\\n\\vdots \\\\\nx_{n2}\n\\end{pmatrix}\n+ \\dots + w_{m1}\n\\begin{pmatrix}\nx_{1m} \\\\\nx_{2m} \\\\\n\\vdots \\\\\nx_{nm}\n\\end{pmatrix}\n\\tag{27.2}\\]\nIf a feature positively loads on a principal component (i.e., \\(w &gt; 0\\)), it means that as the value of this feature increases, the score for this principal component also increases. The magnitude of \\(w\\) indicates the strength of this relationship. Conversely, negative loadings (\\(w &lt; 0\\)) indicate that as the feature value increases, the PC score decreases as well.\n\n\n\n\n\n\nHow do we find PCs?\n\n\n\n\n\nPCs are identified using common techniques from matrix algebra, namely singular value decomposition and eigenvalue decomposition. By breaking down the input data into products of several further matrices, it becomes possible to characterise the exact ‘shape’ of its variance (Mair 2018: 181).\n\n\n\nThe figure below offers a visual summary of PCA:",
    "crumbs": [
      "Multivariate Data Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "PCA.html#application-in-r",
    "href": "PCA.html#application-in-r",
    "title": "27  Principal Components Analysis",
    "section": "27.5 Application in R",
    "text": "27.5 Application in R\n\n27.5.1 Fitting the model and identifying number of PCs\nFirst, we fit a PCA object with the number of PCs equivalent to the number of columns in scope_sem_sub.\n\n# Fit initial PCA\npca1 &lt;- principal(scope_sem_sub[,-1],\n                  nfactors = ncol(scope_sem_sub[,-1]),\n                  rotate = \"none\")\n\n# Print loadings\nloadings(pca1)\n\n\nLoadings:\n                   PC1    PC2    PC3    PC4    PC5    PC6    PC7    PC8   \nResnik_strength            0.666 -0.271 -0.100  0.250  0.627              \nConc_Brys           0.813  0.210 -0.173         0.149        -0.170 -0.260\nNsenses_WordNet     0.523 -0.696  0.124                0.241              \nNmeanings_Websters  0.493 -0.683  0.149                0.343              \nVisual_Lanc         0.691  0.168 -0.236  0.382  0.152 -0.127  0.484  0.136\nAuditory_Lanc      -0.388 -0.228  0.199  0.734  0.413        -0.208       \nHaptic_Lanc         0.728  0.113        -0.272  0.401 -0.254 -0.266  0.120\nOlfactory_Lanc      0.324  0.444  0.671  0.163 -0.200               -0.347\nGustatory_Lanc      0.256  0.377  0.759        -0.160                0.377\nInteroceptive_Lanc -0.341 -0.164  0.577 -0.366  0.543         0.265 -0.100\n                   PC9    PC10  \nResnik_strength                 \nConc_Brys          -0.378       \nNsenses_WordNet            0.405\nNmeanings_Websters        -0.373\nVisual_Lanc                     \nAuditory_Lanc                   \nHaptic_Lanc         0.266       \nOlfactory_Lanc      0.234       \nGustatory_Lanc     -0.187       \nInteroceptive_Lanc -0.120       \n\n                 PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10\nSS loadings    2.629 1.898 1.595 0.937 0.806 0.657 0.461 0.378 0.330 0.309\nProportion Var 0.263 0.190 0.160 0.094 0.081 0.066 0.046 0.038 0.033 0.031\nCumulative Var 0.263 0.453 0.612 0.706 0.787 0.852 0.898 0.936 0.969 1.000\n\n\nIt is common practice to retain only those PCs with eigenvalues (variances) \\(&gt; 1\\) (cf. scree plot).\n\n# Scree plot\nbarplot(pca1$values, main = \"Scree plot\", ylab = \"Variances\", xlab = \"PC\", # first three PCs\n        names.arg = 1:length(pca1$values))\n  abline(h = 1, col = \"blue\", lty = \"dotted\")\n\n\n\n\n\n\n\n\nAlternatively, one can perform parallel analysis to identify statistically significant PCs whose variances are “larger than the 95% quantile […] of those obtained from random or resampled data” (Mair 2018: 31). The corresponding function is fa.parallel() from the psych package.\n\npca.pa &lt;- fa.parallel(scope_sem_sub[,-1], # raw data\n                     fa = \"pc\", # Use PCA instead of factor analysis\n                     cor = \"cor\",  # Use Pearson correlations (default for PCA)\n                     n.iter = 200, # Number of iterations (increase for more stable results)\n                     quant = 0.95, # Use 95th percentile (common choice)\n                     fm = \"minres\") # Factor method\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  3 \n\n\n\n\n27.5.2 Accessing and visualising the loadings\nSince three PCs appear to be enough to explain the majority of variance in the data, we will refit the model with nfactors = 3.\n\npca2 &lt;- principal(scope_sem_sub[,-1],\n                  nfactors = 3,\n                  rotate = \"none\")\n\nA convenient function for printing the PCA loadings is loadings(). Weights close to \\(0\\) are not displayed.\n\nloadings(pca2)\n\n\nLoadings:\n                   PC1    PC2    PC3   \nResnik_strength            0.666 -0.271\nConc_Brys           0.813  0.210 -0.173\nNsenses_WordNet     0.523 -0.696  0.124\nNmeanings_Websters  0.493 -0.683  0.149\nVisual_Lanc         0.691  0.168 -0.236\nAuditory_Lanc      -0.388 -0.228  0.199\nHaptic_Lanc         0.728  0.113       \nOlfactory_Lanc      0.324  0.444  0.671\nGustatory_Lanc      0.256  0.377  0.759\nInteroceptive_Lanc -0.341 -0.164  0.577\n\n                 PC1   PC2   PC3\nSS loadings    2.629 1.898 1.595\nProportion Var 0.263 0.190 0.160\nCumulative Var 0.263 0.453 0.612\n\n\nIn order to see what features load particularly strongly on the PCs, we can draw a path diagram with diagram(). Note that the red arrows indicate negative weights (i.e., negative “regression coefficients”).\n\ndiagram(pca2, main = NA)\n\n\n\n\n\n\n\n\nThe generic plot method returns a scatterplot of the loadings:\n\nplot(pca2, labels = colnames(scope_sem_sub[,-1]), main = NA)\n\n\n\n\n\n\n\n\nFinally, you can obtain the PC scores for each observation in the input data by accessing the $scores element:\n\nhead(pca2$scores, n = 15)\n\n              PC1         PC2         PC3\n [1,] -1.45999990  0.38323657  0.61549383\n [2,] -0.32170158 -0.60027352 -0.08271852\n [3,]  0.12196548 -0.68757984  0.33959072\n [4,] -0.57929327 -0.35785887  0.26877126\n [5,] -0.34097381 -1.35963060  0.54717808\n [6,] -0.04799048 -0.34404820 -0.19668070\n [7,] -0.33873248  0.52372694 -0.28318588\n [8,] -1.11868861  0.26178424 -0.66465979\n [9,]  0.15263031  0.60489417 -0.84324699\n[10,] -1.75834143 -0.47957110  0.44313029\n[11,] -1.26440095 -1.15766536  0.46594800\n[12,]  0.10641410  0.05075197  0.48556702\n[13,] -1.26133394 -0.35022899 -0.36512925\n[14,] -0.28070472  0.60992380 -1.29547347\n[15,] -0.72805598 -0.45777808  0.56031788\n\n\nBiplots offer juxtaposed visualisations of PC scores (points) and loadings (arrows).\n\n# PC1 and PC2\nbiplot(pca2, choose = c(1, 2), \n       main = NA, pch = 20, col = c(\"darkgrey\", \"blue\"))\n\n\n\n\n\n\n\n# PC2 and PC3\nbiplot(pca2, choose = c(2, 3), \n       main = NA, pch = 20, col = c(\"darkgrey\", \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the PCA output\n\n\n\nAfter inspecting the loadings and biplots, we can see the following patterns:\n\nExternal sensation: Higher ratings in concreteness (i.e., direct perception with one’s senses) as well as the visual and haptic dimensions of verbs are associated with an increase in PC1.\nSenses and selection: PC2 displays notable negative loadings in features relating to the number of meanings a verb has and how much information it carries about the meaning of its objects. PC2 scores decrease if a verb has fewer meanings, but they increase if it displays higher selectional preference strength.\nInternal sensation: PC3 captures variance in olfactory, gustatory and interoceptive2 ratings.\n\n2 Here interoceptive means “[t]o what extent one experiences the referent by sensations inside one’s body” (Gao, Shinkareva, and Desai 2022: 2859).\n\n\n\n\n\nBaguley, Thomas. 2012. Serious Stats: A Guide to Advanced Statistics for the Behavioral Sciences. Houndmills, Basingstoke: Palgrave Macmillan.\n\n\nGao, Chuanji, Svetlana V. Shinkareva, and Rutvik H. Desai. 2022. “SCOPE: The South Carolina Psycholinguistic Metabase.” Behavior Research Methods 55 (6): 2853–84. https://doi.org/10.3758/s13428-022-01934-0.\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nLevshina, Natalia. 2015. How to Do Linguistics with r: Data Exploration and Statistical Analysis. Amsterdam; Philadelphia: John Benjamins Publishing Company.\n\n\nMair, Patrick. 2018. Modern Psychometrics with R. Cham: Springer. https://doi.org/10.1007/978-3-319-93177-7.\n\n\nResnik, Philip. 1996. “Selectional Constraints: An Information-Theoretic Model and Its Computational Realization” 61 (1): 127–59.",
    "crumbs": [
      "Multivariate Data Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "Logistic_regression.html#recommended-reading",
    "href": "Logistic_regression.html#recommended-reading",
    "title": "22  Logistic Regression",
    "section": "22.1 Recommended reading",
    "text": "22.1 Recommended reading\nFor linguists:\n\nLevshina (2015: Chapter 12)\nWinter (2020: Chapter 12)\n\nFull theoretical treatment:\n\nJames et al. (2021: Chapter 4)\nHosmer & Lemeshow (2008)",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Logistic_regression.html#preparation",
    "href": "Logistic_regression.html#preparation",
    "title": "22  Logistic Regression",
    "section": "22.2 Preparation",
    "text": "22.2 Preparation\nConsider the data from Buskin’s (n.d.)1 corpus-study on subject pronoun realisation:\n1 The input data is can be downloaded from this OSF repository: https://osf.io/qgnms.\n# Load libraries\nlibrary(tidyverse)\nlibrary(rms)\nlibrary(broom)\nlibrary(sjPlot)\nlibrary(ggeffects)\nlibrary(ggpubr)\n\n# Load data\ndata_pro &lt;- read.csv(\"INPUT_pronouns.csv\", sep = \",\", header = TRUE)\n\n# Inspect data\nstr(data_pro)\n\n'data.frame':   4838 obs. of  5 variables:\n $ Reference     : chr  \"overt\" \"overt\" \"overt\" \"overt\" ...\n $ Person        : chr  \"3\" \"3\" \"3\" \"3\" ...\n $ Register      : chr  \"S1A\" \"S1A\" \"S1A\" \"S1A\" ...\n $ Variety       : chr  \"GB\" \"GB\" \"GB\" \"GB\" ...\n $ Referentiality: chr  \"referential\" \"referential\" \"referential\" \"referential\" ...\n\nhead(data_pro)\n\n  Reference Person Register Variety Referentiality\n1     overt      3      S1A      GB    referential\n2     overt      3      S1A      GB    referential\n3     overt      3      S1A      GB    referential\n4     overt      3      S1A      GB    referential\n5     overt      3      S1A      GB    referential\n6     overt      3      S1A      GB    referential\n\n\n\nTarget variable:\n\nReference (‘overt’, ‘null’)\n\nExplanatory variables:\n\nPerson (‘1.p.’, ‘2.p’, ‘3.p’ as well as the dummy pronouns ‘it’ and ‘there’)\nRegister (the text category in the International Corpus of English; ‘S1A’ are informal conversations, whereas ‘S1B’ comprises formal class lessons)\nVariety (British English ‘GB’, Singapore English ‘SING’ and Hong Kong English ‘HK’) and\nReferentiality (‘referential’ with an identifiable referent or ‘non-referential’ with no/generic reference)\n\n\n\nhead(data_pro)\n\n  Reference Person Register Variety Referentiality\n1     overt      3      S1A      GB    referential\n2     overt      3      S1A      GB    referential\n3     overt      3      S1A      GB    referential\n4     overt      3      S1A      GB    referential\n5     overt      3      S1A      GB    referential\n6     overt      3      S1A      GB    referential\n\ntable(data_pro$Reference)\n\n\n null overt \n  174  4664 \n\n\n\n22.2.1 Descriptive overview",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Logistic_regression.html#logistic-regression",
    "href": "Logistic_regression.html#logistic-regression",
    "title": "22  Logistic Regression",
    "section": "22.3 Logistic regression",
    "text": "22.3 Logistic regression\nIn contrast to linear regression, logistic regression models a qualitative response variable \\(Y\\) with two outcomes2. In the present study, \\(Y\\) is pronominal Reference and has the outcomes Reference = null and Reference = overt, which represent null and overt subjects, respectively. Dichotomous variables of this kind are also often coded as yes/no or 1/0.\n2 Logistic regression can also be used for \\(\\geq 3\\) classes by breaking down the response variable into a series of dichotomous variables. This is also known as multinomial logistic regression or softmax regression.Another difference from linear regression is the output of the model:\n\nIn linear regression, we obtain a predicted value for the continuous response variable we’re interested in. For instance, if we’re modelling reaction times, the model will return an estimated mean reaction time (given the predictors).\nIn logistic regression, however, we either get a class label or a probability. When modelling pronominal reference, the model will thus either tell us\n\n\nwhether a speaker would use an overt or a null subject in a given observation (class prediction).\nwhat the probability of using one variant vs. the other would be (probability prediction).\n\nA core component of logistic regression is the logistic function. The rationale for using it is that the output of the function will always lie between \\(0\\) and \\(1\\), and it will always denote a probability.\n\n\n\n\n\n\n\n\n\n\n22.3.1 The simple logistic model\nAssuming a binary response variable \\(Y\\) with the values 1 and 0 and a single predictor \\(X\\), the conditional probability \\(P(Y = 1 \\mid X)\\) is then equivalent to the inverse logit in Equation 22.1.\n\\[\nP(Y = 1 \\mid  X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}.\n\\tag{22.1}\\]\nWith some manipulation it can be shaped into a form that is definitely more familiar:\n\\[\n\\log\\left(\\frac{P(Y = 1 \\mid  X)}{1 - P(Y = 1 \\mid  X)}\\right) = \\beta_0 + \\beta_1X.\n\\tag{22.2}\\]\nThe logistic model has several characteristic components. The fraction \\(\\frac{P(Y = 1 \\mid X)}{1-P(Y = 1 \\mid X)}\\) represents the odds, which stand for to the probability of one outcome (e.g., Reference = null) compared to the other (e.g., Reference = overt). Their logarithmic transformation are the log odds (or logits) of one outcome versus the other.\n\n\n\n\n\n\nUnderstanding log odds\n\n\n\nWhen interpreting the output of a logistic model, note that\n\npositive log odds indicate an increase in \\(\\frac{P(Y = 1 \\mid X)}{1-P(Y = 1 \\mid X)}\\), whereas\nnegative log odds indicate a decrease in \\(\\frac{P(Y = 1 \\mid X)}{1-P(Y = 1 \\mid X)}\\).\n\n\n\nIn more concrete terms: If we are interested in the probability that the form of pronominal reference is null (our \\(Y\\)) while taking into account the extra-linguistic context (Register; our \\(X\\)), the model would then have the general form in Equation 22.3.\n\\[\n\\log\\left(\\frac{P(\\text{Reference} = \\text{null} \\mid \\text{Register})}{1- P(\\text{Reference} = \\text{null} \\mid \\text{Register})}\\right) = \\beta_0 + \\beta_1\\text{Register}\n\\tag{22.3}\\]\n\n\n\n\n\n\nFinding \\(\\beta_0\\) and \\(\\beta_1\\): Maximum Likelihood Estimation\n\n\n\n\n\nIn contrast to continuous data, the estimation of parameters for discrete response variables is much less straightforward. Rather than finding a regression line that minimises the distance to all data points, logistic models typically attempt to find the parameter values that are most likely, given the data. Hence this procedure is also known as Maximum Likelihood Estimation (MLE). For further technical details, see Wood (2006: 63-66) or Agresti & Kateri (2022: 291-294).\n\n\n\n\n\n22.3.2 Multiple logistic regression\nIf more than one predictor is included, the above equations can be expanded so as to take into account \\(p\\) slopes for \\(p\\) independent variables \\(X_1, X_2, ..., X_p\\).\n\\[ P(Y = 1 \\mid X_1, ..., X_p) = \\frac{e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}{1 + e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}.\n\\tag{22.4}\\]\nConsequently, the log odds correspond to the sum of linear predictors \\(\\beta_1X_1 + \\beta_2X_2 + ...+ \\beta_pX_p\\) (cf. Equation 22.5).\n\\[\n\\log\\left(\\frac{P(Y = 1 \\mid X_1, ..., X_p)}{1 - P(Y = 1 \\mid X_1, ...,  X_p)}\\right) = \\beta_0 + \\sum_{i=1}^{p} \\beta_i X_i\n\\tag{22.5}\\]\n\n\n22.3.3 Odds ratios\nTo assess the strength of an effect, it is instructive to examine the odds ratios that correspond to the model coefficients. Odds ratios (OR) are defined as\n\\[\nOR(X_1) = e^{\\beta_1}.\n\\]\n\n\n\n\n\n\nUnderstanding odds ratios\n\n\n\n\n\nEssentially, the OR describes the ratio between two odds with respect to another independent variable. This is illustrated for Reference given Register below:\n\\[\n\\text{OR}(\\text{Reference} \\mid \\text{Register}) = \\frac{\\frac{P(\\text{Reference} = \\text{null} \\mid \\text{Register} = \\text{S1A})}{P(\\text{Reference} = \\text{overt} \\mid \\text{Register} = \\text{S1A})}}{\\frac{P(\\text{Reference} = \\text{null} \\mid \\text{Register} = \\text{S1B})}{P(\\text{Reference} = \\text{overt} \\mid \\text{Register} = \\text{S1B})}}\n\\]\nRead as: ‘The ratio between the probability of a null vs. overt object in S1A and the probability of a null vs. overt object in S1B’.",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Logistic_regression.html#workflow-in-r",
    "href": "Logistic_regression.html#workflow-in-r",
    "title": "22  Logistic Regression",
    "section": "22.4 Workflow in R",
    "text": "22.4 Workflow in R\n\n22.4.1 Step 1: Research question and hypotheses\nHow do the intra- and extra-linguistic variables suggested in the literature affect subject pronoun realisation (Definite Null Instantiation) in British English, Singapore English and Hong Kong English?\nGiven a significance level \\(\\alpha = 0.05\\), the hypotheses are: \\[\n\\begin{aligned}\nH_0: & \\quad \\text{None of the predictor coefficients deviate from 0}.\\\\\nH_1: & \\quad \\text{At least one predictor coefficient deviates from 0}.\n\\end{aligned}\n\\]\nThese can be restated mathematically as:\n\\[\n\\begin{aligned}\nH_0: & \\quad \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0 \\\\\nH_1: & \\quad \\text{At least one } \\beta_i \\neq 0 \\text{ for } i \\in \\{1, 2, \\ldots, p\\}\n\\end{aligned} \\]\n\n\n22.4.2 Step 2: Convert to factors and specify reference levels\nThe next step involves specifying reference levels for all categorical variables. This step is very important because it will directly impact the parameter estimation and, consequently, influence our interpretation of the model output.\n\nThe reference level of the response is usually chosen such that it corresponds to the unmarked or most frequent case. Since overt pronouns are much more common in the data, the reference level of the Reference variable will be set to Reference = overt. This way, the model coefficients will directly represent the probability of the null subject variant (i.e., the special case) given certain predictor configurations.\nThe predictor levels need to be specified as well. Among other things, we are interested in how the Asian Englishes pattern relative to British English. Therefore, we will define British English as the baseline for comparison.\n\nWe will use the following specifications:\n\n\n\n\n\n\n\n\nVariable\nFactor Levels\nPreferred Reference level\n\n\n\n\nRegister\nS1A, S1B\nS1A\n\n\nVariety\nGB, SING, HK\nGB\n\n\nPerson\n1, 2, 3, it, there\n3\n\n\nReferentiality\nreferential, non-referential\nreferential\n\n\n\n\n# Store \"Reference\" as factor\ndata_pro$Reference &lt;- as.factor(data_pro$Reference)\n\n## Specify reference level (the 'unmarked' case)\ndata_pro$Reference &lt;- relevel(data_pro$Reference, \"overt\")\n\n## Print levels\nlevels(data_pro$Reference)\n\n[1] \"overt\" \"null\" \n\n\nRepeat the procedure for the remaining categorical variables.\n\n\nCode\n# Store \"Register\" as factor\ndata_pro$Register &lt;- as.factor(data_pro$Register)\n\n## Specify reference level\ndata_pro$Register &lt;- relevel(data_pro$Register, \"S1A\")\n\n# Store \"Variety\" as factor\ndata_pro$Variety &lt;- as.factor(data_pro$Variety)\n\n## Specify reference level\ndata_pro$Variety &lt;- relevel(data_pro$Variety, \"GB\")\n\n# Store \"Person\" as factor\ndata_pro$Person &lt;- as.factor(data_pro$Person)\n\n## Specify reference level\ndata_pro$Person &lt;- relevel(data_pro$Person, \"3\")\n\n# Store \"Referentiality\" as factor\ndata_pro$Referentiality &lt;- as.factor(data_pro$Referentiality)\n\n## Specify reference level\ndata_pro$Referentiality &lt;- relevel(data_pro$Referentiality, \"referential\")\n\n\n\n\n22.4.3 Step 3: Fit the model\nThere are two functions that can fit logistic models in R: lrm() and glm().\n\n\n\n\n\n\nNote\n\n\n\nThe model formula below does not include Referentiality because several intermediary steps revealed it to be almost completely irrelevant for predicting Reference. In addition, the existing (and significant) interaction Variety:Person has been excluded to improve the interpretability of the model.\n\n\n\n# With lrm(); requires library(\"rms\")\n\n# Fit interaction model\nReference.lrm &lt;- lrm(Reference ~ Register + Variety + Register:Variety + Person, data = data_pro)\n\n# View model statistics\nReference.lrm\n\nLogistic Regression Model\n\nlrm(formula = Reference ~ Register + Variety + Register:Variety + \n    Person, data = data_pro)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs          4838    LR chi2     120.43      R2       0.092    C       0.729    \n overt       4664    d.f.             9     R2(9,4838)0.023    Dxy     0.458    \n null         174    Pr(&gt; chi2) &lt;0.0001    R2(9,503.2)0.199    gamma   0.488    \nmax |deriv| 4e-10                            Brier    0.034    tau-a   0.032    \n\n                            Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept                   -3.4132 0.2746 -12.43 &lt;0.0001 \nRegister=S1B                 0.0269 0.3807   0.07 0.9437  \nVariety=HK                   0.6712 0.3174   2.11 0.0345  \nVariety=SING                 1.1193 0.2959   3.78 0.0002  \nPerson=1                    -0.8807 0.1811  -4.86 &lt;0.0001 \nPerson=2                    -1.6441 0.2695  -6.10 &lt;0.0001 \nPerson=it                    0.7897 0.2978   2.65 0.0080  \nPerson=there                -2.5641 1.0095  -2.54 0.0111  \nRegister=S1B * Variety=HK    0.6035 0.4521   1.34 0.1819  \nRegister=S1B * Variety=SING -0.4753 0.4688  -1.01 0.3107  \n\n\n\n# With (glm); available in base R\n# Note the additional \"family\" argument!\nReference.glm &lt;- glm(Reference ~ Register + Variety + Register:Variety + Person, data = data_pro, family = \"binomial\")\n\n# View model statistics\nsummary(Reference.glm)\n\n\ntab_model(Reference.glm, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, show.fstat = TRUE, transform = NULL)\n\n\n\n\n\n\n\n\n\n\n\n \nReference\n\n\nPredictors\nLog-Odds\nstd. Error\nCI\np\n\n\n(Intercept)\n-3.41\n0.27\n-3.99 – -2.91\n&lt;0.001\n\n\nRegister [S1B]\n0.03\n0.38\n-0.74 – 0.77\n0.944\n\n\nVariety [HK]\n0.67\n0.32\n0.06 – 1.32\n0.034\n\n\nVariety [SING]\n1.12\n0.30\n0.56 – 1.73\n&lt;0.001\n\n\nPerson [1]\n-0.88\n0.18\n-1.24 – -0.53\n&lt;0.001\n\n\nPerson [2]\n-1.64\n0.27\n-2.21 – -1.14\n&lt;0.001\n\n\nPerson [it]\n0.79\n0.30\n0.18 – 1.35\n0.008\n\n\nPerson [there]\n-2.56\n1.01\n-5.44 – -1.05\n0.011\n\n\nRegister [S1B] × Variety\n[HK]\n0.60\n0.45\n-0.28 – 1.50\n0.182\n\n\nRegister [S1B] × Variety\n[SING]\n-0.48\n0.47\n-1.40 – 0.45\n0.311\n\n\nObservations\n4838\n\n\nR2 Tjur\n0.030\n\n\nDeviance\n1378.406\n\n\nAIC\n1398.406\n\n\n\n\n\n\n\n\n\n\n\n\n\nStepwise variable selection\n\n\n\nWith the function drop1(), it is possible to successively remove variables from the complex model to ascertain which ones improve the model significantly (i.e., decrease the deviance and AIC scores).\n\ndrop1(Reference.glm, test = \"Chisq\")\n\nSingle term deletions\n\nModel:\nReference ~ Register + Variety + Register:Variety + Person\n                 Df Deviance    AIC    LRT Pr(&gt;Chi)    \n&lt;none&gt;                1378.4 1398.4                    \nPerson            4   1460.2 1472.2 81.828  &lt; 2e-16 ***\nRegister:Variety  2   1387.5 1403.5  9.100  0.01057 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n22.4.4 Step 4: Confidence intervals and odds ratios\n\n# Tidy the model output\ntidy_model &lt;- tidy(Reference.glm, conf.int = TRUE)\n\n# Remove intercept, compute odds ratios and their CIs\ntidy_model &lt;- tidy_model %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(\n    odds_ratio = exp(estimate),\n    odds.conf.low = exp(conf.low),\n    odds.conf.high = exp(conf.high)\n  )\n\n\n\n22.4.5 Step 5: Visualise the model\n\nPlot model coefficients:\n\n\n\nCode\n# Create the coefficient plot\nggplot(tidy_model, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient Estimate (log-odds)\",\n    y = \"Predictor\",\n    title = \"Coefficient Estimates with Confidence Intervals\",\n    caption = \"*Note that the CIs of singificant predictors do not include 0.\"\n  )\n\n\n\n\n\n\n\n\n\nCode\n# Plot odds ratios\nggplot(tidy_model, aes(x = exp(estimate), y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = odds.conf.low, xmax = odds.conf.high), height = 0.2) +\n  geom_vline(xintercept = 1, linetype = \"dashed\", color = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient Estimate (odds ratios)\",\n    y = \"Predictor\",\n    title = \"Odds ratios with Confidence Intervals\",\n    caption = \"*Note that the CIs of singificant predictors do not include 1.\"\n  )\n\n\n\n\n\n\n\n\n\n\nPlot predicted probabilities:\n\n\n\nCode\n# Use ggeffect() from the ggeffects package\nplot(ggeffect(Reference.glm, terms = c(\"Register\"))) + geom_line(col = \"steelblue\")\n\n\n\n\n\n\n\n\n\nCode\nplot(ggeffect(Reference.glm, terms = c(\"Variety\"))) + geom_line(col = \"steelblue\")\n\n\n\n\n\n\n\n\n\nCode\nplot(ggeffect(Reference.glm, terms = c(\"Person\"))) + geom_line(col = \"steelblue\")\n\n\n\n\n\n\n\n\n\n\n\n22.4.6 Step 6: Interpret the model\nThe logistic regression model is statistically significant at \\(p &lt; 0.001\\) (\\(\\chi^2 = 120.43\\), \\(df = 9\\)) and has acceptable fit (Nagelkerke’s-\\(R^2\\) = \\(0.09\\), \\(C = 0.73\\)).\nThe model coefficients indicate that null subjects are significantly more likely in Singapore English compared to British English (Estimate = 1.12, 95% CI [0.56, 1.73], \\(p &lt; 0.001\\)). This effect is moderate with an \\(OR\\) of 3.06 (95% CI [1.75, 5.64]), suggesting that the probability of subject omission is elevated by a factor of approximately 3 in the Singaporean variety.\n…\n\n\n22.4.7 Step 7: Further model diagnostics\n\nCross-validation\n\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Refit the model with additional settings\nReference.val &lt;- lrm(Reference ~ Register + Variety + Register:Variety + Person, data = data_pro, x = T, y = T)\n\n# Perform 200-fold cross-validation\nmodel.validated &lt;- validate(Reference.val, B = 200)\n\n# Slope optimism should be as low possible!\nmodel.validated\n\n\n          index.orig training    test optimism index.corrected   n\nDxy           0.4592   0.4655  0.4456   0.0200          0.4393 200\nR2            0.0923   0.0981  0.0843   0.0138          0.0785 200\nIntercept     0.0000   0.0000 -0.2177   0.2177         -0.2177 200\nSlope         1.0000   1.0000  0.9262   0.0738          0.9262 200\nEmax          0.0000   0.0000  0.0622   0.0622          0.0622 200\nD             0.0247   0.0263  0.0225   0.0038          0.0208 200\nU            -0.0004  -0.0004  0.0002  -0.0006          0.0002 200\nQ             0.0251   0.0268  0.0223   0.0045          0.0206 200\nB             0.0336   0.0336  0.0337  -0.0001          0.0337 200\ng             1.0081   1.2487  1.1361   0.1127          0.8954 200\ngp            0.0319   0.0326  0.0303   0.0022          0.0297 200\n\n\n\nMulticollinearity\n\n\n\nCode\n# Variable inflation factors further reveal severe multicollinearity\nvif(Reference.lrm)\n\n\n               Register=S1B                  Variety=HK \n                   5.818111                    4.006084 \n               Variety=SING                    Person=1 \n                   3.421687                    1.140407 \n                   Person=2                   Person=it \n                   1.089502                    1.102908 \n               Person=there   Register=S1B * Variety=HK \n                   1.007148                    6.218803 \nRegister=S1B * Variety=SING \n                   3.685075 \n\n\n\n\n\n\nAgresti, Alan, and Maria Kateri. 2022. Foundations of Statistics for Data Scientists: With r and Python. Boca Raton: CRC Press.\n\n\nBuskin, Vladimir. n.d. “Definite Null Instantiation in English(es): A Usage-based Construction Grammar Approach.” Constructions and Frames.\n\n\nHosmer, David W., and Stanley Lemeshow. 2008. Applied Logistic Regression. 2nd ed. New York: Wiley.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. New York: Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nLevshina, Natalia. 2015. How to Do Linguistics with r: Data Exploration and Statistical Analysis. Amsterdam; Philadelphia: John Benjamins Publishing Company.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using r. New York; London: Routledge.\n\n\nWood, Simon N. 2006. Generalized Additive Models: An Introduction with R. Boca Raton: Chapman & Hall/CRC.",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Linear_regression.html#recommended-reading",
    "href": "Linear_regression.html#recommended-reading",
    "title": "21  Linear regression",
    "section": "21.1 Recommended reading",
    "text": "21.1 Recommended reading\nFor linguists:\n\nLevshina (2015: Chapter 7)\nWinter (2020: Chapter 4)\n\nGeneral:\n\nHeumann et al. (2022: Chapter 11)\nJames et al. (2021: Chapter 3)",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "Linear_regression.html#preparation",
    "href": "Linear_regression.html#preparation",
    "title": "21  Linear regression",
    "section": "21.2 Preparation",
    "text": "21.2 Preparation\n\n# Load libraries\nlibrary(readxl) # for reading in Excel data\nlibrary(tidyverse) # data manipulation and visualisation framework\nlibrary(broom) # converting models to data frames\nlibrary(sjPlot) # exporting regression tables\nlibrary(effects) # plot marginal effects\nlibrary(ggeffects) # generating predictions\nlibrary(car) # model diagnostics\n\n# Load data\nELP &lt;- read_xlsx(\"ELP.xlsx\")\n\n# Inspect data structure\nstr(ELP)\n\ntibble [880 × 5] (S3: tbl_df/tbl/data.frame)\n $ Word  : chr [1:880] \"rackets\" \"stepmother\" \"delineated\" \"swimmers\" ...\n $ Length: num [1:880] 7 10 10 8 6 5 5 8 8 6 ...\n $ Freq  : num [1:880] 0.96 4.24 0.04 1.49 1.06 3.33 0.1 0.06 0.43 5.41 ...\n $ POS   : chr [1:880] \"NN\" \"NN\" \"VB\" \"NN\" ...\n $ RT    : num [1:880] 791 693 960 771 882 ...",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "Linear_regression.html#introduction",
    "href": "Linear_regression.html#introduction",
    "title": "21  Linear regression",
    "section": "21.3 Introduction",
    "text": "21.3 Introduction\nConsider the distribution of the variables RT (reaction times) and Freq from the ELP (English Lexicon Project) dataset (Balota et al. 2007).\nWe will apply a \\(\\log\\)-transformation to both variables in order to even out the differences between extremely high and extremely low frequency counts (Winter 2020: 90-94).\n\nLog-transformedUntransformed\n\n\n\n\nCode\n# Log-transformed\nggplot(ELP, aes(x = log(RT))) +\n  geom_histogram() +\n  geom_vline(xintercept = mean(log(ELP$RT)), color = \"steelblue\") +\n  geom_vline(xintercept = mean(log(ELP$RT)), color = \"red\") +\n  theme_minimal() +\n  labs(\n    x = \"Reaction time (log)\"\n  ) +\n   annotate(\"text\", x = log(mean(ELP$RT)), y = 0, \n           label = \"mean\", \n           vjust = 1.5, hjust = -0.2, color = \"steelblue\", parse = TRUE) +\n  annotate(\"text\", x = log(mean(ELP$RT)) + -0.2, y = .7, \n           label = \"median\", \n           vjust = 1.5, hjust = -0.2, color = \"red\", parse = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Skewed\nggplot(ELP, aes(x = RT)) +\n  geom_histogram() +\n  geom_vline(xintercept = mean(ELP$RT), color = \"steelblue\") +\n  geom_vline(xintercept = median(ELP$RT), color = \"red\") +\n  theme_minimal() +\n  labs(\n    x = \"Reaction time\"\n  ) +\nannotate(\"text\", x = mean(ELP$RT) + 10, y = 0, \n           label = \"mean\", \n           vjust = 1.5, hjust = -0.2, color = \"steelblue\", parse = TRUE) +\n  annotate(\"text\", x = median(ELP$RT) -175, y = .7, \n           label = \"median\", \n           vjust = 1.5, hjust = -0.2, color = \"red\", parse = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nWe are particularly interested in the relationship between reaction times RT and the (log-)frequency Freq of a lexical stimulus. What kind of pattern does the scatter plot below suggest?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome open questions\n\n\n\n\n\n\nCan word frequency help us explain variation in reaction times?\nIf it can, then how could we characterise the effect of word frequency? In other words, does it increase or decrease reaction times?\nWhat reaction times should we expect for new observations?\n\n\n\n\n\n21.3.1 A simple statistical model\nRT is the response or target that we wish to explain. We generically refer to the response as \\(Y\\).\nFreq is the feature, input, or predictor, which we will call \\(X\\).\nWe can thus summarise our preliminary and fairly general statistical model as\n\\[Y = f(X) + \\epsilon.\n\\tag{21.1}\\]\nThe term \\(f(X)\\) describes the contribution of \\(X\\) to the explanation of \\(Y\\). Since no model can explain everything perfectly, we expect there to be some degree of error \\(\\epsilon\\).",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "Linear_regression.html#linear-regression",
    "href": "Linear_regression.html#linear-regression",
    "title": "21  Linear regression",
    "section": "21.4 Linear regression",
    "text": "21.4 Linear regression\n\nLinear regression is a simple approach to supervised machine learning where the response variable is known.1\nIt assumes that the dependence of \\(Y\\) on \\(X\\) is linear, i.e., their relationship is a straight line.\nThis approach is suitable for numerical response variables. The predictors, however, can be either continuous or discrete.\n\n1 If the response variable is unknown or irrelevant, we speak of unsupervised machine learning. Unsupervised models are mostly concerned with finding patterns in high-dimensional datasets with dozens or even hundreds of variables.Although it may seem overly simplistic, linear regression is extremely useful both conceptually and practically.\n\n21.4.1 Model with a single predictor \\(X\\)\nThe simple linear model has the general form\n\\[ Y = \\beta_0 + \\beta_1X + \\epsilon.\n\\tag{21.2}\\]\n\nThe model parameters (or coefficients) \\(\\beta_0\\) and \\(\\beta_1\\) specify the functional relationship \\(f\\) between \\(Y\\) and \\(X\\).\nThe first parameter \\(\\beta_0\\) determines the intercept of the regression line, and \\(\\beta_1\\) indicates the slope.\nOnce again, \\(\\epsilon\\) captures the model error, which is equivalent to the sum of all distances of the data points from the regression line.\n\n\n\n\n\n\n\n\n\n\nApplying the model formula to our dataset, we get the following updated regression equation:\n\\[ \\text{Reaction time} = \\beta_0 + \\beta_1\\text{Frequency} + \\text{Model Error}. \\]\nBut how do we find the exact values of the intercept and the slope? In short: We can’t! We are dealing with population parameters and can, therefore, only provide an approximation of the true relationship between RT and Freq.\nTo reflect the tentative nature of the model coefficients, we use the hat symbol ^ (e.g., \\(\\hat{\\beta_0}\\)) to indicate estimations rather than true values. The estimation procedure requires training data, based on which the algorithm “learns” the relationship between \\(Y\\) and \\(X\\) (hence the term “Machine Learning”).\n\n\n\n\n\n\nHow exactly do you estimate the model parameters?\n\n\n\n\n\nThe most common way of estimating parameters for linear models is the Least Squares approach. In essence, the parameters are chosen such that the residual sum of squares, i.e., the sum of the differences between observed and predicted values, is as low as possible. In other words, we are trying to minimise the distances between the data points and the regression line.\nIt can be computed using the equivalence in Equation 21.3.\n\\[ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i- \\bar{y})}{\\sum_{i=1}^n(x_i- \\bar{x})^2}.\n\\tag{21.3}\\]\nWe can then obtain the intercept via Equation 21.4.\n\\[\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta}_1\\bar{x}\n\\tag{21.4}\\]\n\n\n\nOnce we’ve fitted the model, we can then predict reaction times if we know the frequency of a lexical stimulus:\n\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x,\n\\tag{21.5}\\]\nwhere \\(\\hat{y}\\) indicates a prediction of \\(Y\\) on the basis of the predictor values \\(X = x\\).\n\n\n21.4.2 Application in R\nIn R, we can fit a linear model with the lm() function.\n\n# Fit linear model\nrt.lm1 = lm(log(RT) ~ log(Freq), data = ELP)\n\n# View model data\nsummary(rt.lm1)\n\n\ntab_model(rt.lm1, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, show.fstat = TRUE, digits = 3)\n\n\n\n\n \nlog(RT)\n\n\nPredictors\nEstimates\nstd. Error\nCI\np\n\n\n(Intercept)\n6.633\n0.004\n6.625 – 6.642\n&lt;0.001\n\n\nFreq [log]\n-0.049\n0.002\n-0.053 – -0.044\n&lt;0.001\n\n\nObservations\n880\n\n\nR2 / R2 adjusted\n0.357 / 0.356\n\n\nDeviance\n13.385\n\n\nAIC\n10534.255\n\n\n\n\n\n\n\nThe model statistics comprise the following elements:\n\n\n\n\n\n\nCall\n\n\n\n\n\ni.e., the model formula.\n\n\n\n\n\n\n\n\n\nResiduals\n\n\n\n\n\nThese indicate the difference between the observed values in the data set and the values predicted by the model (= the fitted values). These correspond to the error term \\(\\epsilon\\). The lower the residuals, the better the model describes the data.\n\n# Show fitted values (= predictions) for the first six observations\nhead(rt.lm1$fitted.values)\n\n       1        2        3        4        5        6 \n6.635345 6.563152 6.789806 6.613980 6.630529 6.574894 \n\n# Show deviation of the fitted values from the observed values\nhead(rt.lm1$residuals)\n\n          1           2           3           4           5           6 \n 0.03778825 -0.02277165  0.07759556  0.03387713  0.15222949 -0.10432670 \n\n\n\n\n\n\n\n\n\n\n\nCoefficients\n\n\n\n\n\nThe regression coefficients correspond to \\(\\hat{\\beta}_0\\) (“Intercept”) and \\(\\hat{\\beta}_1\\) (“log(Freq)”), respectively. The model shows that for a one-unit increase in log-frequency the log-reaction time decreases by approx. 0.05.\n\n# Convert coefficients to a tibble (= tidyverse-style data frame)\ntidy_model &lt;- tidy(rt.lm1)\n\ntidy_model\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   6.63     0.00429    1548.  0       \n2 log(Freq)    -0.0486   0.00220     -22.1 2.88e-86\n\n\n\n\n\n\n\n\n\n\n\n\\(p\\)-values and \\(t\\)-statistic\n\n\n\n\n\n\\(p\\)-values and \\(t\\)-statistic: Given the null hypothesis \\(H_0\\) that there is no correlation between log(RT) and log(Freq) (i.e., \\(H_0: \\beta_1 = 0\\)), a \\(p\\)-value lower than 0.05 indicates that \\(\\beta_1\\) considerably deviates from 0, thus providing evidence for the alternative hypothesis \\(H_1: \\beta_1 \\ne 0\\). Since \\(p &lt; 0.001\\), we can reject \\(H_0\\).\nThe \\(p\\)-value itself crucially depends on the \\(t\\)-statistic2, which measures “the number of standard deviations that \\(\\hat{\\beta_1}\\) is away from 0” (James et al. 2021: 67). The standard error (SE) reflects how much an estimated coefficient differs on average from the true values of \\(\\beta_0\\) and \\(\\beta_1\\). They can be used to compute the 95% confidence interval \\[[\\hat{\\beta}_1 - 2 \\cdot SE(\\hat{β}_1), \\hat{\\beta}_1 + 2 \\cdot SE(\\hat{\\beta}_1)].\n\\tag{21.6}\\]\nThe true value of the parameter \\(\\beta_1\\) lies within the specified range 95% of the time.\n\n# Compute confidence intervals for intercept and log(Freq)\ntidy_model_ci &lt;- tidy(rt.lm1, conf.int = TRUE)\n\ntidy_model_ci\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   6.63     0.00429    1548.  0          6.62      6.64  \n2 log(Freq)    -0.0486   0.00220     -22.1 2.88e-86  -0.0529   -0.0443\n\n\nThe estimated parameter for log(Freq), which is -0.049, thus has the 95% confidence interval [-0.053, -0.044].\n\n\n\n2 If the response variable is unknown or irrelevant, we speak of unsupervised machine learning. Unsupervised models are mostly concerned with finding patterns in high-dimensional datasets with dozens or even hundreds of variables.\n\n\n\n\n\nResidual standard error (RSE)\n\n\n\n\n\nThis is an estimation of the average deviation of the predictions from the observed values.\n\\[RSE = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n (y_i - \\hat{y_i}})^2\n\\tag{21.7}\\]\n\n\n\n\n\n\n\n\n\n\\(R^2\\)\n\n\n\n\n\nThe \\(R^2\\) score is important for assessing model fit because it “measures the proportion of variability in \\(Y\\) that can be explained using \\(X\\)” (James et al. 2021: 70), varying between 0 and 1.\n\\[R^2 = 1-\\frac{TSS}{RSS} = 1-\\frac{\\sum_{i=1}^n (y_i - \\hat{y_i})^2}{\\sum_{i=1}^n (y_i - \\bar{y_i})^2}\n\\tag{21.8}\\]\n\n\n\n\n\n\n\n\n\n\\(F\\)-statistic\n\n\n\n\n\nIt is used to measure the association between the dependent variable and the independent variable(s). Generally speaking, values greater than 1 indicate a possible correlation. A sufficiently low \\(p\\)-value suggests that the null hypothesis \\(H_0: \\beta_1 = 0\\) can be rejected. The \\(F\\) statistic is computed as shown below (cf. Agresti and Kateri 2022: 232) and follows an \\(F\\)-distribution with two different \\(df\\) values.\n\\[\nF = \\frac{(TSS - SSE) / p}{SSE / [n - (p + 1)]}\n\\tag{21.9}\\]\n\n\n\n\n\n21.4.3 Multiple linear regression\nIn multiple linear regression, more than one predictor variable is taken into account. For instance, modelling log(RT) as a function of log(Freq), POS and Length requires a more complex model of the form\n\\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon.\n\\tag{21.10}\\]\nPredictions are then obtained via the formula\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2 + ... + \\hat{\\beta}_px_p.\n\\tag{21.11}\\]\n\n\n21.4.4 Application in R\nIn R, a multiple regression model is fitted as in the code example below:\n\n# Fit multiple regression model\nrt.lm2 &lt;- lm(log(RT) ~ log(Freq) + POS + Length, data = ELP)\n\n# View model statistics\nsummary(rt.lm2)\n\n\ntab_model(rt.lm2, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, show.fstat = TRUE, digits = 3)\n\n\n\n\n \nlog(RT)\n\n\nPredictors\nEstimates\nstd. Error\nCI\np\n\n\n(Intercept)\n6.460\n0.017\n6.426 – 6.493\n&lt;0.001\n\n\nFreq [log]\n-0.038\n0.002\n-0.042 – -0.034\n&lt;0.001\n\n\nPOS [NN]\n-0.006\n0.010\n-0.026 – 0.014\n0.539\n\n\nPOS [VB]\n-0.035\n0.012\n-0.059 – -0.011\n0.004\n\n\nLength\n0.023\n0.002\n0.020 – 0.026\n&lt;0.001\n\n\nObservations\n880\n\n\nR2 / R2 adjusted\n0.478 / 0.476\n\n\nDeviance\n10.858\n\n\nAIC\n10356.130",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "Linear_regression.html#visualising-regression-models",
    "href": "Linear_regression.html#visualising-regression-models",
    "title": "21  Linear regression",
    "section": "21.5 Visualising regression models",
    "text": "21.5 Visualising regression models\n\nPlot coefficient estimates:\n\n\n\nCode\n# Tidy the model output\ntidy_model &lt;- tidy(rt.lm2, conf.int = TRUE)\n\n# Remove intercept\ntidy_model &lt;- tidy_model %&gt;% filter(term != \"(Intercept)\")\n\n# Create the coefficient plot\nggplot(tidy_model, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient Estimate\",\n    y = \"Predictor\",\n    title = \"Coefficient Estimates with Confidence Intervals\"\n  )\n\n\n\n\n\n\n\n\n\n\nPlot predictions:\n\n\n\nCode\nplot(ggeffect(rt.lm2, \"Freq\"), residuals = TRUE) + geom_line(col = \"steelblue\") + labs(subtitle = \"Untransformed frequencies\", y = \"log(RT)\")\n\n\n\n\n\n\n\n\n\nCode\nplot(ggeffect(rt.lm2, \"Freq [log]\"), residuals = TRUE) + geom_line(col = \"steelblue\") + labs(subtitle = \"Log-transformed frequencies\", y = \"log(RT)\")\n\n\n\n\n\n\n\n\n\nCode\nplot(ggeffect(rt.lm2, \"POS\"), residuals = TRUE) + geom_line(col = \"steelblue\") + labs(y = \"log(RT)\")\n\n\n\n\n\n\n\n\n\nCode\nplot(ggeffect(rt.lm2, \"Length\"), residuals = TRUE) + geom_line(col = \"steelblue\") + labs(y = \"log(RT)\")",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "Linear_regression.html#model-assumptions-and-diagnostics",
    "href": "Linear_regression.html#model-assumptions-and-diagnostics",
    "title": "21  Linear regression",
    "section": "21.6 Model assumptions and diagnostics",
    "text": "21.6 Model assumptions and diagnostics\nAs a parametric method, linear regression makes numerous assumptions about the training data. It is, therefore, essential to run further tests to rule out possible violations. Among other things, the model assumptions include:\n\nA linear relationship between the response and the quantitative predictors: The residuals should not display a clear pattern. For this reason, it is recommended to use component residual plots (e.g., crPlot() from the car library) for the visual identification of potentially non-linear trends.\n\n\n\nCode\n# pink line = main tendency vs. blue line = slope coefficients;\n# some minor non-linearity can be observed\n\ncrPlot(rt.lm2, var = \"log(Freq)\") \n\n\n\n\n\n\n\n\n\nCode\ncrPlot(rt.lm2, var = \"POS\")\n\n\n\n\n\n\n\n\n\nCode\ncrPlot(rt.lm2, var = \"Length\") # potentially problematic\n\n\n\n\n\n\n\n\n\n\nNo heteroscedasticity (i.e, non-constant variance of error terms): Visually, a violation of this assumption becomes apparent if the residuals form a funnel-like shape. It is also possible to conduct a non-constant variance test ncvTest(): If it returns \\(p\\)-values &lt; 0.05, it suggests non-constant variance.\n\n\n\nCode\nplot(rt.lm2, which = 1)\n\n\n\n\n\n\n\n\n\nCode\nncvTest(rt.lm2) # significant, meaning that errors do not vary constantly\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 30.0101, Df = 1, p = 4.298e-08\n\n\n\nNo multicollinearity: Predictors should not be correlated with each other. In the model data, correlated variables have unusually high standard errors, thereby decreasing the explanatory power of both the coefficients and the model as a whole. Another diagnostic measure are variance inflation factors (VIF-scores); predictors with VIF scores &gt; 5 are potentially collinear. They can be computed using the vif() function.\n\n\n\nCode\nvif(rt.lm2) # vif &lt; 5 indicates that predictors are not correlated\n\n\n              GVIF Df GVIF^(1/(2*Df))\nlog(Freq) 1.150140  1        1.072446\nPOS       1.026925  2        1.006664\nLength    1.151054  1        1.072872\n\n\n\nNormally distributed residuals: The residuals should follow the normal distribution and be centered around 0:\n\n\\[\n\\epsilon \\sim N(0, \\sigma^2)\n\\tag{21.12}\\]\nUsually, a visual inspection using qqnorm() is sufficient, but the Shapiro-Wilke test shapiro.test() can also be run on the model residuals. Note that a \\(p\\)-value below 0.05 provides evidence for non-normality.\n\n\nCode\nplot(rt.lm2, which = 2)\n\n\n\n\n\n\n\n\n\nCode\nshapiro.test(residuals(rt.lm2)) # residuals are not normally distributed because p &lt; 0.05\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(rt.lm2)\nW = 0.99062, p-value = 2.139e-05\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBeside the points mentioned above, it is always recommend to examine the model with regard to\n\noutliers that might skew the regression estimates,\n\n\n\nCode\ninfluencePlot(rt.lm2, id.method = \"identify\")\n\n\n\n\n\n\n\n\n\n       StudRes         Hat       CookD\n16   0.4910868 0.027128347 0.001346143\n207 -0.7674391 0.030963411 0.003765568\n452  3.3366052 0.009338916 0.020749639\n498  3.5794954 0.004047706 0.010275907\n660  3.0847082 0.008750676 0.016638370\n\n\n\ninteractions, i.e., combined effects of predictors, and\n\n\n\nCode\nrt.lm.int &lt;- lm(log(RT) ~ log(Freq) + POS + Length + log(Freq):Length, data = ELP)\n\nsummary(rt.lm.int)\n\n\n\nCall:\nlm(formula = log(RT) ~ log(Freq) + POS + Length + log(Freq):Length, \n    data = ELP)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27355 -0.07723 -0.00651  0.06623  0.39971 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       6.4669304  0.0171836 376.343  &lt; 2e-16 ***\nlog(Freq)        -0.0243748  0.0062631  -3.892 0.000107 ***\nPOSNN            -0.0054973  0.0101366  -0.542 0.587735    \nPOSVB            -0.0344559  0.0120992  -2.848 0.004506 ** \nLength            0.0217620  0.0018007  12.085  &lt; 2e-16 ***\nlog(Freq):Length -0.0017681  0.0007606  -2.325 0.020319 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1111 on 874 degrees of freedom\nMultiple R-squared:  0.4816,    Adjusted R-squared:  0.4786 \nF-statistic: 162.4 on 5 and 874 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# ANOVA (analysis of variance)\n\n## Compare interaction model with main effects model\n\nanova(rt.lm.int, rt.lm2) # interaction term improves the model\n\n\nAnalysis of Variance Table\n\nModel 1: log(RT) ~ log(Freq) + POS + Length + log(Freq):Length\nModel 2: log(RT) ~ log(Freq) + POS + Length\n  Res.Df    RSS Df Sum of Sq     F  Pr(&gt;F)  \n1    874 10.792                             \n2    875 10.858 -1 -0.066726 5.404 0.02032 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\noverfitting, which results in poor model performance outside the training data.\n\n\n\nCode\nlibrary(\"rms\")\n\n# Refit the model with ols(), which is equivalent to lm()\nols.rt &lt;- ols(log(RT) ~ log(Freq) + POS + Length, data = ELP, x = TRUE, y = TRUE)\n\n# Cross-validate\nols.val &lt;- validate(ols.rt, bw = TRUE, B = 200) # Perform 200 random resampling iterations (= bootstrapping); compare model performance on training vs. test (= new) data. The slope optimism should be below 0.05 to rule out overfitting.\n\n\n\n        Backwards Step-down - Original Model\n\nNo Factors Deleted\n\nFactors in Final Model\n\n[1] Freq   POS    Length\n\n\nCode\nols.val[,1:5] # The model does not overfit.\n\n\n          index.orig  training       test      optimism index.corrected\nR-square  0.47839094 0.4796176 0.47570919  0.0039084156      0.47448252\nMSE       0.01233904 0.0122714 0.01240248 -0.0001310708      0.01247011\ng         0.11875152 0.1187177 0.11856383  0.0001538558      0.11859767\nIntercept 0.00000000 0.0000000 0.01919518 -0.0191951836      0.01919518\nSlope     1.00000000 1.0000000 0.99706225  0.0029377543      0.99706225\n\n\n\n\n\n\n\n\nAgresti, Alan, and Maria Kateri. 2022. Foundations of Statistics for Data Scientists: With r and Python. Boca Raton: CRC Press.\n\n\nBalota, David A., Melvin J. Yap, Keith A. Hutchison, Michael J. Cortese, Brett Kessler, Bjorn Loftis, James H. Neely, Douglas L. Nelson, Greg B. Simpson, and Rebecca Treiman. 2007. “The English Lexicon Project.” Behavior Research Methods 39 (3): 445–59. https://doi.org/10.3758/BF03193014.\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. New York: Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nLevshina, Natalia. 2015. How to Do Linguistics with r: Data Exploration and Statistical Analysis. Amsterdam; Philadelphia: John Benjamins Publishing Company.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using r. New York; London: Routledge.",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "Mixed_effects_regression.html#recommended-reading",
    "href": "Mixed_effects_regression.html#recommended-reading",
    "title": "23  Mixed-effects regression",
    "section": "23.1 Recommended reading",
    "text": "23.1 Recommended reading\nFor linguists:\n\nSchäfer (2020)\n\nGeneral:\n\nGelman and Hill (2007)",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Mixed-effects regression</span>"
    ]
  },
  {
    "objectID": "Mixed_effects_regression.html#preparation",
    "href": "Mixed_effects_regression.html#preparation",
    "title": "23  Mixed-effects regression",
    "section": "23.2 Preparation",
    "text": "23.2 Preparation\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(tidyr)\nlibrary(lme4) # for linear mixed-effects models\nlibrary(sjPlot)\nlibrary(ggeffects)\nlibrary(ggpubr)\n\n# Load data\nvarmorph &lt;- read.csv(\"varmorph_data.csv\", header = TRUE)\n\n# Reduce data\nvarmorph %&gt;%\n  select(rt, target, prime_type, subj_id) %&gt;%\n  filter(prime_type != \"filler\") %&gt;% \n  drop_na() -&gt; varmorph2\n\n# Overview\nglimpse(varmorph2)\n\nRows: 7,038\nColumns: 4\n$ rt         &lt;dbl&gt; 599.63, 885.39, 1124.94, 568.68, 726.24, 1095.37, 492.96, 6…\n$ target     &lt;chr&gt; \"print\", \"defend\", \"tempt\", \"hunt\", \"staple\", \"pose\", \"kick…\n$ prime_type &lt;chr&gt; \"derived\", \"derived\", \"derived\", \"derived\", \"derived\", \"der…\n$ subj_id    &lt;chr&gt; \"3202\", \"3202\", \"3202\", \"3202\", \"3202\", \"3202\", \"3202\", \"32…",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Mixed-effects regression</span>"
    ]
  },
  {
    "objectID": "Mixed_effects_regression.html#sec-mer",
    "href": "Mixed_effects_regression.html#sec-mer",
    "title": "23  Mixed-effects regression",
    "section": "23.3 Multilevel models",
    "text": "23.3 Multilevel models\nAt their core, mixed-effects models “are extensions of regression in which data are structured in groups and coefficients can vary by group” (Gelman and Hill 2007: 237). Typical grouping structures found in linguistic data include speakers, regions, or lexical stimuli for which multiple observations are attested. Normally, such structures would violate the assumption of independence, but can be controlled for by capturing group-wise tendencies.\nFor illustration, a simple example of a hierarchical dataset is presented in Figure 23.1. If one were to, for instance, measure test scores for every student, it may be of interest how their performance varies not only from student to student but also from school to school. After all, the students are nested within their schools.\n\n\n\n\n\n\n\ngraph LR\n    A1[School 1] --&gt; B11(Student 1)\n    A1[School 1] --&gt; B12(Student 2)\n    A1[School 1] --&gt; B13(Student 3)\n    \n    A2[School 2] --&gt; B21(Student 4)\n    A2[School 2] --&gt; B22(Student 5)\n    A2[School 2] --&gt; B23(Student 6)\n    \n\n\n\n\n\n\n\nFigure 23.1\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nRead the following (partial) description of the experiments conducted by Ciaccio & Veríssimo on the morphological processing of complex lexical items (2022):\n\nSixty-nine intermediate to advanced non-native speakers of English (54 women; 15 men) took part in the experiment in exchange for payment or course credits. […] The experiment included 102 English monomorphemic verbs used as targets (e.g., print). These were preceded by their -ed past-tense form (e.g., printed) as the inflected prime, their -er nominalization (e.g., printer) as the derived prime, or by an unrelated prime. Unrelated primes were dissimilar in form and meaning from their corresponding targets; half of them were -ed inflected forms and half of them were -er derived words. (Ciaccio and Veríssimo 2022: 2267)\n\nInspect varmorph2 and characterise its multilevel structure.\n\n\n\n23.3.1 Types of mixed-effects models\nVariance across groups can be captured by varying-intercept and/or varying-slope models. These varying coefficients also known as random effects (cf. Gelman and Hill (2007): 245). In the model equation, the intercept \\(\\alpha\\) and/or the slope \\(\\beta\\) is additionally indexed for the grouping factor. Let \\(J\\) denote the number of groups for \\(j = 1, ..., J\\).\n\n\n\n\n\n\nVarying-intercept model\n\n\n\nWe allow group-wise variation in the intercept by replacing \\(\\alpha\\) with \\(\\alpha_{j}\\) to indicate the intercept for the \\(j\\)-th group. It is defined as a random variable and follows the normal distribution. For instance, each participant in the aforementioned psycholinguistic would receive its own intercept rather than a global one for all participants.\n\\[\nY = \\alpha_{j} + \\beta_1X_{1} + \\beta_2X_{2} + ... + \\epsilon \\qquad \\alpha_{j} \\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)\n\\tag{23.1}\\]\n\n\n\n\n\n\n\n\nVarying-slope model\n\n\n\nWe will allow group-wise variation in the slope coefficients by replacing them with \\(\\beta_{ij}\\) to indicate the slope for the \\(j\\)-th group. The slope now functions as a random variable and is normally distributed. In the psycholinguistic study, each participant would be assigned its own slope coefficient.\n\\[\nY = \\alpha + \\beta_{1j}X_{1} + \\beta_{2j}X_{2} + ... + \\epsilon \\qquad \\beta_{j} \\sim N(\\mu_{\\beta}, \\sigma_{\\beta}^2)\n\\tag{23.2}\\]\n\n\n\n\n23.3.2 Example\nAssume we are predicting test performance by School. Using simulated data, the following series of plots plots illustrate …\n\n… School as a fixed effect,\n… random intercepts for each School,\n… random slopes for each School, and\n… random intercepts and slopes for each School.\n\n\n\n\n\n\n\nPlot: (1) Fixed-effects model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot: (2) Varying-intercept model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot: (3) Varying-slope model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot: (4) Varying-intercept and varying-slope model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n23.3.3 Linear mixed-effects models\n\n\n23.3.4 Application in R\n\n23.3.4.1 Varying-intercept model\n\n# Varying intercept model\n\n# Define reference level for \"prime_type\"\nvarmorph2$prime_type &lt;- factor(varmorph2$prime_type, levels = c(\"unrelated\", \"derived\", \"inflected\"))\n\n# Fit mixed-effects models\nvarmorph.me &lt;- lmer(rt ~ prime_type + # fixed effect\n                      (1 | subj_id) + # let intercept vary by subject\n                      (1 | target), # # let intercept vary by target word\n                      data = varmorph2)\n\n# Summarise results\nsummary(varmorph.me)\n\n\ntab_model(varmorph.me, show.se = TRUE, show.aic = TRUE, show.dev = TRUE)\n\n\n\n \nrt\n\n\nPredictors\nEstimates\nstd. Error\nCI\np\n\n\n(Intercept)\n715.70\n12.66\n690.89 – 740.52\n&lt;0.001\n\n\nprime type [derived]\n-31.31\n5.01\n-41.14 – -21.48\n&lt;0.001\n\n\nprime type [inflected]\n-33.96\n5.01\n-43.78 – -24.13\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n29483.89\n\n\n\nτ00 target\n3690.56\n\n\nτ00 subj_id\n7693.61\n\n\nICC\n0.28\n\n\nN subj_id\n69\n\n\nN target\n102\n\nObservations\n7038\n\n\nMarginal R2 / Conditional R2\n0.006 / 0.283\n\n\nDeviance\n92860.321\n\n\nAIC\n92855.624\n\n\n\n\n\n\n\n\n\n\n\n\nICC\n\n\n\nThe intraclass correlation coefficient (ICC) “ranges from \\(0\\) if the grouping conveys no information to \\(1\\) if all members of a group are identical” (Gelman and Hill 2007: 258). In other words, it indicates how much of the variance in the outcome can be explained by the grouping factor (e.g. school or participant).\n\n\n\n\nShow the code\n# Extract random effects and their standard errors\nranef_obj &lt;- ranef(varmorph.me)  # Extract random effects with conditional variance\nse_ranef &lt;- arm::se.ranef(varmorph.me)           # Extract standard errors for random effects\n\n# Prepare a data frame for 'subj_id' random intercepts with confidence intervals\nsubj_ranef &lt;- ranef_obj$subj_id  # Random effects for subjects\nsubj_se &lt;- se_ranef$subj_id      # Standard errors for subjects\n\n# Combine random effects and standard errors into a data frame\nsubj_df &lt;- data.frame(\n  subj_id = rownames(subj_ranef),\n  intercept = subj_ranef[, \"(Intercept)\"],\n  se = subj_se[, \"(Intercept)\"],\n  conf.low = subj_ranef[, \"(Intercept)\"] - 1.96 * subj_se[, \"(Intercept)\"],\n  conf.high = subj_ranef[, \"(Intercept)\"] + 1.96 * subj_se[, \"(Intercept)\"]\n)\n\n# Create the waterfall plot\nggplot(subj_df, aes(x = reorder(subj_id, intercept), y = intercept)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", col = \"grey10\") +\n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\n  coord_flip() +\n  geom_vline(xintercept = 0, linetype = \"dashed\", col = \"grey10\") +\n  labs(title = \"Random Intercepts by Subject\",\n       x = \"Subject ID\",\n       y = \"Random Intercept\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n23.3.4.2 Varying-slope model\n\n# Varying-slope model; replace 0 with 1 if you want the intercept to vary too\nvarmorph.me2 &lt;- lmer(rt ~ prime_type +\n                      (0 + prime_type | subj_id),\n                      data = varmorph2)\n\nsummary(varmorph.me2)\n\n\ntab_model(varmorph.me2, show.se = TRUE, show.aic = TRUE, show.dev = TRUE)\n\n\n\n \nrt\n\n\nPredictors\nEstimates\nstd. Error\nCI\np\n\n\n(Intercept)\n715.70\n11.11\n693.92 – 737.49\n&lt;0.001\n\n\nprime type [derived]\n-31.31\n5.57\n-42.23 – -20.39\n&lt;0.001\n\n\nprime type [inflected]\n-33.96\n5.32\n-44.38 – -23.53\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n33126.00\n\n\n\nτ00\n \n\n\nτ00\n \n\n\nτ11 subj_id.prime_typeunrelated\n7545.01\n\n\nτ11 subj_id.prime_typederived\n8193.86\n\n\nτ11 subj_id.prime_typeinflected\n7385.08\n\n\nρ01\n \n\n\nρ01\n \n\n\nICC\n0.14\n\n\nN subj_id\n69\n\nObservations\n7038\n\n\nMarginal R2 / Conditional R2\n0.006 / 0.141\n\n\nDeviance\n93452.160\n\n\nAIC\n93455.369\n\n\n\n\n\n\n\n\nShow the code\n# Extract random slopes\nranef_data1 &lt;- ranef(varmorph.me2)$subj_id\n\n# Extract standard errors\nranef_data1_se &lt;- arm::se.ranef(varmorph.me2)$subj_id\n\n# Convert data into long format\nrandom_effects_df &lt;- ranef_data1 %&gt;%\n  as.data.frame() %&gt;% \n  rownames_to_column(var = \"subj_id\") %&gt;%\n  pivot_longer(cols = -subj_id, \n               names_to = \"prime_type\", \n               values_to = \"random_effect\")\n\n\n# Create a data frame for standard errors\nse_df &lt;- as.data.frame(ranef_data1_se) %&gt;%\n  rownames_to_column(var = \"subj_id\") %&gt;%\n  pivot_longer(cols = -subj_id, \n               names_to = \"prime_type\", \n               values_to = \"se\")\n\n\n# Combine random effects with standard errors\ncombined_df &lt;- random_effects_df %&gt;%\n  left_join(se_df, by = c(\"subj_id\", \"prime_type\"))\n\n# Calculate confidence intervals\ncombined_df &lt;- combined_df %&gt;%\n  mutate(lower_ci = random_effect - 1.96 * se,\n         upper_ci = random_effect + 1.96 * se)\n\n\n# Dotplots with confidence intervals\nggplot(combined_df, aes(x = subj_id, y = random_effect, col = prime_type)) +\n  coord_flip() +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", col = \"grey10\") +\n  facet_wrap(~ prime_type) +\n  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), width = 0.2) +\n  labs(title = \"Random Effect of Prime Type by Subject ID\",\n       x = \"Random Slope\",\n       y = \"Subject ID\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCiaccio, Laura Anna, and João Veríssimo. 2022. “Investigating Variability in Morphological Processing with Bayesian Distributional Models.” Psychonomic Bulletin & Review 29 (6): 2264–74. https://doi.org/10.3758/s13423-022-02109-w.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Analytical Methods for Social Research. Cambridge: Cambridge University Press.\n\n\nSchäfer, Roland. 2020. “Mixed-Effects Regression Modeling.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Thomas Gries, 535–61. Cham: Springer.",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Mixed-effects regression</span>"
    ]
  },
  {
    "objectID": "Ordinal_regression.html#suggested-reading",
    "href": "Ordinal_regression.html#suggested-reading",
    "title": "24  Ordinal regression",
    "section": "24.1 Suggested reading",
    "text": "24.1 Suggested reading\nGeneral:\n\nBaguley (2012): Chapter 17.4.5\nO’Connell (2006)\nPowers and Xie (2008): Chapter 7\nDocumentation of Cumulative Link Models",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ordinal regression</span>"
    ]
  },
  {
    "objectID": "Ordinal_regression.html#introduction",
    "href": "Ordinal_regression.html#introduction",
    "title": "24  Ordinal regression",
    "section": "24.2 Introduction",
    "text": "24.2 Introduction\nIn her recent contribution, Glass (2021) examines possible reasons why certain transitive verbs have a stronger affinity towards object omission compared to others, placing special emphasis on the routinisation of the actions denoted by the verbs. Specifically, she assesses how high/low-routine contexts affect the acceptability of object omission for transitive verbs from different frequency bins.\nWe will replicate her findings using her survey data Glass_2021_survey_processed.csv1:\n1 The original dataset can be retrieved from Lelia Glass’s OSF repository: https://osf.io/t6zw5 [Last accessed: 27th September, 2024].\n# Load libraries\nlibrary(tidyverse)\nlibrary(ordinal)\nlibrary(MASS)\nlibrary(sjPlot)\nlibrary(effects)\nlibrary(ggeffects)\nlibrary(ggpubr)\n\n# For additional tests\nlibrary(DescTools)\nlibrary(generalhoslem)\nlibrary(brant)\n\n# Load data\nsurvey &lt;- read.csv(\"Glass_2021_survey_processed.csv\")\n\n# Inspect dataset\nstr(survey)\n\n'data.frame':   784 obs. of  5 variables:\n $ rating       : int  3 4 3 3 2 4 3 2 3 2 ...\n $ freq         : chr  \"hi\" \"lo\" \"lo\" \"hi\" ...\n $ verb         : chr  \"pick\" \"catch\" \"throw\" \"break\" ...\n $ ParticipantID: chr  \"Participant1\" \"Participant1\" \"Participant1\" \"Participant1\" ...\n $ routine      : chr  \"hi\" \"lo\" \"lo\" \"lo\" ...\n\nhead(survey)\n\n  rating freq   verb ParticipantID routine\n1      3   hi   pick  Participant1      hi\n2      4   lo  catch  Participant1      lo\n3      3   lo  throw  Participant1      lo\n4      3   hi  break  Participant1      lo\n5      2   hi  taste  Participant1      hi\n6      4   hi bottle  Participant1      hi\n\n\n\n\n\n\n\n\nShort breakdown of the variables\n\n\n\n\n\n\nroutine: In Glass’s study, transitive verbs were randomly assigned to one of the following conditions:\n\n\n\n(High routine condition:) I worked at my poultry farm. Just like I always do, I butchered some chickens. Then I gathered some eggs.\n(Low-routine condition:) I visited a friend’s job. Just because people wanted me to try it, I butchered some chickens. Then I went for a walk.\n\nCf. Glass (2021: 66)\n\n\nunique(survey$routine)\n\n[1] \"hi\" \"lo\"\n\n\n\nrating records the responses of participants to a follow-up question regarding the acceptability of object omission. The answers are recorded on a 1-5 Likert scale.\n\n\nThe next time Caroline talks about butchering chickens the day before, how likely do you think she is to say the following?\n‘I butchered yesterday’\nCf. Glass (2021: 66)\n\n\nunique(survey$rating)\n\n[1] 3 4 2 5 1\n\n\n\nverb contains the items to be rated for the conditions in routine\n\n\nunique(survey$verb)\n\n[1] \"pick\"   \"catch\"  \"throw\"  \"break\"  \"taste\"  \"bottle\" \"sell\"   \"chop\"  \n\n\n\nfrequency relates to the frequency bins of the verbs:\n\n\nunique(survey$freq)\n\n[1] \"hi\" \"lo\"\n\n\n\nParticipantID identifies each of the 98 subjects who provided ratings",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ordinal regression</span>"
    ]
  },
  {
    "objectID": "Ordinal_regression.html#descriptive-overview",
    "href": "Ordinal_regression.html#descriptive-overview",
    "title": "24  Ordinal regression",
    "section": "24.3 Descriptive overview",
    "text": "24.3 Descriptive overview\n\n\nShow the code\nggplot(survey, aes(x = rating, fill = freq)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~freq) +\n  theme_minimal() +\n  labs(title = \"Density of Ratings by Frequency\",\n       x = \"Rating\", y = \"Density\", fill = \"Frequency\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(survey, aes(x = rating, fill = routine)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~routine) +\n  theme_minimal() +\n  labs(title = \"Density of Ratings by Routine\",\n       x = \"Rating\", y = \"Density\", fill = \"Routine\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggridges)\nggplot(survey, aes(x = rating, y = verb, fill = verb)) +\n  geom_density_ridges(scale = 5, rel_min_height = 0.01, alpha = 0.6) +\n  theme_ridges() +\n  #theme(legend.position = \"none\") +\n  labs(title = \"Distribution of Ratings by Verb\",\n       x = \"Rating\", y = \"Verb\")",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ordinal regression</span>"
    ]
  },
  {
    "objectID": "Ordinal_regression.html#modelling-ordinal-data",
    "href": "Ordinal_regression.html#modelling-ordinal-data",
    "title": "24  Ordinal regression",
    "section": "24.4 Modelling ordinal data",
    "text": "24.4 Modelling ordinal data\nOur task is clear: We need to measure how routine and freq affect the variability in the acceptability ratings, while controlling for repeated measurements for verb and ParticipantID, which impose a hierarchical structure on the dataset.\nFormally speaking, we have \\(p\\) explanatory variables \\(X_1, X_2, ..., X_p\\) for \\(1, ..., p\\). The target variable, i.e. our \\(Y\\), is rating with the ordered, discrete outcomes \\(y \\in \\{1, 2, 3, 4, 5\\}\\).\nThe goal is to find a model \\(f\\) that describes the relationship between \\(Y\\) and \\(X_p\\) as accurately as possible and minimises the error term \\(\\epsilon\\):\n\\[\nY = f(X_1, X_2, ..., X_p) + \\epsilon\n\\tag{24.1}\\]\n\n24.4.1 Ordered logistic regression\nOne family of models that respects the ordered, yet categorical nature of \\(Y\\) is ordered (or ordinal) logistic regression. Other terms include proportional odds models and cumulative logit/link models.\n\n\n\n\n\n\nRecap: Logistic regression\n\n\n\n\n\nLogistic regression is used to model categorical response variables with two or more levels. For instance, let’s assume our \\(Y\\) is dichotomous with the following two outcomes:\n\\[\nY =\n\\begin{cases}\n\\text{yes} \\\\\n\\text{no}\n\\end{cases}\n\\tag{24.2}\\]\nUsing the logistic function, we can estimate the probability of one outcome versus the other given the predictors \\(X_p\\). Their log-transformed odds ratio (log odds) is equivalent of the all-too-familiar linear model:\n\\[\n\\log\\left(\\frac{P(Y = yes \\mid X_1, X_2, ..., X_p)}{1 - P(Y = yes \\mid X_1, X_2, ..., X_p)}\\right) = \\beta_0 + \\sum_{i=1}^p \\beta_iX_i\n\\tag{24.3}\\]\n\n\n\nCore to this approach is the notion of cumulative probabilities. Let \\(J\\) denote the number of ordered categories in \\(Y\\). In Glass’s case study, the estimated cumulative probabilities for each ordered outcome (= acceptability rating) would have the forms in Equation 24.4.\n\\[\n\\begin{array}{rcl}\nP(Y \\leq 1) & = & P(Y = 1) \\\\\nP(Y \\leq 2) & = & P(Y = 1) + P(Y = 2) \\\\\nP(Y \\leq 3) & = & P(Y = 1) + P(Y = 2) + P(Y = 3) \\\\\n& \\vdots & \\\\\nP(Y \\leq j) & = & P_1 + ... + P_j\n\\end{array}\n\\tag{24.4}\\]\nWe can now update our logistic regression model to take into account cumulative probabilities for \\(j = 1, ..., J-1\\).\n\\[\n\\log\\left(\\frac{P(Y \\leq j \\mid X_1, X_2, ..., X_p)}{1 - P(Y \\leq j \\mid X_1, X_2, ..., X_p)}\\right) = \\beta_0 + \\sum_{i=1}^p \\beta_iX_i\n\\tag{24.5}\\]\nThe intercepts \\(\\beta_{0_j}\\) serve as cutpoints between the adjacent ordinal categories. For \\(J = 5\\) categories, there are \\(J - 1 = 4\\) cutpoints, i.e.,\n\n1|2 for \\(P(Y \\leq 1)\\)\n2|3 for \\(P(Y \\leq 2)\\)\n3|4 for \\(P(Y \\leq 3)\\)\n4|5 for \\(P(Y \\leq 4)\\).\n\nGiven a change in predictor values, the slope coefficients \\(\\beta_pX_p\\) indicate how the probability of being in a higher rating category changes (Baguley 2012: 691–2).\nWe can obtain “regular” probabilities from the cumulative ones by drawing on the equivalence in Equation 24.6.\n\\[\nP(Y = j) = P(Y \\leq j) - P(Y \\leq j - 1)\n\\tag{24.6}\\]\nFor instance, the probability \\(P(Y = 3)\\) is equivalent to\n\\[\nP(Y = 3) = P(Y \\leq 3) - P(Y \\leq 2).\n\\tag{24.7}\\]\n\n\n\n\n\n\nAssumptions of proportional odds models\n\n\n\n\n\nThe proportional odds assumption stipulates a stable effect of the predictors on the (log) odds of the ordinal outcomes across all possible cutpoints (O’Connell 2006: 29). In case of violation, it is better to rely on partial proportional odds models or multinomial logistic regression instead.",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ordinal regression</span>"
    ]
  },
  {
    "objectID": "Ordinal_regression.html#application-in-r",
    "href": "Ordinal_regression.html#application-in-r",
    "title": "24  Ordinal regression",
    "section": "24.5 Application in R",
    "text": "24.5 Application in R\nThere are several R packages that support ordinal logistic regression models. This section provides an overview of some of the more common (as well as well-documented) implementations.\n\n24.5.1 Using polr() from the MASS library\n\n# Convert to factor and determine ordering\nsurvey$rating &lt;- factor(survey$rating, ordered = TRUE, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n# Fit polr model\nsurvey.polr &lt;- polr(rating ~ \n                      freq +\n                      routine,\n                      data = survey)\n\n# Model summary\nsummary(survey.polr)\n\nCall:\npolr(formula = rating ~ freq + routine, data = survey)\n\nCoefficients:\n             Value Std. Error  t value\nfreqlo    -0.01095     0.1291 -0.08483\nroutinelo -0.55521     0.1302 -4.26449\n\nIntercepts:\n    Value   Std. Error t value\n1|2 -0.8704  0.1228    -7.0859\n2|3  0.1342  0.1188     1.1290\n3|4  1.2528  0.1293     9.6856\n4|5  2.8915  0.2003    14.4345\n\nResidual Deviance: 2246.662 \nAIC: 2258.662 \n\n# R-squared and AIC\nPseudoR2(survey.polr, c(\"Nagelkerke\", \"AIC\"))\n\n  Nagelkerke          AIC \n   0.0244793 2258.6617419 \n\n\n\ntab_model(survey.polr, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, transform = NULL)\n\n\n\n\n \nrating\n\n\nPredictors\nLog-Odds\nstd. Error\nCI\np\n\n\n1|2\n-0.87\n0.12\n-1.11 – -0.63\n&lt;0.001\n\n\n2|3\n0.13\n0.12\n-0.10 – 0.37\n0.259\n\n\n3|4\n1.25\n0.13\n1.00 – 1.51\n&lt;0.001\n\n\n4|5\n2.89\n0.20\n2.50 – 3.28\n&lt;0.001\n\n\nfreq [lo]\n-0.01\n0.13\n-0.26 – 0.24\n0.932\n\n\nroutine [lo]\n-0.56\n0.13\n-0.81 – -0.30\n&lt;0.001\n\n\nObservations\n784\n\n\nR2 Nagelkerke\n0.024\n\n\nDeviance\n2246.662\n\n\nAIC\n2258.662\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the model parameters\n\n\n\n\n\n\nCoefficients: The conditions freqlo (low frequency) and routinelo (low-routine context) both have negative values, which means that both of them decrease the probability of obtaining a higher acceptability rating (compared to freqhi and routinehi).\nIntercepts: These represent the cutpoints between the ordinal categories, which are necessary for calculating the probabilities of each ordinal category.\n\n\n\n\n\n\n24.5.2 Testing assumptions and goodness of fit\n\nTest proportional odds assumption:\n\n\nbrant(survey.polr) # p &lt; 0.05 is a violation of the assumption\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     14.45   6   0.02\nfreqlo      6.5 3   0.09\nroutinelo   8.14    3   0.04\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\n\nHosmer-Lemeshow test, which is essentially a \\(\\chi^2\\)-test:\n\n\nlogitgof(survey$rating, # observed\n         fitted(survey.polr), # expected\n         ord = TRUE) # respect ordering\n\n\n    Hosmer and Lemeshow test (ordinal model)\n\ndata:  survey$rating, fitted(survey.polr)\nX-squared = 51.173, df = 35, p-value = 0.03808\n\n\n\nThe Lipsitz test is an extension of the Hosmer-Lemeshow test. Note that it requires the response to be a factor.\n\n\nlipsitz.test(survey.polr)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  rating ~ freq + routine\nLR statistic = 20.261, df = 9, p-value = 0.01637\n\n\n\nPart of the same family of tests is the Pulkstenis-Robinson test, which also relies on the \\(\\chi^2\\)-distribution:\n\n\npulkrob.chisq(survey.polr, catvars = c(\"freq\", \"routine\"))\n\n\n    Pulkstenis-Robinson chi-squared test\n\ndata:  formula:  rating ~ freq + routine\nX-squared = 21.476, df = 9, p-value = 0.0107\n\n\n\n\n24.5.3 Visualisation\n\n24.5.3.1 With effects\n\n# Routine effect plot\nplot(Effect(focal.predictors = c(\"routine\"), mod = survey.polr), rug = FALSE, style=\"stacked\")\n\n\n\n24.5.3.2 With ggeffects and ggplot2\n\n\nShow the code\n# Get the ggeffects data\neff &lt;- ggeffects::ggeffect(survey.polr, \"freq\")\n\n# Convert to a data frame\nplot_data &lt;- as.data.frame(eff)\n\n# Ensure the response.level has the desired levels\nplot_data$response.level &lt;- factor(plot_data$response.level, \n                                   levels = c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"),\n                                   labels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n# Create the plot with confidence intervals\np1 &lt;- ggplot(plot_data, aes(x = x, y = predicted, color = response.level, group = response.level)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1, linewidth = 0.5) +\n  scale_color_viridis_d() +\n  labs(\n    x = \"Frequency\",\n    y = \"Predicted Probability\",\n    color = \"Rating\"\n  ) +\n  facet_grid(~ response.level) +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    panel.grid.minor = element_blank(),\n    axis.title = element_text(face = \"bold\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(margin = margin(b = 10))\n  ) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1))\n\np1\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Get the ggeffects data for \"routine\"\neff_routine &lt;- ggeffects::ggeffect(survey.polr, \"routine\")\n\n# Convert to a data frame\nplot_data_routine &lt;- as.data.frame(eff_routine)\n\n# Ensure the response.level has the desired levels for \"routine\"\nplot_data_routine$response.level &lt;- factor(plot_data_routine$response.level, \n                                           levels = c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"),\n                                           labels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n# Create the second plot for \"routine\"\np2 &lt;- ggplot(plot_data_routine, aes(x = x, y = predicted, color = response.level, group = response.level)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1, linewidth = 0.5) +\n  scale_color_viridis_d() +\n  labs(\n    x = \"Routine\",\n    y = \"Predicted Probability\",\n    color = \"Rating\"\n  ) +\n  facet_grid(~ response.level) +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    panel.grid.minor = element_blank(),\n    axis.title = element_text(face = \"bold\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(margin = margin(b = 10))\n  ) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1))\n\np2\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Generate the interaction effects for \"freq\" and \"routine\"\neff_interaction &lt;- ggeffect(survey.polr, terms = c(\"freq\", \"routine\"))\n\n# Convert to a data frame\nplot_data_interaction &lt;- as.data.frame(eff_interaction)\n\n# Ensure the response.level has the desired levels\nplot_data_interaction$response.level &lt;- factor(plot_data_interaction$response.level, \n                                               levels = c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"),\n                                               labels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n\n# Create the interaction plot with facet by 'x' and color by 'response.level'\np_interaction &lt;- ggplot(plot_data_interaction, aes(x = group, y = predicted, color = response.level, group = response.level)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1, linewidth = 0.5) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Predicted Probabilities for Interaction of Frequency and Routine\",\n    x = \"Routine\",\n    y = \"Predicted Probability\",\n    color = \"Rating\"\n  ) +\n  facet_wrap(~ x, labeller = labeller(x = c(\"hi\" = \"High Frequency\", \"lo\" = \"Low Frequency\"))) +  # Facet by \"freq\"\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    panel.grid.minor = element_blank(),\n    axis.title = element_text(face = \"bold\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(margin = margin(b = 10))\n  ) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1))\n\n# Display the interaction plot\np_interaction\n\n\n\n\n\n\n\n\n\n\n\n\n24.5.4 Using clm() from the ordinal library\n\n# Convert to factor and determine ordering\nsurvey$rating &lt;- factor(survey$rating, ordered = TRUE, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n# Fit cumulative link model\nclm.1 &lt;- ordinal::clm(rating ~ \n                    freq +\n                    routine,\n                    data = survey, Hess=TRUE)\n\n# Model summary\nsummary(clm.1)\n\n\ntab_model(clm.1, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, transform = NULL)\n\n\n\n\n \nrating\n\n\nPredictors\nLog-Odds\nstd. Error\nCI\np\n\n\n1|2\n-0.87\n0.12\n-1.11 – -0.63\n&lt;0.001\n\n\n2|3\n0.13\n0.12\n-0.10 – 0.37\n0.259\n\n\n3|4\n1.25\n0.13\n1.00 – 1.51\n&lt;0.001\n\n\n4|5\n2.89\n0.20\n2.50 – 3.28\n&lt;0.001\n\n\nfreq [lo]\n-0.01\n0.13\n-0.26 – 0.24\n0.932\n\n\nroutine [lo]\n-0.56\n0.13\n-0.81 – -0.30\n&lt;0.001\n\n\nObservations\n784\n\n\nR2 Nagelkerke\n0.024\n\n\nAIC\n2258.662\n\n\n\n\n\n\n\n\n\n24.5.5 Mixed-effects ordinal regression\n\n\n\n\n\n\nRecap: Mixed-effects models\n\n\n\n\n\nIf the data is nested according to some grouping factor with \\(1, ..., k\\) groups, we can let the intercept and/or slopes vary by group. For instance, recall the varying-intercept model:\n\\[\nY = \\alpha_{k} + \\beta_1X_{1} + \\beta_2X_{2} + ... + \\epsilon \\qquad \\alpha_{k} \\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha}^2).\n\\] In this case we also speak of random effects.\n\n\n\n\n# Convert to factor and determine ordering\nsurvey$rating &lt;- factor(survey$rating, ordered = TRUE, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n# Fit mixed model with random intercepts for \"verb\" and \"ParticipantID\"\nclm.2 &lt;- ordinal::clmm(rating ~ \n                    freq * routine +\n                    (1 | verb) +\n                    (1 | ParticipantID),\n                    data = survey, Hess=TRUE)\n\n# Model summary\nsummary(clm.2)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: rating ~ freq * routine + (1 | verb) + (1 | ParticipantID)\ndata:    survey\n\n link  threshold nobs logLik  AIC     niter     max.grad cond.H \n logit flexible  784  -991.65 2001.30 598(2386) 3.81e-04 1.5e+02\n\nRandom effects:\n Groups        Name        Variance Std.Dev.\n ParticipantID (Intercept) 2.0628   1.4363  \n verb          (Intercept) 0.9059   0.9518  \nNumber of groups:  ParticipantID 98,  verb 8 \n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \nfreqlo            -0.1567     0.2138  -0.733 0.463437    \nroutinelo         -0.7473     0.2103  -3.553 0.000381 ***\nfreqlo:routinelo  -0.1406     0.2981  -0.472 0.637046    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -1.3562     0.4020  -3.374\n2|3   0.2062     0.3988   0.517\n3|4   1.8329     0.4053   4.523\n4|5   3.8639     0.4401   8.780\n\n\n\ntab_model(clm.2, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, transform = NULL)\n\n\n\n \nrating\n\n\nPredictors\nLog-Odds\nstd. Error\nCI\np\n\n\n1|2\n-1.36\n0.40\n-2.14 – -0.57\n0.001\n\n\n2|3\n0.21\n0.40\n-0.58 – 0.99\n0.605\n\n\n3|4\n1.83\n0.41\n1.04 – 2.63\n&lt;0.001\n\n\n4|5\n3.86\n0.44\n3.00 – 4.73\n&lt;0.001\n\n\nfreq [lo]\n-0.16\n0.21\n-0.58 – 0.26\n0.463\n\n\nroutine [lo]\n-0.75\n0.21\n-1.16 – -0.34\n&lt;0.001\n\n\nfreq [lo] × routine [lo]\n-0.14\n0.30\n-0.72 – 0.44\n0.637\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 ParticipantID\n2.06\n\n\nτ00 verb\n0.91\n\n\nICC\n0.47\n\n\nN verb\n8\n\n\nN ParticipantID\n98\n\nObservations\n784\n\n\nMarginal R2 / Conditional R2\n0.028 / 0.489\n\n\nAIC\n2001.305\n\n\n\n\n\n\n\n\nShow the code\n# Extract random effects\nre_verb &lt;- ranef(clm.2)$verb\nre_participant &lt;- ranef(clm.2)$ParticipantID\n\n# Create dataframes for random effects\ndf_verb &lt;- data.frame(verb = rownames(re_verb), re = re_verb[,1])\ndf_participant &lt;- data.frame(ParticipantID = rownames(re_participant), re = re_participant[,1])\n\n# Get predictions for an average case\npred_avg &lt;- ggpredict(clm.2, terms = c(\"freq\", \"routine\"))\n\n# Add random effects to predictions\npred_verb &lt;- crossing(pred_avg, df_verb) %&gt;%\n  mutate(predicted = predicted + re)\n\npred_participant &lt;- crossing(pred_avg, df_participant) %&gt;%\n  mutate(predicted = predicted + re)\n\n# Create a horizontal dot plot for random effects of participants\np_caterpillar &lt;- ggplot(df_participant, aes(x = re, y = reorder(ParticipantID, re))) +\n  geom_point(size = 3, color = \"steelblue3\") +  # Dots representing the random effects\n  labs(title = \"Random Effects for Participants\", \n       x = \"Random Effect Estimate (log odds)\", \n       y = \"Participant ID\") +\n   geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey20\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank())  # Removes the gridlines for y-axis\n\np_caterpillar2 &lt;- ggplot(df_verb, aes(x = re, y = reorder(verb, re))) +\n  geom_point(size = 3, color = \"steelblue3\") +  # Dots representing the random effects\n  labs(title = \"Random Effects for Verbs\", \n       x = \"Random Effect Estimate (log odds)\", \n       y = \"Verb\") +\n   geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey20\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank())  # Removes the gridlines for y-axis\n\nggarrange(p_caterpillar, p_caterpillar2, ncol = 2, common.legend = TRUE, legend = \"right\")",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ordinal regression</span>"
    ]
  },
  {
    "objectID": "Ordinal_regression.html#generalised-additive-mixed-effects-models-gamms",
    "href": "Ordinal_regression.html#generalised-additive-mixed-effects-models-gamms",
    "title": "24  Ordinal regression",
    "section": "24.6 Generalised Additive Mixed-effects Models (GAMMs)",
    "text": "24.6 Generalised Additive Mixed-effects Models (GAMMs)\n\n24.6.1 Suggested reading\nFor linguists:\n\nBaayen & Linke (2020)\n\nGeneral:\n\nHastie & Tibshirani (1991)\nWood (2006)\n\n\n\n24.6.2 Rationale\nA core assumption of Generalised Linear Models (GLMs) is a linear relationship between predictor(s) and response. If, however, one is interested in exploring potential non-linear trends without the risk of extreme overfitting, GAMs offer an elegant solution: Instead of relying on the linear sum of model coefficients, GAMs estimate more flexible smooth terms \\(f_k\\) for \\(k = 1, ..., p\\). For illustration, Equation 24.8 shows a linear additive model for a continuous target variable with \\(p\\) predictors.\n\\[\nY = \\beta_0 + \\sum\\limits_{k = 1}^p f_k(X_k)\n\\tag{24.8}\\]\n\n\n24.6.3 Application in R\n\n# Load libraries\nlibrary(mgcv)\nlibrary(itsadug)\nlibrary(gratia)\n\n# Convert predictors to factors\nsurvey$ParticipantID &lt;- as.factor(survey$ParticipantID)\nsurvey$verb &lt;- as.factor(survey$verb)\n\n# Fit GAMM\ngam1 &lt;- bam(as.numeric(rating) ~ # treated as numeric term\n              freq + # linear term\n              routine + # linear term\n              s(ParticipantID, bs = \"re\") + # smooth term\n              s(verb, bs = \"re\"), # smooth term\n              data = survey, \n              family = ocat(R = 5) # number of ordinal categories\n            )\n\n# Model summary\nsummary(gam1)\n\n\nFamily: Ordered Categorical(-1,0.51,2.09,4.06) \nLink function: identity \n\nFormula:\nas.numeric(rating) ~ freq + routine + s(ParticipantID, bs = \"re\") + \n    s(verb, bs = \"re\")\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.3613     0.3880   0.931    0.352    \nfreqlo       -0.2195     0.1431  -1.534    0.125    \nroutinelo    -0.7752     0.1428  -5.430 7.79e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                    edf Ref.df      F p-value    \ns(ParticipantID) 77.654     97  4.677  &lt;2e-16 ***\ns(verb)           6.728      7 25.860  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDeviance explained = 27.3%\nfREML =   1631  Scale est. = 1         n = 784\n\n# Extract the intercepts for plotting\nthresh &lt;- gratia::theta(gam1) %&gt;% \n  tibble::as_tibble() %&gt;% \n  setNames(c(\"threshold\"))\n\n# Extract predictions for \"routine\"\nroutine_pred &lt;- ggpredict(gam1, terms = \"routine\")\n\n# Plot predictions\nroutine_pred %&gt;% \n  ggplot(aes(x = x, y = predicted)) +\n  geom_point(col = \"steelblue\") +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = 0), col = \"steelblue\") +\n  geom_hline(data = thresh, aes(yintercept = threshold), linetype = \"dashed\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Extract random effects for \"verb\"\nverb_pred &lt;- ggpredict(gam1, terms = \"verb\")\n\n# Plot random effect\nverb_pred %&gt;% \n  ggplot(aes(x = x, y = predicted)) +\n  geom_point(col = \"steelblue\") +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = 0), col = \"steelblue\") +\n  geom_hline(data = thresh, aes(yintercept = threshold), linetype = \"dashed\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Extract random effects for \"ParticipantID\"\nsubj_pred &lt;- ggpredict(gam1, terms = \"ParticipantID\")\n\n# Plot random effect\nsubj_pred |&gt;\n  ggplot(aes(x = x, y = predicted)) +\n  geom_point(col = \"steelblue\") +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = 0), col = \"steelblue\") +\n  #geom_line() +\n  geom_hline(data = thresh, aes(yintercept = threshold), linetype = \"dashed\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nBaayen, R. Harald, and Maja Linke. 2020. “Generalized Additive Mixed Models.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Thomas Gries, 563–91. Cham: Springer.\n\n\nBaguley, Thomas. 2012. Serious Stats: A Guide to Advanced Statistics for the Behavioral Sciences. Houndmills, Basingstoke: Palgrave Macmillan.\n\n\nGlass, Lelia. 2021. “English Verbs Can Omit Their Objects When They Describe Routines.” English Language and Linguistics 26 (1): 49–73. https://doi.org/10.1017/S1360674321000022.\n\n\nHastie, Trevor, and Robert Tibshirani. 1991. Generalized Additive Models. London: Chapman & Hall.\n\n\nO’Connell, Ann A. 2006. Logistic Regression Models for Ordinal Response Variables. Vol. 146. Thousand Oaks, Calif.: Sage.\n\n\nPowers, Daniel A., and Yu Xie. 2008. Statistical Methods for Categorical Data Analysis. 2. ed. Bingley: Emerald.\n\n\nWood, Simon N. 2006. Generalized Additive Models: An Introduction with R. Boca Raton: Chapman & Hall/CRC.",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ordinal regression</span>"
    ]
  },
  {
    "objectID": "Decision_trees_and_random_forests.html#recommended-reading",
    "href": "Decision_trees_and_random_forests.html#recommended-reading",
    "title": "25  Decision trees and random forests",
    "section": "25.1 Recommended reading",
    "text": "25.1 Recommended reading\nFor linguists:\n\nLevshina (2015): Chapter 14\nLevshina (2020)\nGries (2021): Chapter 7\n\nGeneral:\n\nJames et al. (2021): Chapter 8\nHastie, Tibshirani, and Friedman (2017): Chapters 9.2 & 15",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Decision trees and random forests</span>"
    ]
  },
  {
    "objectID": "Decision_trees_and_random_forests.html#introduction",
    "href": "Decision_trees_and_random_forests.html#introduction",
    "title": "25  Decision trees and random forests",
    "section": "25.2 Introduction",
    "text": "25.2 Introduction\nDecision trees and random forests are very popular non-parametric methods. As such, “they do not make explicit assumptions about the functional form of \\(f\\)’’ (James et al. 2021: 23).\nIn this unit, we will cover the conceptual basics of these methods as well as their implementation in R using the tv data from Levshina (2020) in addition to the ELP data from the unit on Linear Regression. The libraries we will need are listed below:\n\n# Load libraries\nlibrary(\"tidyverse\")\nlibrary(\"readxl\")\nlibrary(\"tree\") # for CART\nlibrary(\"randomForest\") # for traditional random forests\nlibrary(\"party\") # for Conditional Inference Trees\nlibrary(\"pdp\") # for partial dependence plots\n\n## Reaction time data\nELP &lt;- read_xlsx(\"data/ELP.xlsx\")\n\nELP$POS &lt;- as.factor(ELP$POS)\n\n## Levshina's (2020) data set on T/V forms in Russian\ntv &lt;- read.csv(\"data/Levshina_2020_tv_data.csv\", sep = \",\", header = TRUE, stringsAsFactors = TRUE)\n\nIn the tv data frame, our target variable will be T/V Form with the two outcomes ty (Russian 2.p.sg., informal) and vy (Russian 2.p.pl., polite).\n\nstr(tv)\n\nhead(tv)",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Decision trees and random forests</span>"
    ]
  },
  {
    "objectID": "Decision_trees_and_random_forests.html#decision-trees",
    "href": "Decision_trees_and_random_forests.html#decision-trees",
    "title": "25  Decision trees and random forests",
    "section": "25.3 Decision trees",
    "text": "25.3 Decision trees\nCore concepts:\n\nSegmenting the feature space: “[T]he feature space (i.e., the space spanned by all predictor variables) is recursively partitioned into a set of rectangular areas” (Strobl, Malley, and Tutz 2009: 325).\nImpurity reduction: These simplified prediction areas should consist of mostly homogeneous (i.e., ‘pure’ rather than ‘mixed’) observations.\nTree construction: The ‘decisions’ made when partitioning the training data can be visualised using tree structures. The nodes of a tree represent variables, the branches represent decision rules, and leaf nodes indicate the final outcome (e.g., a prediction).\nCART: The original computational implementation of decision trees is known as the CART (Classification and Regression Trees) algorithm developed by Breiman (1984).\n\n\n25.3.1 Classification trees\nIf we are dealing with a categorical response variable, the tree() function can be used to fit a classification tree in accordance with Breiman’s CART algorithm. For illustration, consider the tv data frame. We will model the choice of the pronoun Form based on the speaker’s and hearer’s social circle (Rel_Circle) and their difference in social class (Rel_Class).\n\n# Set random number generator for reproducibility\nset.seed(123)\n\n# Supply model formula\ntree.tv &lt;- tree(Form ~ Rel_Circle + Rel_Class, data = tv)\n\n# View tree statistics\nsummary(tree.tv)\n\n\nClassification tree:\ntree(formula = Form ~ Rel_Circle + Rel_Class, data = tv)\nNumber of terminal nodes:  9 \nResidual mean deviance:  0.974 = 213.3 / 219 \nMisclassification error rate: 0.2412 = 55 / 228 \n\n# Visualisation\nplot(tree.tv)\n\ntext(tree.tv, pretty = 3)\n\n\n\n\n\n\n\n\nAn important problem that arises during tree construction is that of split selection. When should the tree split a node into two further nodes and when not? Furthermore, when should the tree stop the splitting process entirely? In this respect, CART relies on the principle of impurity reduction: “The fundamental idea is to select each split of a subset so that the data in each of the descendent subsets are ‘purer’ than the data in the parent subset” (Breiman 1984: 23). A measure for node purity is the Gini index \\(G\\), which is defined as\n\\[\nG = \\sum_{k=1}^{K}{\\hat{p}_{mk}(1-\\hat{p}_{mk}),}\n\\]\nwhere \\(\\hat{p}_{mk}\\) measures the proportion of observations of a response level \\(k\\) in the \\(m\\)th prediction area of the training data set. Values close to 0 are indicative of high node purity, meaning that most observations belong to the same class (e.g., Form = ty). If splitting a node no longer leads to a substantial increase in purity, it becomes the terminal node, i.e., it is not split further. This terminal node returns the tree’s class prediction.\nIt is worth noting that modern CART implementations rely on different splitting criteria. For instance, Conditional Inference Trees use the \\(p\\) values of internal association tests to identify which variables warrant further subdivision of the training data. The presence or absence of correlation thus also determines whether or not a given node will be terminal (for more details, see B. Greenwell 2022: 122).\n\n# Fitting a conditional inference tree\nctree.tv &lt;- ctree(Form ~ ., data = tv) # dot . means 'include all predictors'\n\nplot(ctree.tv)\n\n\n\n\n\n\n\n\n\n\n25.3.2 Regression trees\nRegression trees are used for continuous response variables. Instead of providing class predictions, they return the mean value of observations in a given prediction area. The algorithm now strives to minimize the residual sum of squares (\\(RSS\\)). Consider the regression tree for reaction times depending on word length, frequency and part of speech:\n\n# CART tree\ntree.rt &lt;- tree(RT ~ Length + Freq + POS, data = ELP)\n\nsummary(tree.rt)\n\n\nRegression tree:\ntree(formula = RT ~ Length + Freq + POS, data = ELP)\nVariables actually used in tree construction:\n[1] \"Freq\"   \"Length\"\nNumber of terminal nodes:  10 \nResidual mean deviance:  8629 = 7507000 / 870 \nDistribution of residuals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-220.10  -59.09  -11.17    0.00   49.66  397.40 \n\nplot(tree.rt)\n\ntext(tree.rt, pretty = 0)\n\n\n\n\n\n\n\n# Conditional inference tree\nctree.rt &lt;- ctree(RT ~ Length + Freq + POS, data = ELP) \n\nplot(ctree.rt)",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Decision trees and random forests</span>"
    ]
  },
  {
    "objectID": "Decision_trees_and_random_forests.html#random-forests",
    "href": "Decision_trees_and_random_forests.html#random-forests",
    "title": "25  Decision trees and random forests",
    "section": "25.4 Random forests",
    "text": "25.4 Random forests\nRandom forests (Breiman 2001) belong to the class of ensemble methods because they combine simpler models (e.g., individual decision trees) into a more complex and possibly more accurate model. As part of the RF algorithm, a great number of decision trees is trained on bootstrapped samples of the training data.\nSo far, random forests are essentially identical with Bagging (= bootstrap aggregation); however, an important additional characteristic of the RF algorithm is that only a random subset of the predictors is taken into consideration at each split. According to Strobl et al. (2009: 332), the resulting variability in tree structure is advantageous: “By combining the prediction of such a diverse set of trees, ensemble methods utilize the fact that classification trees are unstable, but, on average, produce the right prediction”.\n\n25.4.1 Regression forest\nFor regression tasks, random forests return the average prediction of all trees in the ensemble.\n\n# For regression\nrt.rf.reg &lt;- randomForest(RT ~ Length + Freq + POS, data = ELP,\n                                mtry = 1, # = sqrt(number of variables)\n                                ntree = 500) # number of trees\n\nrt.rf.reg\n\n\nCall:\n randomForest(formula = RT ~ Length + Freq + POS, data = ELP,      mtry = 1, ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 8972.927\n                    % Var explained: 43.64\n\n# Conditional random forest\nrt.crf.reg &lt;- cforest(RT ~ Length + Freq + POS, data = ELP, \n                    controls = cforest_unbiased(ntree = 500, mtry = 1))\n\n\n\n25.4.2 Classification forest\nFor classification, all trees cast a vote for one of the response classes. The OOB error estimate refers to the accuracy of out-of-bag (OOB) predictions. After the initial bootstrapping procedure, roughly a third of the training data remains unused. These observations, which were not used for fitting trees, can be used as a test data set. Predictions based on this internal test data set are called OOB predictions.\n\n# For classification\ntv.rf.class &lt;- randomForest(Form ~ ., data = tv,\n                            mtry = 4,\n                            ntree = 500)\n\ntv.rf.class\n\n\nCall:\n randomForest(formula = Form ~ ., data = tv, mtry = 4, ntree = 500) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n        OOB estimate of  error rate: 18.86%\nConfusion matrix:\n   ty vy class.error\nty 86 22   0.2037037\nvy 21 99   0.1750000\n\n# Conditional random forest\ntv.crf.class &lt;- cforest(Form ~ ., data = tv,\n                    controls = cforest_unbiased(ntree = 500, mtry = 4))\n\ntv.crf.class\n\n\n     Random Forest using Conditional Inference Trees\n\nNumber of trees:  500 \n\nResponse:  Form \nInputs:  Film, Rel_Age, Rel_Sex, Rel_Power, Rel_Circle, S_Class, H_Class, S_Age, H_Age, Rel_Class, Before68, Others, Office, S_Sex, H_Sex, Place \nNumber of observations:  228 \n\n\n\n\n25.4.3 Variable importance\nRandom forests allow users to assess whether or not certain predictors are useful for the model. The Gini index can be re-used to identify those variables that have led to the greatest reduction in impurity. However, this measure is biased towards predictors with many values (cf. Strobl et al. 2007).\n\n# Gini importance (Reaction times)\nvarImpPlot(rt.rf.reg)\n\n\n\n\n\n\n\n# Gini importance (Form of 2.p.)\nvarImpPlot(tv.rf.class)\n\n\n\n\n\n\n\n\nA more robust measure is (Conditional) Permutation Accuracy Importance which compares the predictive accuracy of the random forest model before and after randomly permuting the values of the predictors (cf. Strobl et al. 2008; Debeer and Strobl 2020).\n\n# Conditional permutation accuracy importance\nlibrary(\"permimp\")\n\n# Refit RF model with additional parameters\ntv.rf.class &lt;- randomForest(Form ~ .,\n                            data = tv,\n                            mtry = 4,\n                            ntree = 500,\n                            keep.inbag = TRUE,\n                            keep.forest = TRUE)\n\n# Compute CPI scores\ntv.rf.permimp &lt;- permimp(tv.rf.class, conditional = TRUE, progressBar = FALSE, threshold = .95) # Choose \"Yes\" in the console\n\n# Plot CPI scores\nplot(tv.rf.permimp, horizontal = TRUE, type = \"dot\", sort = TRUE)\n\n\n\n25.4.4 Visualising random forest models\nPartial dependence plots provide averaged predictions (\\(\\hat{y}\\)) for a given constellation of predictors.\nThese averages are produced by the partial dependence function \\(f_k\\). If a categorical response variable has \\(K\\) possible values, it has the form \\[\nf_k(X) = \\log [p_k (x)] - \\frac{1}{N}\\sum_{k=1}^{K} \\log [p_k (x)],\n\\] with \\(p_k(x)\\) corresponding to the fitted probability of the 𝑘th level of the response variable for \\(k \\in \\{1, 2, ..., K\\}\\) (B. M. Greenwell 2017: 430; see also Hastie, Tibshirani, and Friedman 2017).\n\n\nShow the code\n# Form ~ Rel_Circle\nRel_Circle.partial &lt;- pdp::partial(tv.rf.class, pred.var = \"Rel_Circle\", which.class = \"ty\")\n\nRel_Circle.partial %&gt;% \n  ggplot(aes(x = Rel_Circle, y = yhat, group = 1)) +\n  geom_point(col = \"steelblue\") +\n  geom_line(col = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    title = \"Probability of 'ty' (2.p.sg.) depending on social circle\",\n    y = \"Log odds of 'ty'\"\n  )\n\n\n\n\n\n\n\n\n\nShow the code\n# RT ~ POS\npos.partial &lt;- pdp::partial(rt.rf.reg, pred.var = \"POS\")\n\npos.partial %&gt;% \n  ggplot(aes(x = POS, y = yhat, group = 1)) +\n  geom_point(col = \"steelblue\") +\n  geom_line(col = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    title = \"Predicted reaction times by POS\",\n    y = \"Predicted reaction time\"\n  )\n\n\n\n\n\n\n\n\n\nShow the code\n# RT ~ Length\nlength.partial &lt;- pdp::partial(rt.rf.reg, pred.var = \"Length\")\n\nlength.partial %&gt;% \n  ggplot(aes(x = Length, y = yhat)) +\n  geom_line(col = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    title = \"Predicted reaction times by word length\",\n    y = \"Predicted reaction time\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nBreiman, Leo. 1984. Classification and Regression Trees. Belmont, Calif.: Wadsworth International Group.\n\n\n———. 2001. “Random Forests.” Machine Learning 45 (1): 5–32. https://doi.org/10.1023/A:1010933404324.\n\n\nDebeer, Dries, and Carolin Strobl. 2020. “Conditional Permutation Importance Revisited.” Bioinformatics 21 (1): 307.\n\n\nGreenwell, Brandon. 2022. Tree-Based Methods for Statistical Learning in r. London & New York: Taylor & Francis Group. https://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=3288358.\n\n\nGreenwell, Brandon M. 2017. “Pdp: An r Package for Constructing Partial Dependence Plots.” The R Journal 9 (1): 421–36.\n\n\nGries, Stefan Thomas. 2021. Statistics for Linguistics with r: A Practical Introduction. 3rd rev. ed. Berlin; Boston: De Gruyter Mouton.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome H. Friedman. 2017. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York, NY: Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. New York: Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nLevshina, Natalia. 2015. How to Do Linguistics with r: Data Exploration and Statistical Analysis. Amsterdam; Philadelphia: John Benjamins Publishing Company.\n\n\n———. 2020. “Conditional Inference Trees and Random Forests.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Thomas Gries, 611–43. Cham: Springer.\n\n\nStrobl, Carolin, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin, and Achim Zeileis. 2008. “Conditional Variable Importance for Random Forests.” BMC Bioinformatics 9 (1): 307.\n\n\nStrobl, Carolin, Anne-Laure Boulesteix, Achim Zeileis, and Torsten Hothorn. 2007. “Bias in Random Forest Variable Importance Measures: Illustrations, Sources and a Solution.” BMC Bioinformatics 8 (1): 25.\n\n\nStrobl, Carolin, James Malley, and Gerhard Tutz. 2009. “An Introduction to Recursive Partitioning: Rationale, Application, and Characteristics of Classification and Regression Trees, Bagging, and Random Forests.” Psychological Methods 14: 323–48. https://doi.org/10.1037/a0016973.",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Decision trees and random forests</span>"
    ]
  },
  {
    "objectID": "Vectors_Factors.html#preparation",
    "href": "Vectors_Factors.html#preparation",
    "title": "7  Vectors",
    "section": "7.1 Preparation",
    "text": "7.1 Preparation\n\n\n\n\n\n\nScript\n\n\n\nYou can find the full R script associated with this unit here.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Vectors_Factors.html#recommended-reading",
    "href": "Vectors_Factors.html#recommended-reading",
    "title": "7  Vectors",
    "section": "7.2 Recommended reading",
    "text": "7.2 Recommended reading\n\nWinter (2020): Chapter 1.1–1.9\n\nSuggested video tutorial:\n\nHow to Create and Name Vectors in R (DataCamp; 5min)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Vectors_Factors.html#word-frequencies-i",
    "href": "Vectors_Factors.html#word-frequencies-i",
    "title": "7  Vectors",
    "section": "7.3 Word frequencies I",
    "text": "7.3 Word frequencies I\nYou are given the following token counts of English verb lemmas in the International Corpus of English.\n\n\n\nLemma\nFrequency\n\n\n\n\nstart\n418\n\n\nenjoy\n139\n\n\nbegin\n337\n\n\nhelp\n281\n\n\n\nWhile this table is relatively small and easy to interpret, it is still a good idea to supply readers with a simple visual representation of the frequency distributions (e.g., a barplot.). Quite conveniently, R happens to provide us with an abundance of plotting functions! In order to make use of them, all we need to do is communicate to R the data we want to visualise. We can supply the data either\n\nby manually listing all the elements of interest or\nautomatically by importing it from an existing spreadsheet file (e.g., from Microsoft Excel).\n\nFor now, we will stick to option 1 and move on to option 2 in a later unit (cf. Section 10.2.2).\n\n7.3.1 Storing data in R\nTo create a two-dimensional plot, we will first need to generate two objects in R: one for the individual lemmas and one for the frequency counts.\nLet’s start by combining the lemmas start, enjoy, begin and help into an object lemma using R’s c() function. Enter the following line into a new R script and click on Run (or simply press Ctrl+Enter/Cmd+Enter).\n\nlemma &lt;- c(\"start\", \"enjoy\", \"begin\", \"help\")\n\nTo make sure this worked, we can apply the print() function to lemma to view the elements it holds:\n\nprint(lemma)\n\n[1] \"start\" \"enjoy\" \"begin\" \"help\" \n\n\nNaturally, it is also possible to combine numeric information with c().\n\nfrequency &lt;- c(418, 139, 337, 281)\n\nThe print() functions allows us to inspect the contents of frequency:\n\nprint(frequency)\n\n[1] 418 139 337 281\n\n\n\n\n\n\n\n\nWhen do I use quotation marks?\n\n\n\n\n\nLetters and numbers represent two distinct data types in R. Anything that should be understood as a simple sequence of letters must be enclosed by quotation marks \"...\". A linguistic item such as start will be will be evaluated as a string if it’s encoded as \"start\".\nNumbers (or integers), by contrast, appear without quotation marks.\n\n\n\n\n\n7.3.2 Creating the barplot\nOur linguistic data is now stored in two variables lemma and frequency, which you can conceptualise as virtual container-like objects. These ‘containers’ are now showing in the Environment tab in the top right corner of your RStudio interface.\nThe combination of categorical labels and numeric information renders our data ideally suited for a barplot. R’s most basic barplot function (which is, unsurprisingly, called barplot()) needs at the very least …\n\na height argument, i.e., our y-axis values and\na names.arg argument, i.e., our x-axis labels.\n\n\nbarplot(frequency, names.arg = lemma)\n\n\n\n\n\n\n\n\nAfter some tinkering, our plot looks more presentable:\n\nbarplot(frequency, names.arg = lemma, \n        main = \"Frequency of Lemmas\", # title\n        xlab = \"Lemmas\",  # label for x-axis\n        ylab = \"Frequency\", # label for y-axis\n        col = \"steelblue\") # color\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does ‘#’ mean? On comments in R\n\n\n\n\n\nIn R, everything followed by the hashtag # will be interpreted as a comment and won’t be evaluated by the R compiler. While comments don’t affect the output of our code in the slightest, they are crucial to any kind of programming project.\nAdding prose annotations will make your code not only easier to understand for others but also for your future self. Poor documentation is a common, yet unnecessary source of frustration for all parties involved …\n\n\n\n\n\n\n\n\nIn RStudio, you now have the option to save the plot to your computer. Once the figure has appeared in your “Plots” panel, you can click on “Export” in the menu bar below and proceed to choose the desired output format and file directory.\n\n\n7.3.3 Essential R concepts\nThe example above demonstrates one of the most important data structures in R: vectors. They form the cornerstone of various more complex objects such as data frames, and are essential to handling large data sets (e.g., corpora). And yet, vectors are very simple in that they are merely one-dimensional sequences of characters or numbers — no more, no less.\n\nprint(lemma)\n\n[1] \"start\" \"enjoy\" \"begin\" \"help\" \n\nprint(frequency)\n\n[1] 418 139 337 281\n\n\nThe individual elements in these two vectors are not randomly jumbling around in virtual space, but are in fact following a clear order. Each element comes with an “ID” (or index), by which it can be accessed. For example, if we want to print the first lemma in our lemma variable, we append square brackets [ ] to it. This will allow us to subset it.\n\nlemma[1]\n\n[1] \"start\"\n\n\nSimilarly, we can subset frequency according to, for example, its third element:\n\nfrequency[3]\n\n[1] 337\n\n\nIt is also possible to obtain entire ranges of elements, such as everything from the second to the fourth element:\n\nfrequency[2:4]\n\n[1] 139 337 281",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Vectors_Factors.html#exercises",
    "href": "Vectors_Factors.html#exercises",
    "title": "7  Vectors",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\n\n\n\n\n\n\nSolutions\n\n\n\nYou can find the solutions to the exercises here.\n\n\n\nExercise 7.1 Create a vector that lists the third person personal pronouns of English (subject and object forms). Store them in a variable pp3.\n\n\nExercise 7.2 Now print …\n\n… the fourth element in pp3.\n… elements 3 through 5.\n… all elements.\n… elements 1, 3 and 5.\n\n\n\nExercise 7.3 When working with large datasets, we often don’t know whether an element is in the vector to begin with, let alone its position. For instance, if we wanted to check whether they is in pp3 or not, we could use the handy notation below, returning a TRUE or FALSE value:\n\n\"they\" %in% pp3\n\nAscertain whether the following items are in pp3:\n\nhim\nyou\nit and them\nwe, us and me\n\n\n\nExercise 7.4 Once we are sure that an element is in the vector of interest, another common problem that arises is finding its location. In this case, we can use which() to return the index of an element.\n\nwhich(pp3 == \"they\")\n\nYou can read the code above as “Which element in pp3 is they?”. Note that the index number depends on the order of elements you’ve chosen when creating pp3.\nFind the locations of it and them in pp3!\n\n\nExercise 7.5 Consider the vector numbers.\n\nnumbers &lt;- c(500:1000)\n\n\nWhat does the following code do? How does the output change when you subset numbers according to this expression?\n\n\nwhich(numbers &gt; 600)\n\n\nDescribe the output of these code chunks:\n\n\nnumbers[numbers != 500]\n\n\nnumbers[numbers &gt; 500 & numbers &lt; 550]\n\n\nnumbers[numbers &lt; 510 | numbers &gt; 990]\n\n\n\n\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using r. New York; London: Routledge.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "First_steps.html#why-learn-r-to-begin-with",
    "href": "First_steps.html#why-learn-r-to-begin-with",
    "title": "5  First steps",
    "section": "5.1 Why learn R to begin with?",
    "text": "5.1 Why learn R to begin with?\nWhen it comes to data analysis, learning R offers an overwhelming number of short- and long-term advantages over conventional spreadsheet software such as Microsoft Excel or LibreOffice Calc:\n\nFirst of all, it’s completely free. There’s no need to obtain any expensive licenses, as it is the case for commercial software such as SPSS or MS Excel.\nR makes it very easy to document and share every step of the analysis, thereby facilitating reproducible workflows.\nLarge (and by that I mean extremely large!) datasets pose no problems whatsoever. Loading tabular data with hundreds of thousands (or even millions) of rows only takes a few seconds, whereas most other software would crash.\nThere are numerous extensions that provide tailored functions for corpus linguistics that aren’t available in general-purpose spreadsheet software. This allows us to work with corpora, use complex search expression, perform part-of-speech annotation, dependency parsing, and much more – all from within R.\nR’s ggplot2 offers an incredibly powerful framework for data visualisation. Don’t believe it? Check out the ggplot2 gallery.\nThe CRAN repository features more than 20,000 packages that can be installed to expand the functionality of R almost indefinitely. Should none of them meet your needs, R gives you the tools to comfortably write and share your own functions and packages.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>First steps</span>"
    ]
  },
  {
    "objectID": "First_steps.html#installing-r",
    "href": "First_steps.html#installing-r",
    "title": "5  First steps",
    "section": "5.2 Installing R",
    "text": "5.2 Installing R\nThe first step involves downloading the R programming language itself. The link will take you to the homepage of the Comprehensive R Archive Network (CRAN) where you can download the binary distribution. Choose the one that corresponds to your operating system (Windows/MAC/Linux).\n\n\n\n\n\n\nInstallation instructions for Windows users\n\n\n\n\n\nClick “Download R for Windows” \\(\\rightarrow\\) Select “base” \\(\\rightarrow\\) Click on “Download R-4.4.1 for Windows” (or whatever most recent version is currently displayed).\nOpen the set-up file you’ve just downloaded and simply follow the instructions on screen. It’s fine to go with the default options.\nVideo tutorial on YouTube\n\n\n\n\n\n\n\n\n\nInstallation instructions for MacOS users\n\n\n\n\n\nClick “Download R for macOS” \\(\\rightarrow\\) Select the latest release for your OS\nOpen the downloaded .pkg file and follow the instructions in the installation window.\nVideo tutorial on YouTube",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>First steps</span>"
    ]
  },
  {
    "objectID": "First_steps.html#installing-rstudio",
    "href": "First_steps.html#installing-rstudio",
    "title": "5  First steps",
    "section": "5.3 Installing RStudio",
    "text": "5.3 Installing RStudio\nYou can now download and install RStudio. RStudio is a so-called “Integrated Development Environment” (IDE), which will provide us with a variety of helpful tools to write and edit code comfortably. If R was a musical instrument, then RStudio would be the recording studio, so-to-speak.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>First steps</span>"
    ]
  },
  {
    "objectID": "Importing_exporting.html#recommended-reading",
    "href": "Importing_exporting.html#recommended-reading",
    "title": "10  Import/export data",
    "section": "10.1 Recommended reading",
    "text": "10.1 Recommended reading\n\nWinter (2020): Chapter 1.11",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Import/export data</span>"
    ]
  },
  {
    "objectID": "Importing_exporting.html#preparation",
    "href": "Importing_exporting.html#preparation",
    "title": "10  Import/export data",
    "section": "10.2 Preparation",
    "text": "10.2 Preparation\nThe first section of an R script should always specify the libraries that are needed for executing the code to follow. In this unit, we will need readxl and writexl to aid us with importing MS Excel files.\n\nlibrary(readxl)\nlibrary(writexl)\n\nIf you haven’t installed them yet, the R console will throw an error message. For instructions on how to install an R package, consult the unit on Libraries.\n\n10.2.1 Exporting data\nAssume we’d like to export our data frame with word frequencies to a local file on our system. Let’s briefly regenerate the data frame:\n\n# Generate data frame\ndata &lt;- data.frame(lemma = c(\"start\", \"enjoy\", \"begin\", \"help\"), \n                   frequency = c(418, 139, 337, 281))\n\n# Print contents\nprint(data)\n\n  lemma frequency\n1 start       418\n2 enjoy       139\n3 begin       337\n4  help       281\n\n\nThere are two common formats in which tabular data can be stored:\n\nas .csv-files (‘comma-separated values’; native format of LibreOffice Calc)\nas .xls/.xlsx-files (Microsoft Excel)\n\n\n\n\n\n\n\nExport to CSV\n\n\n\n\n\nTo save our data data frame in .csv-format, we can use the write_table() function:\n\nwrite.csv(data, \"frequency_data.csv\")\n\nThe file is now stored at the location of your current R script. You can open this file …\n\nin LibreOffice\nin Microsoft Excel via File &gt; Import &gt; CSV file &gt; Select the file &gt; Delimited and then Next &gt; Comma and Next &gt; General and Finish.\n\nClearly, opening CSV files in MS Excel is quite cumbersome, which is why it’s better to export it as an Excel file directly.\n\n\n\n\n\n\n\n\n\nExport to Excel\n\n\n\n\n\nWe use the write_xlsx() function provided by the package writexl.\n\nwrite_xlsx(data, \"frequency_data.xlsx\")\n\nThe file is now stored at the location of your currently active R script. You should now be able to open it in MS Excel without any issues.\n\n\n\n\n\n10.2.2 Importing data\nLet’s read the two files back into R.\n\n\n\n\n\n\nImport from CSV\n\n\n\n\n\nTo import the CSV file, we can use the read.csv() function:\n\nimported_csv &lt;- read.csv(\"frequency_data.csv\")\nprint(imported_csv)\n\n  X lemma frequency\n1 1 start       418\n2 2 enjoy       139\n3 3 begin       337\n4 4  help       281\n\n\nIt appears that read.csv() has also written the row numbers to the file. This is not the desired outcome and can be prevented by adding an additional argument:\n\nimported_csv &lt;- read.csv(\"frequency_data.csv\", row.names = 1)\nprint(imported_csv) # Problem solved!\n\n  lemma frequency\n1 start       418\n2 enjoy       139\n3 begin       337\n4  help       281\n\n\n\n\n\n\n\n\nA note on file encodings and separators\n\n\n\n\n\nWhen working with CSV files, you may encounter issues with character encodings and separators, especially when:\n\nworking with files from different operating systems,\ndealing with text containing special characters (é, ü, ñ, etc.), or\nimporting files created in different regions (e.g., European vs. US).\n\nThe most common encoding-related parameters for read.csv() are:\n\n# For files with special characters (recommended default)\ndata &lt;- read.csv(\"myfile.csv\", encoding = \"UTF-8\")\n\n# For files from Windows systems\ndata &lt;- read.csv(\"myfile.csv\", encoding = \"latin1\")\n\n# For files using semicolons and commas as decimal points\ndata &lt;- read.csv(\"myfile.csv\", sep = \";\", dec = \",\")\n\n\nIf you see garbled text like Ã© instead of é, try specifying encoding = \"UTF-8\".\nIf your data appears in a single column, check if your file uses semicolons (;) instead of commas (,) as separators.\nIf numeric values are incorrect, verify whether the file uses commas or periods as decimal separators.\n\n\n\n\n\n\n\n\n\n\n\n\n\nImport from Excel\n\n\n\n\n\nFor importing the Excel file, we’ll use the read_xlsx() function from the readxl package:\n\nimported_excel &lt;- read_xlsx(\"frequency_data.xlsx\")\nprint(imported_excel)\n\n# A tibble: 4 × 2\n  lemma frequency\n  &lt;chr&gt;     &lt;dbl&gt;\n1 start       418\n2 enjoy       139\n3 begin       337\n4 help        281\n\n\n\n\n\nThat’s it! Nevertheless, remember to always check your imported data to ensure it has been read in correctly, especially when working with CSV files.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Import/export data</span>"
    ]
  },
  {
    "objectID": "Data_frames.html#preparation",
    "href": "Data_frames.html#preparation",
    "title": "8  Data frames",
    "section": "8.1 Preparation",
    "text": "8.1 Preparation\n\n\n\n\n\n\nScript\n\n\n\nYou can find the full R script associated with this unit here.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "Data_frames.html#recommended-reading",
    "href": "Data_frames.html#recommended-reading",
    "title": "8  Data frames",
    "section": "8.2 Recommended reading",
    "text": "8.2 Recommended reading\n\nWinter (2020): Chapter 1.10-1.16\n\nSuggested video tutorial:\n\nUsing the Data Frame in R (DataCamp, 5min)\nLearn How to Subset, Extend & Sort Data Frames in R (DataCamp, 7min)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "Data_frames.html#word-frequencies-ii",
    "href": "Data_frames.html#word-frequencies-ii",
    "title": "8  Data frames",
    "section": "8.3 Word frequencies II",
    "text": "8.3 Word frequencies II\nRecall our corpus-linguistic data from the previous unit:\n\n\n\nLemma\nFrequency\n\n\n\n\nstart\n418\n\n\nenjoy\n139\n\n\nbegin\n337\n\n\nhelp\n281\n\n\n\nWe thought of the columns as one-dimensional, indexed lists of elements:\n\nlemma &lt;- c(\"start\", \"enjoy\", \"begin\", \"help\")\n\nfrequency &lt;- c(418, 139, 337, 281)\n\nActually, R allows us to combine these two vectors into something that resembles a real spreadsheet. To this end, we apply the data.frame() function to two vectors of our choice.\n\ndata &lt;- data.frame(lemma, frequency)\n\nprint(data)\n\n  lemma frequency\n1 start       418\n2 enjoy       139\n3 begin       337\n4  help       281\n\n\n\n8.3.1 Essential R concepts\nThe variable data is no longer a vector, but a data frame (often abbreviated as ‘df’). Once again, each element carries its own label and can, therefore, be accessed or manipulated.\nSince we now have two dimensions, the subsetting notation in square brackets [ ] has to reflect that. This is the general pattern:\n\\[ \\text{df[row, column]}\n\\tag{8.1}\\]\nSay, we’re looking for the element at the intersection of the first row and first column. Applying the pattern above, we can access it like so:\n\ndata[1,1]\n\n[1] \"start\"\n\n\nBut what if we need the entire first row? We simply omit the column part. Note, however, that the comma , needs to remain:\n\ndata[1,]\n\n  lemma frequency\n1 start       418\n\n\nSubsetting by columns is interesting. We can either use the square bracket notation [ ] or the column operator $:\n\ndata[,1]\n\n[1] \"start\" \"enjoy\" \"begin\" \"help\" \n\ndata$lemma\n\n[1] \"start\" \"enjoy\" \"begin\" \"help\" \n\n\n\n\n8.3.2 Filtering\nNot all the information contained in a data frame is always relevant for our research. In those cases, it’s important to subset the rows and columns according to certain criteria.\nAssume we only need those observations where the lemma frequencies are greater than 300. We can obtain those by specifying\n\nthe data frame,\nthe column of interest and\nthe condition to apply.\n\nYou can read the code below as\n\nTake the data frame data and subset it according to the column data$frequency. Show me those rows where the values of data$frequency are greater than 300.\n\n\ndata[data$frequency &gt; 300, ]\n\n  lemma frequency\n1 start       418\n3 begin       337\n\n\nWhat if we wanted to filter by lemma instead? Let’s say we’re looking for frequency data on the verbs start and help.\nThis will give us the row associated with start:\n\ndata[data$lemma == \"start\", ]\n\n  lemma frequency\n1 start       418\n\n\nCombining multiple statements requires a logical operator. Here we’re using | , which corresponds to a logical ‘or’ (disjunction).\n\ndata[data$lemma == \"start\" | data$lemma == \"help\", ]\n\n  lemma frequency\n1 start       418\n4  help       281\n\n\n\n\n\n\n\n\nWhy do we need to use “or” (|) and not “and” (&)?\n\n\n\nThe idea of combining statements somewhat naturally suggests a conjunction, which could be achieved via &. How come R doesn’t return anything if we do it that way?\n\ndata[data$lemma == \"start\" & data$lemma == \"help\", ]\n\n[1] lemma     frequency\n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\n\n\n8.3.3 I don’t like the way this looks – is there another way to filter in R?\nYes, absolutely. The subsections below demonstrate a few popular alternatives. In the end, the exact way you filter doesn’t really matter, so long as you (as well as the people who have to work with your script) can understand what you’re trying to achieve. Always consider adding comments to your filtering operations!\n\n\n\n\n\n\nsubset()\n\n\n\n\n\nAlmost every subsetting operation we perform with square brackets can also be performed using the subset() function. Here are some expressions that are synonymous to the ones above:\n\nsubset(data, frequency &gt; 300)\n\n  lemma frequency\n1 start       418\n3 begin       337\n\nsubset(data, lemma == \"start\" | lemma == \"help\")\n\n  lemma frequency\n1 start       418\n4  help       281\n\n\n\n\n\n\n\n\n\n\n\ntidyverse\n\n\n\n\n\nThe tidyverse-ecosystem is a collection of packages specifically designed for handling typical data science tasks as comfortably and elegantly as possible, supplying countless helper functions for data manipulation, transformation and visualisation. Installation instructions are provided in Section 9.2.1.\nIt offers some appealing alternatives to the Base R subsetting functions. Let’s generate a tidyverse-style data frame, the tibble:\n\nlibrary(tidyverse)\n\ndata2 &lt;- tibble(\n  lemma = c(\"start\", \"enjoy\", \"begin\", \"help\"),\n  frequency = c(418, 139, 337, 281)\n)\n\nprint(data2)\n\n# A tibble: 4 × 2\n  lemma frequency\n  &lt;chr&gt;     &lt;dbl&gt;\n1 start       418\n2 enjoy       139\n3 begin       337\n4 help        281\n\n\nWe can single out certain columns by using select():\n\nselect(data2, lemma)\n\n# A tibble: 4 × 1\n  lemma\n  &lt;chr&gt;\n1 start\n2 enjoy\n3 begin\n4 help \n\n\nIt is very easy to filter the data frame according to certain criteria:\n\nfilter(data2, frequency &gt; 300)\n\n# A tibble: 2 × 2\n  lemma frequency\n  &lt;chr&gt;     &lt;dbl&gt;\n1 start       418\n2 begin       337\n\nfilter(data2, lemma == \"start\" | lemma == \"help\")\n\n# A tibble: 2 × 2\n  lemma frequency\n  &lt;chr&gt;     &lt;dbl&gt;\n1 start       418\n2 help        281\n\n\nA extensive guide to the main tidyverse functions is provided in Chapter 3 of the free eBook R For Data Science (2nd edition).",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "Data_frames.html#exercises",
    "href": "Data_frames.html#exercises",
    "title": "8  Data frames",
    "section": "8.4 Exercises",
    "text": "8.4 Exercises\n\n\n\n\n\n\nSolutions\n\n\n\nYou can find the solutions to the exercises here.\n\n\n\nExercise 8.1 Recreate the barplot from the previous unit by subsetting the data variable accordingly.\n\n\nExercise 8.2 Print the following elements by subsetting the data frame data accordingly.\n\n337\nbegin\nenjoy\nenjoy 139\nthe entire frequency column\n\n\n\nExercise 8.3 Extension of Exercise 7.3. Verify that the following verbs are represented in the lemma column: enjoy, hit, find, begin. If they are in the data frame, print their frequency information.\n\n\nExercise 8.4 Extension of Exercise 7.4. Use which() to find the rows where the frequency is greater than 200, and then print the lemma and frequency of only those rows.\n\n\n\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using r. New York; London: Routledge.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "Chi_square_test.html#preparation",
    "href": "Chi_square_test.html#preparation",
    "title": "19  Chi-squared test",
    "section": "19.2 Preparation",
    "text": "19.2 Preparation\n\n\n\n\n\n\nScript\n\n\n\nYou can find the full R script associated with this unit here.\n\n\n\n#  Load libraries\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(confintr) # for effect size calculation\n\n# Load data\ncl.order &lt;- read_xlsx(\"Paquot_Larsson_2020_data.xlsx\")",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Chi-squared test</span>"
    ]
  },
  {
    "objectID": "Chi_square_test.html#overview",
    "href": "Chi_square_test.html#overview",
    "title": "19  Chi-squared test",
    "section": "19.2 Overview",
    "text": "19.2 Overview\n\n\n\n\n\n\nThis unit in a nutshell\n\n\n\n\n\nThe chi-square test helps determine if there is a statistically significant association between two categorical variables. It compares observed frequencies (actual data) with expected frequencies (assuming the null hypothesis were true). The \\(\\chi^2\\) (chi-squared) score quantifies the difference between them for every cell in the table according to this simplified formula:\n\\[\n\\text{chi-squared =}\\frac{(observed - expected)^2}{expected}\n\\tag{19.1}\\]\nThe greater the difference between observed and expected, the higher the \\(\\chi^2\\) score, the more evidence we obtain against \\(H_0\\). We should always report the following three elements:\n\n\\(p\\)-value: Indicates significance level.\n\\(\\chi^2\\) score: Shows the deviation between observed and expected frequencies.\nDegrees of freedom: Affects the shape of the chi-square distribution.\n\nIf the \\(p\\)-value is below 0.05, we reject \\(H_0\\), indicating a significant association between the variables.\nExample:\n\n# Hypotheses:\n# H0: ORDER and SUBORDTYPE are independent.\n# H1: ORDER and SUBORDTYPE are not independent.\n\nchisq.test(cl.order$ORDER, cl.order$SUBORDTYPE)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  cl.order$ORDER and cl.order$SUBORDTYPE\nX-squared = 104.24, df = 1, p-value &lt; 2.2e-16\n\n# We can reject H0 and accept H1.",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Chi-squared test</span>"
    ]
  },
  {
    "objectID": "Chi_square_test.html#the-chi2-test",
    "href": "Chi_square_test.html#the-chi2-test",
    "title": "19  Chi-squared test",
    "section": "19.3 The \\(\\chi^2\\)-test",
    "text": "19.3 The \\(\\chi^2\\)-test\nRecall our statistical hypotheses stated in the previous unit:\n\n\\(H_0:\\) The variables ORDER and SUBORDTYPE are independent.\n\\(H_1:\\) The variables ORDER and SUBORDTYPE are not independent.\n\nThe next step is to compute a test statistic that indicates how strongly our data conforms to \\(H_0\\), such as Pearson’s \\(\\chi^2\\). In order to compute it, we need two types of values: the observed frequencies \\(n_{ij}\\) in our data set and the expected frequencies \\(m_{ij}\\), which we would expect to see if \\(H_0\\) were true, with \\(n, m \\in \\mathbb{N}\\). The indices \\(i\\) and \\(j\\) uniquely identify the frequencies found in all column-row combinations of a contingency table.\n\n\n\n\n\n\nThe structure of a contingency table\n\n\n\n\n\nThe table below represents a generic contingency table where \\(X\\) and \\(Y\\) are categorical variables. Each \\(x_i\\) represents a category of \\(X\\) and each \\(y_j\\) represents a category of \\(Y\\). In the table, each cell indicates the count of observation \\(n_{ij}\\) corresponding to the \\(i\\)-th row and \\(j\\)-th column.\n\n\n\n\n\n\n\\(Y\\)\n\n\n\n\n\n\n\n\n\n\\(y_1\\)\n\\(y_2\\)\n…\n\\(y_J\\)\n\n\n\n\n\\(x_1\\)\n\\(n_{11}\\)\n\\(n_{12}\\)\n…\n\\(n_{1J}\\)\n\n\n\n\n\\(x_2\\)\n\\(n_{21}\\)\n\\(n_{22}\\)\n…\n\\(n_{2J}\\)\n\n\n\n\\(X\\)\n…\n…\n…\n…\n…\n\n\n\n\n\\(x_I\\)\n\\(n_{I1}\\)\n\\(n_{I2}\\)\n…\n\\(n_{3J}\\)\n\n\n\n\n\n\n\nIn the cl.order data, the observed frequencies correspond to how often each ORDER value (i.e., mc-sc and sc-mc) is attested for a given SUBORDTYPE (i.e., temp and caus). This can be done in a very straightforward fashion using R’s table() function on the variables of interest.\n\nobserved_freqs &lt;- table(cl.order$ORDER, cl.order$SUBORDTYPE)\n\nprint(observed_freqs)\n\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n\n\nThe expected frequencies require a few additional steps. Usually, these steps are performed automatically when conducting the chi-squared test in R, so you don’t have to worry about calculating them by hand. We will do it anyway to drive home the rationale of the test.\nThe expected frequencies \\(m_{ij}\\) are given by the formula in Equation 19.1. In concrete terms, we go through each cell in the cross-table and multiply the corresponding row sums with the column sums, dividing the result by the total number of occurrences in the sample. For example, there are \\(184\\) occurrences of mc-sc clause orders where the subordinate clause is causal. The row sum is \\(184 + 91 = 275\\) and the column sum is \\(184 + 15 = 199\\). Next, we take their product \\(275 \\times 199\\) and divide it by the total number of observations, which is \\(184 + 91 + 15 + 113 = 403\\). Thus we obtain an expected frequency of \\(\\frac{275 \\times 199}{403} = 135.79\\) under the null hypothesis.\n\\[\nm_{ij} = \\frac{i\\textrm{th row sum} \\times j \\textrm{th column sum}}{\\textrm{number of observations}}.\n\\tag{19.1}\\]\nThe expected frequencies for our combination of variables is shown below. In which cells can you see the greatest deviations between observed and expected frequencies?\n\n\nShow the code\n## Calculate row totals\nrow_totals &lt;- rowSums(observed_freqs)\n\n## Calculate column totals\ncol_totals &lt;- colSums(observed_freqs)\n\n## Total number of observations\ntotal_obs &lt;- sum(observed_freqs)\n\n## Calculate expected frequencies\nexpected_freqs &lt;- outer(row_totals, col_totals) / total_obs\n\nprint(expected_freqs)\n\n\n           caus      temp\nmc-sc 135.79404 139.20596\nsc-mc  63.20596  64.79404\n\n\nThe \\(\\chi^2\\)-test now offers a convenient way of quantifying the differences between the two tables above. It measures how much the observed frequencies deviate from the expected frequencies for each cell in a contingency table (cf. Heumann, Schomaker, and Shalabh 2022: 249-251). The gist of this procedure is summarised in Equation 19.2.\n\\[\n\\text{Chi-squared } \\chi^2 =\\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\n\\tag{19.2}\\]\n\n\n\n\n\n\nFormal definition of the chi-squared test\n\n\n\n\n\nGiven \\(n\\) observations and \\(k\\) degrees of freedom \\(df\\), the joint deviations between \\(n_{ij}\\) \\(m_{ij}\\) contribute to the final \\(\\chi^2\\)-score, which is defined as\n\\[\n\\chi^2 = \\sum_{i=1}^{I}\\sum_{i=j}^{J}{\\frac{(n_{ij} - m_{ij})^2}{m_{ij}}}\n\\tag{19.3}\\]\nfor \\(i = 1, ..., I\\) and \\(j = 1, ..., J\\) and \\(df = (\\textrm{number of rows} -1) \\times (\\textrm{number of columns} - 1)\\).\n\n\n\nThe implementation in R is a simple one-liner. Keep in mind that we have to supply absolute frequencies to chisq.test() rather than percentages.\n\nfreqs_test &lt;- chisq.test(observed_freqs)\n\nprint(freqs_test)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  observed_freqs\nX-squared = 104.24, df = 1, p-value &lt; 2.2e-16\n\n\nQuite conveniently, the test object freqs_test stores the expected frequencies, which can be easily accessed via subsetting. Luckily, they are identical to what we calculated above!\n\nfreqs_test$expected\n\n       \n             caus      temp\n  mc-sc 135.79404 139.20596\n  sc-mc  63.20596  64.79404\n\n\n\n\n\n\n\n\nAssumptions of the chi-squared test\n\n\n\nThis \\(\\chi^2\\)-test comes with certain statistical assumptions. Violations of these assumptions decrease the validity of the result and could, therefore, lead to wrong conclusions about relationships in the data. In this case, other tests should be consulted.\n\nAll observations are independent of each other.\n80% of the expected frequencies are \\(\\geq\\) 5.\nAll observed frequencies are \\(\\geq\\) 1.\n\nIf assumptions 2 and 3 are violated, it is recommended to use a more robust test such as the Fisher’s Exact Test (see ?fisher.test() for details) or the log-likelihood test (\\(G\\)-test).\nIn case of dependent observations (e.g., multiple measurements per participant), the default approach is to fit a multilevel model that can control for grouping factors (see mixed-effects regression in Section 23.3.)\n\n\n\n19.3.1 How do I make sense of the test results?\nThe test output has three ‘ingredients’:\n\nthe chi-squared score (X-squared)\nthe degrees of freedom (df)\nthe p-value.\n\nIt is absolutely essential to report all three of those as they determine each other. Here is an example:\n\nAccording to a \\(\\chi^2\\)-test, there is a highly significant association between clause ORDER and SUBORDTYPE at \\(p &lt; 0.001\\) (\\(\\chi^2 = 106.44, df = 1\\)), thus justifying the rejection of \\(H_0\\).\n\nThe test results provide evidence that the dependent variable ORDER and the explanatory variable SUBORDTYPE are not independent of each other. The probability of randomly observing usage patterns such as those found in the cl.order data is lower than 0.001 \\(\\approx\\) 0.1%, which is enough to reject the null hypothesis at \\(\\alpha = 0.05\\).\nWe can infer that a speaker’s choice of clause ORDER is very likely influenced by the semantic type of subordinate clause; in other words, these two variables are correlated. However, there are still several things the test does not tell us:\n\nAre there certain variable combinations where the \\(\\chi^2\\)-scores are particularly high?\nHow strongly do ORDER and SUBORDTYPE influence each other?\nDoes a causal subordinate clause make the mc-sc clause order more likely?",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Chi-squared test</span>"
    ]
  },
  {
    "objectID": "Chi_square_test.html#typical-workflow",
    "href": "Chi_square_test.html#typical-workflow",
    "title": "19  Chi-squared test",
    "section": "19.3 Typical workflow",
    "text": "19.3 Typical workflow\n\n19.3.1 Define hypotheses\n\n\\(H_0:\\) The variables ORDER and SUBORDTYPE are independent.\n\\(H_1:\\) The variables ORDER and SUBORDTYPE are not independent.\n\n\n\n19.3.2 Cross-tabulate the categories of interest\n\n# Cross-tabulate the frequencies for the variables of interest\nfreqs &lt;- table(cl.order$ORDER, cl.order$SUBORDTYPE)\n\nfreqs ## Assumption met: all observed freqs =&gt; 1\n\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n\n\n\n\n19.3.3 Run the test\n\n# Run a chis-quared test on the absolute frequencies and print the results\ntest &lt;- chisq.test(freqs, correct = FALSE)\n\n# Inspect expected frequencies\ntest$expected # Assumption met: all expected frequences =&gt; 5\n\n       \n             caus      temp\n  mc-sc 135.79404 139.20596\n  sc-mc  63.20596  64.79404\n\n\n\n\n\n\n\n\nAdvanced: Effect size\n\n\n\n\n\nThe sample-size independent effect size measure Cramer’s V (\\(\\phi\\)) is defined as\n\\[V = \\sqrt{\\frac{\\chi^2}{N \\times df}}.\n\\tag{19.4}\\]\nThe outcome varies between \\(0\\) (= no correlation) and \\(1\\) (= perfect correlation); cf. also Gries (2013: 186).\n\n\nCode\n# Compute Cramer's V\nlibrary(\"confintr\") # Load library\n\ncramersv(test)\n\n\n[1] 0.5139168\n\n\n\n\n\n\n19.3.3.1 Reporting the results\nAccording to a \\(\\chi^2\\)-test, there is a highly significant association between clause ORDERand SUBORDTYPE at \\(p &lt; 0.001\\) (\\(\\chi^2 = 106.44, df = 1\\)), thus justifying the rejection of \\(H_0\\).\n\n\n\n\nGries, Stefan Thomas. 2013. Statistics for Linguistics with r: A Practical Introduction. 2nd rev. ed. Berlin; Boston: De Gruyter Mouton.\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Chi-squared test</span>"
    ]
  },
  {
    "objectID": "Summary_statistics.html#preparation",
    "href": "Summary_statistics.html#preparation",
    "title": "16  Continuous data",
    "section": "16.1 Preparation",
    "text": "16.1 Preparation\nWe will use the dataset from the previous unit:\n\n# Libraries\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\n\n# Load data from working directory\ncl.order &lt;- read_xlsx(\"Paquot_Larsson_2020_data.xlsx\")",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Continuous data</span>"
    ]
  },
  {
    "objectID": "Summary_statistics.html#measures-of-central-tendency",
    "href": "Summary_statistics.html#measures-of-central-tendency",
    "title": "16  Continuous data",
    "section": "16.2 Measures of central tendency",
    "text": "16.2 Measures of central tendency\n\n16.2.1 The mean\nA useful summary statistic is the arithmetic mean \\(\\bar{x}\\) (cf. Heumann, Schomaker, and Shalabh 2022: 38). Consider a continuous variable \\(X\\) with observations \\(x_1, x_2, ..., x_n\\) from a sample of size \\(n\\). The sample mean \\(\\bar{x}\\) then corresponds to\n\\[\n\\bar{x} = \\frac{x_1 + x_2 + ... + x_n}{n} \\\\ = \\frac{1}{n}\\sum_{i=1}^n{x_i}.\n\\]\n\n\n\n\n\n\nHold on, what does this large Σ symbol mean?\n\n\n\nThe summation symbol \\(\\sum\\) is a concise way to express the sum of a sequence of numbers. It works like this:\n\nThe expression below the \\(\\sum\\) (e.g., \\(i = 1\\)) indicates the starting value of the index \\(i\\).\nThe number above the \\(\\sum\\) (e.g., \\(n\\)) is the ending value of the index \\(i\\).\nThe expression to the right of the \\(\\sum\\) (e.g., \\(x_i\\)) is the term to be summed.\n\nFor instance, the expression \\(\\sum_{i=1}^n{x_i}\\) means “sum the values of \\(x_i\\) starting from \\(i = 1\\) up to \\(i = n\\).” In other words, it adds up the values \\(x_1 + x_2 + \\dots + x_n\\).\n\n\nIn R, we can obtain the average value of a numeric vector with the mean() function. Let’s do that for the length of main clauses found in the cl.order data:\n\nmean(cl.order$LEN_MC)\n\n[1] 9.265509\n\n\nThe output returned by this function provides a one-value summary of all observations contained in LEN_MC. How could we visualise it?\nFirst, let’s create a histogram of LEN_MC. Histograms summarise how often certain values occur in a continuous variable. The blue line indicates the sample mean. Alternatively, you can use a density plot to represent the probability of encountering a specific value in the data.\n\nHistogram (ggplot2)Density plot (ggplot2)Histogram (Base R)Density plot (Base R)\n\n\n\n# Plot distribution of LEN_MC\ncl.length.hist &lt;- ggplot(cl.order, aes(x = LEN_MC)) +\n                  geom_histogram(binwidth = 2)\n\ncl.length.hist +\n  # Add mean\n  geom_vline(aes(xintercept = mean(LEN_MC)),\n             color = \"steelblue\",\n             linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n# Plot distribution of LEN_MC\ncl.length.dens &lt;- ggplot(cl.order, aes(x = LEN_MC)) +\n                  geom_density()\n\ncl.length.dens +\n  # Add mean\n  geom_vline(aes(xintercept = mean(LEN_MC)),\n             color = \"steelblue\",\n             linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nhist(cl.order$LEN_MC)\n  abline(v=mean(cl.order$LEN_MC),lwd=3, col = \"steelblue\")\n\n\n\n\n\n\n\n\n\n\n\nplot(density(cl.order$LEN_MC))\n  abline(v=mean(cl.order$LEN_MC),lwd=3, col = \"steelblue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.2.2 The median\nThe median() function computes the “the halfway point of the data (50% of the data are above the median; 50% of the data are below” (Winter 2020: 58).\n\\[\n\\tilde{x}_{0.5} =\n\\begin{cases}\nx_{((n+1)/2)} & \\text{if } n \\text{ is odd.} \\\\\n\\frac{1}{2}(x_{n/2}+x_{(n/2+1)}) & \\text{if } n \\text{ is even.}\n\\end{cases}\n\\]\n\nmedian(cl.order$LEN_MC)\n\n[1] 8\n\n\nThe median of LEN_MC is represented by the red vertical line.\n\nHistogram (ggplot2)Density plot (ggplot2)Histogram (Base R)Density plot (Base R)\n\n\n\ncl.length.hist +\n  # Add mean\n  geom_vline(aes(xintercept = mean(LEN_MC)), color = \"steelblue\", linewidth = 1) +\n  # Add median\n  geom_vline(aes(xintercept = median(LEN_MC)), color = \"red\", linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\ncl.length.dens +\n  # Add mean\n  geom_vline(aes(xintercept = mean(LEN_MC)), color = \"steelblue\", linewidth = 1) +\n  # Add median\n  geom_vline(aes(xintercept = median(LEN_MC)), color = \"red\", linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nhist(cl.order$LEN_MC)\n  abline(v=mean(cl.order$LEN_MC),lwd=3, col = \"steelblue\")\n  abline(v=median(cl.order$LEN_MC),lwd=3, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nplot(density(cl.order$LEN_MC))\n  abline(v=mean(cl.order$LEN_MC),lwd=3, col = \"steelblue\")\n  abline(v=mean(cl.order$LEN_MC),lwd=3, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.2.3 Sample variance and standard deviation\nIn order to assess how well the mean represents the data, it is instructive to compute the variance var() and the standard deviation sd() for a sample.\nThe sample variance \\(s^2\\) is defined as\n\\[s^2 = \\frac{1}{n}\\sum_{i=1}^n{(x_i - \\bar{x})^2}. \\]\nIn other words, it stands for the average squared deviation of all observations from the sample mean.\n\nvar(cl.order$LEN_MC)\n\n[1] 25.12585\n\n\nCorrespondingly, the standard deviation of the mean is the square root of the variance (cf. Heumann, Schomaker, and Shalabh 2022: 51-2).\n\\[ s = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n{(x_i - \\bar{x})^2}} \\]\n\nsd(cl.order$LEN_MC)\n\n[1] 5.012569\n\n\nVisualisation:\n\nExample 1Example 2\n\n\n\ncl.length.hist +\n  # Add verticle line for the mean\n  geom_vline(aes(xintercept = mean(LEN_MC)), color = \"steelblue\", linewidth = 1) +\n  # Add -1sd\n  geom_vline(aes(xintercept = mean(LEN_MC) - sd(LEN_MC)), color = \"orange\", linewidth = 1) +\n  # Add +1sd\n  geom_vline(aes(xintercept = mean(LEN_MC) + sd(LEN_MC)), color = \"orange\", linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n# Create data frame with mean and sd for each clause ORDER\n\ncl.order %&gt;% \n  # Select variables of interest\n  select(ORDER, LEN_MC) %&gt;% \n  # Group results of following operations by ORDER\n  group_by(ORDER) %&gt;% \n    # Create grouped summary of mean and sd for each ORDER\n    summarise(mean = mean(LEN_MC),\n                sd = sd(LEN_MC)) -&gt; cl_mean_sd; cl_mean_sd\n\n# A tibble: 2 × 3\n  ORDER  mean    sd\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 mc-sc  9.04  4.91\n2 sc-mc  9.75  5.22\n\n# Plot results \n\nggplot(cl_mean_sd, aes(x = ORDER, y = mean)) +\n  # Barplot with a specific variable mapped onto y-axis\n  geom_col() +\n  # Add mean and standard deviation to the plot\n  geom_errorbar(aes(x = ORDER,\n                    ymin = mean-sd,\n                    ymax = mean+sd), width = .2) +\n  theme_classic() +\n  labs(y = \"Mean length of main clauses\", x = \"Clause order\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.2.4 Quantiles\nWhile median() divides the data into two equal sets (i.e., two 50% quantiles), the quantile() function makes it possible to partition the data further.\n\nquantile(cl.order$LEN_MC)\n\n  0%  25%  50%  75% 100% \n   2    6    8   11   31 \n\n\nquantile(x, 0) and quantile(x, 1) thus show the minimum and maximum values, respectively.\n\nquantile(cl.order$LEN_MC, 0)\n\n0% \n 2 \n\nquantile(cl.order$LEN_MC, 1)\n\n100% \n  31 \n\n\n\n\n16.2.5 Quartiles and boxplots\nConsider the distribution of clause length by clause order:\n\nBoxplot (Base R)Boxplot (ggplot2)\n\n\n\nboxplot(LEN_MC ~ ORDER, cl.order)\n\n\n\n\n\n\n\n\n\n\n\nggplot(cl.order, aes(x = ORDER, y = LEN_MC)) +\n  geom_boxplot() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nCompare it to the corresponding rotated density plot:\n\nggplot(cl.order, aes(x = LEN_MC, fill = ORDER)) +\n  geom_density(alpha = 0.5) +\n  coord_flip() +\n  theme_classic()",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Continuous data</span>"
    ]
  },
  {
    "objectID": "Summary_statistics.html#visualising-mixed-data",
    "href": "Summary_statistics.html#visualising-mixed-data",
    "title": "16  Continuous data",
    "section": "16.3 Visualising mixed data",
    "text": "16.3 Visualising mixed data\n\n16.3.1 A numerical and categorical variable\n\nBoxplot with geom_boxplot()\n\n\nggplot(cl.order, aes(x = ORDER, y = LEN_MC)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nDensitiy plot using the optional arguments color and/or fill\n\n\nggplot(cl.order, aes(x = LEN_MC, fill = ORDER)) +\n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nA barplot with geom_col()\n\n\nggplot(cl.order, aes(x = ORDER, y = LEN_MC)) +\n  geom_col(aes(x = ORDER, y = LEN_MC))\n\n\n\n\n\n\n\n\n\n\n16.3.2 Multivariate plots\n\nAdvanced scatterplot with four variables: LEN_MC (x), LEN_SC (y), ORDER (colour) and SUBORDTYPE (shape)\n\n\n# 4 variables\nggplot(cl.order, aes(x = LEN_MC, y = LEN_SC)) +\n  geom_point(aes(color = ORDER, shape = SUBORDTYPE))\n\n\n\n\n\n\n\n\n\nFacets\n\n\n# 5 variables\nggplot(cl.order, aes(x = LEN_MC, y = LEN_SC)) +\n  geom_point(aes(color = ORDER, shape = SUBORDTYPE)) +\n  facet_wrap(~MORETHAN2CL)\n\n\n\n\n\n\n\n\n\n\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using r. New York; London: Routledge.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Continuous data</span>"
    ]
  },
  {
    "objectID": "Regular_expressions.html#regular-expressions",
    "href": "Regular_expressions.html#regular-expressions",
    "title": "12  Regular expressions",
    "section": "12.2 Regular expressions",
    "text": "12.2 Regular expressions\nRegular expressions (or ‘regex’) help us find more complex patterns in strings of text. Suppose we are interested in finding all inflectional forms of the lemma PROVIDE in a corpus, i.e., provide, provides, providing and provided. Insteading of searching for all forms individually, we can construct a regular expression of the form\n\\[\n\\text{provid(es | ing | ed)?}\n\\] which can be read as\n\n‘Match the sequence of letters &lt;provid&gt; as well as when it is optionally followed by the letters &lt;s&gt; or &lt;ing&gt; or &lt;ed&gt;’.\n\nNotice how optionality is signified by the ? operator and alternatives by |.\nTo activate regular expression in a kwic() query, the valuetype argument has to be set to \"regex\":\n\n# Load library and corpus\nlibrary(quanteda)\nICE_GB &lt;- readRDS(\"ICE_GB.RDS\")\n\n# Perform query\nkwic_provide &lt;- kwic(ICE_GB,\n                     \"provid(es|ing|ed)?\",\n                     valuetype = \"regex\",\n                     window = 20)\n\nThe number of hits has more than doubled. However, upon closer inspection, we’ll notice a few false positives, namely providential, provider and providers:\n\ntable(kwic_provide$keyword)\n\n\n      provid      provide     provided     Provided    Provident providential \n           1          165          118            5            1            1 \n    provider    providers     provides    providing    Providing \n           1            3           72           52            1 \n\n\nThere are two ways to handle the output:\n\nRefine the search expression further to only match those cases of interest.\nManually sort out irrelevant cases during qualitative annotation in a spreadsheet software.\n\nAs a rule of thumb, you should consider improving your search expression if you obtain hundreds or even thousands of false hits. Should there be only few false positives, it’s usually easier to simply mark them as “irrelevant” in your spreadsheet.",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Regular expressions</span>"
    ]
  },
  {
    "objectID": "Regular_expressions.html#a-regex-cheatsheet",
    "href": "Regular_expressions.html#a-regex-cheatsheet",
    "title": "12  Regular expressions",
    "section": "12.3 A RegEx Cheatsheet",
    "text": "12.3 A RegEx Cheatsheet\n\n12.3.1 Basic functions\n\n\n\nCommand\nDefinition\nExample\nFinds\n\n\n\n\n\n\npython\npython\n\n\n.\nAny character\n.ython\naython, bython…\n\n\n\n\n\n12.3.2 Character classes and alternatives\n\n\n\n\n\n\n\n\n\nCommand\nDefinition\nExample\nFinds\n\n\n\n\n[abc]\nClass of characters\n[jp]ython\njython, python\n\n\n[ ^pP]\nExcluded class of characters\n[^pP]ython\neverything but python, Python\n\n\n(...|...)\nAlternatives linked by logical operator or\nP(ython|eter)\nPython, Peter\n\n\n\n\n\n12.3.3 Quantifiers\n\n\n\n\n\n\n\n\n\nCommand\nDefinition\nExample\nFinds\n\n\n\n\n?\nOne or zero instances of the preceding symbol\nPy?thon\nPython, Pthon\n\n\n*\nNo matter how many times — also zero\nPy*thon\nPython, Pthon, Pyyyython…\n\n\n\n\nP[Yy]*thon\nPython, Pthon, PyYYython…\n\n\n+\nNo matter how many times but at least once\nPy+thon\nPython, Pyyython, Pyyyython\n\n\n{1,3}\n{min, max}\nPy{1,3}thon\nPython, Pyython, Pyyython\n\n\n\n\n\n12.3.4 Pre-defined character classes\n\n\n\n\n\n\nNote\n\n\n\nThe double backslashes (\\\\) shown here are specific to the quanteda R package. In most other programming languages including Python, you only need a single backslash (e.g., \\w, \\d, \\s). This double-escaping is an R-specific requirement due to how R handles string literals.\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDefinition\nExample\nFinds\n\n\n\n\n\\\\w\nAll alphanumeric characters (A-Z, a-z, 0-9)\n\\\\w+ing\nwalking, running, 42ing\n\n\n\\\\W\nAll non-alphanumeric characters\nhello\\\\W+world\nhello world, hello!!!world\n\n\n\\\\d\nAll decimal numbers (0-9)\n\\\\d{3}-\\\\d{4}\n555-1234, 867-5309\n\n\n\\\\D\nEverything which is not a decimal number\n\\\\D+\nHello!, Python_code\n\n\n\\\\s\nEmpty space\nword\\\\s+word\nword word, word word\n\n\n\\\\b\nWord boundary\n\\\\bpython\\\\b\nMatches python as a whole word",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Regular expressions</span>"
    ]
  },
  {
    "objectID": "Regular_expressions.html#exercises",
    "href": "Regular_expressions.html#exercises",
    "title": "12  Regular expressions",
    "section": "12.5 Exercises",
    "text": "12.5 Exercises\n\nExercise 12.1 How could you refine the search expression for PROVIDE \"provid(es|ing|ed)?\" to get rid of the irrelevant cases?\n\n\nExercise 12.2 Find all nouns ending in -er. How accurate is your query?\n\n\nExercise 12.3 Find all four-digit numbers.\n\n\nExercise 12.4 Write an elegant regular expression which finds all inflectional forms of swim.\n\n\nExercise 12.5 Find all verbs that are followed by a prepositional phrase.\n\n\n\n\n\nLange, Claudia, and Sven Leuckert. 2020. Corpus Linguistics for World Englishes: A Guide for Research. New York: Taylor; Francis.",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Regular expressions</span>"
    ]
  },
  {
    "objectID": "Regular_expressions.html#suggested-reading",
    "href": "Regular_expressions.html#suggested-reading",
    "title": "12  Regular expressions",
    "section": "12.1 Suggested reading",
    "text": "12.1 Suggested reading\n\nLange and Leuckert (2020): Chapter 3.7\nDetailed cheatsheet (DataCamp)",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Regular expressions</span>"
    ]
  },
  {
    "objectID": "Regular_expressions.html#querying-parsed-corpora",
    "href": "Regular_expressions.html#querying-parsed-corpora",
    "title": "12  Regular expressions",
    "section": "12.4 Querying parsed corpora",
    "text": "12.4 Querying parsed corpora\nThe range of linguistic patterns to be matched can be extended further if the corpus contains additional metadata, such as the part of speech (POS) of a token. POS-tagged corpora open up the option of looking for more abstract patterns, such as all instances of the verb eat that are followed by a pronoun or noun:\n\n# Load library and corpus\nICE_GB_POS &lt;- readRDS(\"ICE_GB_POS.RDS\")\n\n# Perform query\nkwic_provide_POS2 &lt;- kwic(ICE_GB_POS,\n                     phrase(\"\\\\b(ate|eat(s|ing|en)?)_VERB\\\\b _(PRON|NOUN)\"),\n                     valuetype = \"regex\",\n                     window = 5)\n\nhead(kwic_provide_POS2)\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nS1A-009.txt\n1198\n1199\nI_PRON must_AUX &lt; , &gt;\neat_VERB them_PRON\n&lt; ICE-GB:S1A-009 #71 : 1\n\\b(ate|eat(s|ing|en)?)_VERB\\b _(PRON|NOUN)\n\n\nS1A-010.txt\n958\n959\nto &lt; , &gt; actually_ADV\neat_VERB it_PRON\nfor_ADP one_NOUN ' s_PART own_ADJ\n\\b(ate|eat(s|ing|en)?)_VERB\\b _(PRON|NOUN)\n\n\nS1A-011.txt\n3245\n3246\n: A &gt; I_PRON have_AUX\neaten_VERB my_PRON\nway_NOUN round_ADP the_DET Yorkshire_PROPN Dales_PROPN\n\\b(ate|eat(s|ing|en)?)_VERB\\b _(PRON|NOUN)\n\n\nS1A-011.txt\n4159\n4160\nI_PRON ended_VERB up_ADP uhm_NOUN just_ADV\neating_VERB sort_NOUN\nof_ADP_ADP lumps_NOUN of chicken_NOUN and_CCONJ\n\\b(ate|eat(s|ing|en)?)_VERB\\b _(PRON|NOUN)\n\n\nS1A-018.txt\n455\n456\norder_VERB on_ADPe_NUM first_ADJ and_CCONJ_CCONJ then_ADV_ADV\neat_VERB it_PRON\nand then sort_ADV of_ADV carry_VERB\n\\b(ate|eat(s|ing|en)?)_VERB\\b _(PRON|NOUN)\n\n\nS1A-019.txt\n1038\n1039\nA &gt; and_CCONJ everybody_PRON was_AUX\neating_VERB something_PRON\n&lt; ICE-GB:S1A-019 #76 : 1\n\\b(ate|eat(s|ing|en)?)_VERB\\b _(PRON|NOUN)\n\n\n\n\n\n\n\nOne R package that supplies functions for tokenisation, POS-tagging and even dependency parsing for dozens of languages is udpipe. They all rely on one common set of tags known as Universal Dependencies, which are listed here:\n\n\n\n\n\n\nUniversial dependencies – Tagset\n\n\n\nCf. https://universaldependencies.org/u/pos/.\n\n\n\n\n\n\n\nPOS Tag\nDescription\n\n\n\n\nADJ\nAdjective: describes a noun (e.g., big, old, green, first)\n\n\nADP\nAdposition: prepositions and postpositions (e.g., in, to, over)\n\n\nADV\nAdverb: modifies verbs, adjectives, or other adverbs (e.g., quickly, very)\n\n\nAUX\nAuxiliary: helps form verb tenses, moods, or voices (e.g., is, have, will)\n\n\nCCONJ\nCoordinating conjunction: links words, phrases, or clauses (e.g., and, or, but)\n\n\nDET\nDeterminer: introduces nouns (e.g., the, a, some, my)\n\n\nINTJ\nInterjection: expresses emotion or reaction (e.g., oh, wow, hello)\n\n\nNOUN\nNoun: person, place, thing, or concept (e.g., cat, city, idea)\n\n\nNUM\nNumeral: expresses a number or ranking (e.g., one, two, second)\n\n\nPART\nParticle: adds meaning without being an independent word class (e.g., not, to as in to run)\n\n\nPRON\nPronoun: replaces nouns (e.g., he, she, they, it)\n\n\nPROPN\nProper noun: names specific entities (e.g., London, Vladimir)\n\n\nPUNCT\nPunctuation: marks boundaries in text (. , ! ?)\n\n\nSCONJ\nSubordinating conjunction: links clauses, often indicating dependency (e.g., if, because, although)\n\n\nSYM\nSymbol: non-alphanumeric symbol (e.g., %, &, #)\n\n\nVERB\nVerb: action or state (e.g., run, be, have)\n\n\nX\nOther: used when a word doesn’t fit into other categories",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Regular expressions</span>"
    ]
  },
  {
    "objectID": "Importing_exporting.html#solutions",
    "href": "Importing_exporting.html#solutions",
    "title": "10  Import/export data",
    "section": "10.4 Solutions",
    "text": "10.4 Solutions\nYou can find the solutions to the exercises here. :::\n\nExercise 10.1 Open the frequency_data.xlsx (or frequency_data.csv, respectively) in a spreadsheet software of your choice. Delete the content of some cells and try to read in the file again. How does R handle missing values?\n\n\nExercise 10.2  \n\n\n\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using r. New York; London: Routledge.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Import/export data</span>"
    ]
  },
  {
    "objectID": "Annotation.html#recommended-reading",
    "href": "Annotation.html#recommended-reading",
    "title": "13  Data annotation",
    "section": "13.1 Recommended reading",
    "text": "13.1 Recommended reading\n\nS. T. Gries (2013): Chapter 1.3.3",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data annotation</span>"
    ]
  },
  {
    "objectID": "Annotation.html#sample-study",
    "href": "Annotation.html#sample-study",
    "title": "13  Data annotation",
    "section": "13.2 Sample study",
    "text": "13.2 Sample study\n\n13.2.1 Theoretical background\nLet’s assume we are interested in the object realisation patterns of the verb eat in the British ICE component. A quick review of the literature tells us that …\n\n… argument realisation may be related to the aspectual structure of a verbal action (cf. Goldberg 2001), but …\n… there is a stronger focus on situation aspect (telicity/atelicity; i.e., logical endpoints of actions) than on grammatical aspect (i.e., perfective/progressive).\n\nSince grammatical aspect is also concerned with the temporal construal of actions, it raises the question of whether or not it can also influence object realisation. To investigate the relationship between aspect and object realisation, we will perform an exemplary analysis on the verb lemma EAT.\n\n\n13.2.2 Obtaining data\nWe load all necessary libraries to query the ICE-GB corpus and run a KWIC-search using the regular expression \\\\b(eat(s|ing|en)?|ate)\\\\b, which finds all inflection forms of EAT. We then store the results in a spreadsheet file kwic_eat.xlsx.\n\n# Load library and corpus\nlibrary(quanteda)\nlibrary(writexl)\n\nICE_GB &lt;- readRDS(\"ICE_GB.RDS\")\n\n# Perform query\nkwic_eat &lt;- kwic(ICE_GB,\n          phrase(\"\\\\b(eat(s|ing|en)?|ate)\\\\b\"),\n          valuetype = \"regex\",\n          window = 15)\n\n# Store results\nwrite_xlsx(kwic_eat, \"kwic_eat.xlsx\")\n\nWhen you open kwic_eat.xlsx in a spreadsheet software, the file will contain 7 columns by default (docname, from, to, keyword, post, pattern). Each row corresponds to a match of your search expression in the corpus, which is equal to 113 here. This is your raw output.",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data annotation</span>"
    ]
  },
  {
    "objectID": "Annotation.html#data-annotation",
    "href": "Annotation.html#data-annotation",
    "title": "13  Data annotation",
    "section": "13.3 Data annotation",
    "text": "13.3 Data annotation\nWhenever you decide to work on your corpus results, it is good practice to duplicate your file and append the current date to the filename. Re-save it as, for instance, kwic_eat_09_09_2024.xlsx and open it again. This way you’re performing basic version control, which will allow you to return to previous stages of your analysis with ease.\nIn your spreadsheet software, you can now assign your variables of interest to the empty columns next to your output data. For our specific example, we will need one that captures object realisation and one the type of verb aspect. Let’s simply call them object_realisation and verb_aspect.\n\n\n\n\n\n\nNaming variables\n\n\n\nOf course, you could also opt for a different column name, as long it has no spaces or special characters (e.g., !?%#). You could also name it object_realisation or, even more plainly, object, but not direct object or object realisation with spaces. Otherwise you are bound to encounter a surge of cryptic error messages in your R console.\n\n\nNow, you are ready to annotate your data! An easy coding scheme would involve classifying rows where eat occurs with an object as yes. Conversely, rows where the direct object is not realised syntactically are assigned the column value no. In the aspect column, verbal aspect will be coded as either perfective, progressive or neutral, following S. Gries and Deshors (2014): 118.\n\n\n\n\n\n\n13.3.1 Dealing with problematic cases\nHowever, things are not always that clear-cut. What if you encounter a false positive, i.e., an erroneous hit in your dataset? Further down in the spreadsheet the keyword ate is actually part of the preceding word, inappropriate.\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\n113\nICE_GB/W2F-019.txt\n696\n696\n: 1 &gt; Too much colour on her face would be inappropri &lt; l &gt;\nate\n, she feels , but she wears a light foundation . &lt; ICE-GB:W2F-019 #28 :\n\\b(eat(s|ing|en)?|ate)\\b\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do I do with false hits?\n\n\n\nShort answer: Do not delete irrelevant rows or columns. Essentially, from the moment you’ve obtained your corpus output, you should withstand the temptation to delete anything from it. Instead, adopt the practice of indicating missing values or irrelevant rows by an NA in a separate column. In later analyses, these can be easily filtered out!\nThis also minimises the risk of accidentally getting rid of data that could have proven important at a later point in time.\n\n\n\n\n13.3.2 Getting the data back into R\nImport the Excel file via\n\n# Load library\nlibrary(readxl)\n\n# Read file contents into the variable \"kwic_data\"\nkwic_data &lt;- read_xlsx(\"kwic_eat_09_09_2024.xlsx\")\n\n# Print the first six lines of \"kwic_data\"\nprint(head(kwic_data))\n\n# A tibble: 6 × 9\n  docname  from    to pre   keyword post  pattern object_realisation aspect_verb\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;              &lt;chr&gt;      \n1 ICE_GB…   458   458 had … eaten   anyw… \"\\\\b(e… no                 perfective \n2 ICE_GB…   478   478 : 1 … eating  will… \"\\\\b(e… no                 progressive\n3 ICE_GB…   785   785 &gt; Ye… eat     befo… \"\\\\b(e… no                 neutral    \n4 ICE_GB…  1198  1198 the … eat     them… \"\\\\b(e… yes                neutral    \n5 ICE_GB…  4529  4529 &gt; Ye… ate     in t… \"\\\\b(e… no                 neutral    \n6 ICE_GB…   958   958 know… eat     it f… \"\\\\b(e… yes                neutral    \n\n\n\n\n13.3.3 Adding a case list\nS. T. Gries (2013) recommends setting up the first column of the data frame such that it “numbers all n cases from \\(1\\) to \\(n\\) so that every row can be uniquely identified and so that you always restore one particular ordering (e.g., the original one)” (S. T. Gries 2013: 26). This is very easy to do: We specify a numeric vector ranging from 1 to the total number of rows in the data frame.\n\n# Create a new Case column (which, by default, is moved to the very end of the data frame)\nkwic_data$Case &lt;- 1:nrow(kwic_data)\n\n# Move the Case column to the front of the data frame\nlibrary(tidyverse)\n\nkwic_data &lt;- relocate(kwic_data, Case)\n\n# Print reordered data frame\nprint(head(kwic_data))\n\n# A tibble: 6 × 10\n   Case docname        from    to pre   keyword post  pattern object_realisation\n  &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             \n1     1 ICE_GB/S1A-0…   458   458 had … eaten   anyw… \"\\\\b(e… no                \n2     2 ICE_GB/S1A-0…   478   478 : 1 … eating  will… \"\\\\b(e… no                \n3     3 ICE_GB/S1A-0…   785   785 &gt; Ye… eat     befo… \"\\\\b(e… no                \n4     4 ICE_GB/S1A-0…  1198  1198 the … eat     them… \"\\\\b(e… yes               \n5     5 ICE_GB/S1A-0…  4529  4529 &gt; Ye… ate     in t… \"\\\\b(e… no                \n6     6 ICE_GB/S1A-0…   958   958 know… eat     it f… \"\\\\b(e… yes               \n# ℹ 1 more variable: aspect_verb &lt;chr&gt;\n\n\n\n\n\n\nGoldberg, Adele E. 2001. “Patient Arguments of Causative Verbs Can Be Omitted: The Role of Information Structure in Argument Distribution.” Language Sciences 23 (4): 503–24.\n\n\nGries, Stefan Thomas. 2013. Statistics for Linguistics with r: A Practical Introduction. 2nd rev. ed. Berlin; Boston: De Gruyter Mouton.\n\n\nGries, Stefan, and Sandra Deshors. 2014. “Using Regressions to Explore Deviations Between Interlanguage and Native Language.” Corpora 9: 109–36.",
    "crumbs": [
      "Corpus Linguistics with R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data annotation</span>"
    ]
  },
  {
    "objectID": "Importing_exporting.html#an-alternatives-rds-files",
    "href": "Importing_exporting.html#an-alternatives-rds-files",
    "title": "10  Import/export data",
    "section": "10.3 An alternatives: RDS files",
    "text": "10.3 An alternatives: RDS files\nIf the main goal is to save an intermediary result and make it available for later use, the most efficient solution would be to save the object to a local R data file ending in .RDS. Since it performs data compression as well, .RDS files can be considered analogous to .zip files, which are very commonly used for other data types.\nIn practice, we need to supply to the saveRDS() function\n\nan R object (e.g., a vector, data frame, matrix, graphs, statistical models – anything goes!) as well as\nthe desired name of the file.\n\n\n# Save data frame \"data\" to the file \"frequency_data.RDS\"\nsaveRDS(data, \"frequency_data.RDS\")\n\nTo read a file back in, we need to indicate the file name (or the full file path if the file is in a different folder).\n\n# Read in \"frequency_data.RDS\" and assign the contents to \"data2\"\ndata2 &lt;- readRDS(\"frequency_data.RDS\")\n\n# Verify contents\nprint(data2)\n\n  lemma frequency\n1 start       418\n2 enjoy       139\n3 begin       337\n4  help       281\n\n\n\n10.3.1 Exercises",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Import/export data</span>"
    ]
  },
  {
    "objectID": "Importing_exporting.html#a-convenient-alternative-rds-files",
    "href": "Importing_exporting.html#a-convenient-alternative-rds-files",
    "title": "10  Import/export data",
    "section": "10.3 A convenient alternative: RDS files",
    "text": "10.3 A convenient alternative: RDS files\nIf the main goal is to save an intermediary result and make it available for later use, the most efficient solution is to save the object to a local R data file ending in .RDS. Since it compressed data, .RDS files can be considered analogous to .zip files, which are very commonly used for other data types.\nIn practice, we use the saveRDS() function and supply it with …\n\n… an R object (e.g., a vector, data frame, matrix, graphs, statistical models – anything goes!) as well as\n… the desired name of the file.\n\n\n# Save data frame \"data\" to the file \"frequency_data.RDS\"\nsaveRDS(data, \"frequency_data.RDS\")\n\nTo read a file back in, we need to indicate the file name (or the full file path if the file is located in a different folder).\n\n# Read in \"frequency_data.RDS\" and assign the contents to \"data2\"\ndata2 &lt;- readRDS(\"frequency_data.RDS\")\n\n# Verify contents\nprint(data2)\n\n  lemma frequency\n1 start       418\n2 enjoy       139\n3 begin       337\n4  help       281",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Import/export data</span>"
    ]
  },
  {
    "objectID": "Categorical_data.html#other-plots",
    "href": "Categorical_data.html#other-plots",
    "title": "15  Categorical data",
    "section": "15.5 Other plots",
    "text": "15.5 Other plots\n\n15.5.1 Bubble plot\n\n\nShow the code\nlibrary(tidyverse)\n\n# Bubble plot\nggplot(pct1_df, aes(x = ORDER, y = SUBORDTYPE, size = pct)) +\n  geom_point(color = \"skyblue\", alpha = 0.7) +\n  scale_size_continuous(range = c(5, 20)) +  # Adjust bubble size range\n  labs(title = \"Bubble Plot of ORDER by SUBORDTYPE\",\n       x = \"ORDER\",\n       y = \"SUBORDTYPE\",\n       size = \"Percentage\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n15.5.2 Alluvial plot\n\n\nShow the code\n# Make sure to install this library prior to loading \nlibrary(ggalluvial)\n\nggplot(pct1_df,\n       aes(axis1 = ORDER, axis2 = SUBORDTYPE, y = pct)) +\n  geom_alluvium(aes(fill = ORDER), width = 0.3, alpha = 0.7) +\n  geom_stratum(width = 0.4, fill = \"gray\", color = \"black\") +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  labs(title = \"Alluvial Plot of ORDER by SUBORDTYPE\",\n       x = \"Categories\", y = \"Percentage\") +\n  theme_minimal()",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Categorical data</span>"
    ]
  },
  {
    "objectID": "Gradient_boosting.html#suggested-reading",
    "href": "Gradient_boosting.html#suggested-reading",
    "title": "26  Gradient boosting",
    "section": "26.1 Suggested reading",
    "text": "26.1 Suggested reading\n\nJames et al. (2021): Chapter 8.2\nHastie, Tibshirani, and Friedman (2017): Chapter 10",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Gradient boosting</span>"
    ]
  },
  {
    "objectID": "Gradient_boosting.html#preparation",
    "href": "Gradient_boosting.html#preparation",
    "title": "26  Gradient boosting",
    "section": "26.2 Preparation",
    "text": "26.2 Preparation",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Gradient boosting</span>"
    ]
  },
  {
    "objectID": "Gradient_boosting.html#boosting",
    "href": "Gradient_boosting.html#boosting",
    "title": "26  Gradient boosting",
    "section": "26.3 Boosting",
    "text": "26.3 Boosting\nThe core idea of boosting is as simple as it is intuitive: By aggregating the insights of multiple weak models, a much more powerful complex model can be formed. The new model ensemble is argued to be superior in terms of predictive performance. Boosting is quite versatile, but we will restrict our scope to decision trees as introduced in the previous unit.\n\n26.3.1 Loss functions and gradient descent\nA possible way of quantifying a model’s errors is by means of a loss function. Given \\(N\\) observations, let \\(y_i\\) represent the labels of the response variable and \\(f(x_i)\\) the predicted value for a data point \\(x_i\\). Then the corresponding multivariable loss function would be \\(L(y_i, f(x_i))\\), with the loss itself having the form in Equation 26.1. There are various metrics that can be used as concrete measures of loss. Squared errors are common for continuous target variables, whereas deviance, which is based on log-likelihood, would be more appropriate for categorical ones.\n\\[\nL(f) = \\sum_{i=1}^N L(y_i, f(x_i))\n\\tag{26.1}\\]\nIn order to find the best possible model, it is crucial to find the values that minimise \\(L(f)\\). Afterwards, it is possible to compute the rate of change in the predicted value, i.e., the gradient \\(g_{m}\\) for a given increment \\(h_m\\). The gradient is computed by\n\\[\ng_{mi} = \\frac{\\partial L(y, f(x_i))}{\\partial f(x_i)}.\n\\tag{26.2}\\]\nEssentially, it is a derivative of the function \\(L(y_i, f(x_i))\\) with respect to the variable \\(f(x_i)\\), which is why it can also be described as a partial derivative.\n\n\n26.3.2 Gradient boosting\n\n\n\n\n\n\nWarning\n\n\n\nThis page is still under construction. More content will be added soon!\n\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome H. Friedman. 2017. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York, NY: Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. New York: Springer. https://doi.org/10.1007/978-1-0716-1418-1.",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Gradient boosting</span>"
    ]
  },
  {
    "objectID": "Clustering.html#suggested-reading",
    "href": "Clustering.html#suggested-reading",
    "title": "29  Clustering",
    "section": "29.1 Suggested reading",
    "text": "29.1 Suggested reading",
    "crumbs": [
      "Multivariate Data Analysis",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "Clustering.html#preparation",
    "href": "Clustering.html#preparation",
    "title": "29  Clustering",
    "section": "29.2 Preparation",
    "text": "29.2 Preparation",
    "crumbs": [
      "Multivariate Data Analysis",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "Clustering.html#clustering-algorithms",
    "href": "Clustering.html#clustering-algorithms",
    "title": "29  Clustering",
    "section": "29.3 Clustering algorithms",
    "text": "29.3 Clustering algorithms\n\n\n\n\n\n\nWarning\n\n\n\nThis page is still under construction. More content will be added soon!\n\n\n\n29.3.1 \\(k\\)-means\n\n\n29.3.2 Partitioning around medoids (PAM)\n\n\n29.3.3 Hierarchical agglomerative clustering\n\n\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome H. Friedman. 2017. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York, NY: Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. New York: Springer. https://doi.org/10.1007/978-1-0716-1418-1.",
    "crumbs": [
      "Multivariate Data Analysis",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "Gradient_boosting.html#recommended-reading",
    "href": "Gradient_boosting.html#recommended-reading",
    "title": "26  Gradient boosting",
    "section": "26.1 Recommended reading",
    "text": "26.1 Recommended reading\n\nJames et al. (2021): Chapter 8.2\nHastie, Tibshirani, and Friedman (2017): Chapter 10",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Gradient boosting</span>"
    ]
  },
  {
    "objectID": "Clustering.html#recommended-reading",
    "href": "Clustering.html#recommended-reading",
    "title": "29  Clustering",
    "section": "29.1 Recommended reading",
    "text": "29.1 Recommended reading\n\nJames et al. (2021): Chapter 12\nHastie, Tibshirani, and Friedman (2017): Chapters 14.3.6, 14.3.10 & 14.3.12",
    "crumbs": [
      "Multivariate Data Analysis",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "More_tests.html#recommended-reading",
    "href": "More_tests.html#recommended-reading",
    "title": "30  More on significance testing",
    "section": "30.1 Recommended reading",
    "text": "30.1 Recommended reading\n\n\n\n\n\n\nWarning\n\n\n\nThis page is still under construction. More content will be added soon!",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>More on significance testing</span>"
    ]
  },
  {
    "objectID": "Variables.html#recommended-reading",
    "href": "Variables.html#recommended-reading",
    "title": "14  Statistical variables",
    "section": "14.1 Recommended reading",
    "text": "14.1 Recommended reading\n\nHeumann, Schomaker, and Shalabh (2022): Chapter 1.2\nAgresti and Kateri (2022): Chapter 1.2",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical variables</span>"
    ]
  },
  {
    "objectID": "Variables.html#preparation",
    "href": "Variables.html#preparation",
    "title": "14  Statistical variables",
    "section": "14.2 Preparation",
    "text": "14.2 Preparation\nPlease download the file Paquot_Larsson_2020_data.xlsx (Paquot and Larsson 2020)1 and store it in your working directory.\n1 The supplementary materials can be downloaded from the publisher’s website [Last accessed April 28, 2024]. Note that the dataset has been originally compiled by Gries (2013).\n# Libraries\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\n\n# Load data\ncl.order &lt;- read_xlsx(\"Paquot_Larsson_2020_data.xlsx\")\n\n# Inspect data\nstr(cl.order)\nhead(cl.order)\n\n\n\n\n\n\n\nWhat’s in the file? The str() function\n\n\n\n\n\nThe easiest way to get a general overview of the full data set is to apply the str() function to the respective data frame.\n\nstr(cl.order)\n\ntibble [403 × 8] (S3: tbl_df/tbl/data.frame)\n $ CASE       : num [1:403] 4777 1698 953 1681 4055 ...\n $ ORDER      : chr [1:403] \"sc-mc\" \"mc-sc\" \"sc-mc\" \"mc-sc\" ...\n $ SUBORDTYPE : chr [1:403] \"temp\" \"temp\" \"temp\" \"temp\" ...\n $ LEN_MC     : num [1:403] 4 7 12 6 9 9 9 4 6 4 ...\n $ LEN_SC     : num [1:403] 10 6 7 15 5 5 12 2 24 11 ...\n $ LENGTH_DIFF: num [1:403] -6 1 5 -9 4 4 -3 2 -18 -7 ...\n $ CONJ       : chr [1:403] \"als/when\" \"als/when\" \"als/when\" \"als/when\" ...\n $ MORETHAN2CL: chr [1:403] \"no\" \"no\" \"yes\" \"no\" ...\n\n\nThis shows us that the data frame has 8 columns, as the $ operators indicate ($ Case, $ ORDER, …). The column names are followed by\n\nthe data type (num for numeric and chr for character strings)\nthe number of values (`[1:403]`) and\nthe first few observations.\n\nAnother intuitive way to display the structure of a data matrix is to simply show the first few rows:\n\nhead(cl.order)\n\n# A tibble: 6 × 8\n   CASE ORDER SUBORDTYPE LEN_MC LEN_SC LENGTH_DIFF CONJ     MORETHAN2CL\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      \n1  4777 sc-mc temp            4     10          -6 als/when no         \n2  1698 mc-sc temp            7      6           1 als/when no         \n3   953 sc-mc temp           12      7           5 als/when yes        \n4  1681 mc-sc temp            6     15          -9 als/when no         \n5  4055 sc-mc temp            9      5           4 als/when yes        \n6   967 sc-mc temp            9      5           4 als/when yes        \n\n\n\n\n\n\n\n\n\n\n\nFurther details on the variables\n\n\n\n\n\n\nORDER: Does the subordinate clause come before or after the main clause? (‘sc-mc’ vs. ‘mc-sc’)\nSUBORDTYPE: Is the subordinate clause temporal or causal? (‘temp’ vs. ‘caus’)\nMORETHAN2CL: Are there most clauses in the sentence than just one subordinate clause and one main clause? (‘yes’ vs. ‘no’)\nLEN_MC: How many words does the main clause contain? (ratio-scaled continuous variable)\nLEN_SC: How many words does the subordinate clause contain? (ratio-scaled continuous variable)\nLENGTH_DIFF: What is the length difference in words between the main clause and subordinate clause? (ratio-scaled continuous variables)",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical variables</span>"
    ]
  },
  {
    "objectID": "Variables.html#variables",
    "href": "Variables.html#variables",
    "title": "14  Statistical variables",
    "section": "14.3 Variables",
    "text": "14.3 Variables\nThe concept of the variable is very handy in that it allows us to quantify various aspects their observations. In general, we distinguish between discrete variables, which can only take a limited set of unique values, and continuous variables, which can take infinitely many values within a specified range.\n\n14.3.1 Subtypes\nWe can further subdivide discrete and continuous variables into into nominal, ordinal, interval-scaled and ratio-scaled ones:\n\n\n\n\n\n\nNominal/categorical\n\n\n\n\n\nThese variables comprise a limited number of categories which cannot be ordered in a meaningful way. For instance, it does not matter which value of SUBORDTYPE or MORETHAN2CL comes first or last:\n\n    unique(cl.order$SUBORDTYPE)\n\n[1] \"temp\" \"caus\"\n\n    unique(cl.order$MORETHAN2CL)\n\n[1] \"no\"  \"yes\"\n\n\n\n\n\n\n\n\n\n\n\nOrdinal/categorical\n\n\n\n\n\nOrdinal variables are ordered. However, the intervals between their individual values are not interpretable. Heumann (2022: 6) provides a pertinent example:\n\n[T]he satisfaction with a product (unsatisfied–satisfied–very satisfied) is an ordinal variable because the values this variable can take can be ordered but the differences between ‘unsatisfied–satisfied’ and ‘satisfied–very satisfied’ cannot be compared in a numerical way.\n\n\n\n\n\n\n\n\n\n\nInterval-scaled/continuous\n\n\n\n\n\n\nIn the case of interval-scaled variables, the differences between the values can be interpreted, but their ratios must be treated with caution. A temperature of 4°C is 6 degrees warmer than -2°C; however, this does not imply that 4°C is three times warmer than -2°C. This is because the temperature scale has no true zero point; 0°C simply signifies another point on the scale and not the absence of temperature altogether.\n\n\n\n\n\n\n\n\n\n\nRatio-scaled/continuous\n\n\n\n\n\n\nRatio-scaled variables allow both a meaningful interpretation of the differences between their values and (!) of the ratios between them. Within the context of clause length, LENGTH_DIFF values such as 4 and 8 not only suggest that the latter is four units greater than the former but also that their ratio \\(\\frac{8}{4} = 2\\) is a valid way to describe the relationship between these values. Here a LENGTH_DIFF of 0 can be clearly viewed as the absence of a length difference.\n\n\n\n\n\n\n14.3.2 Dependent vs. independent variables\nIn empirical studies, it is often of interest whether one variable leads to changes in the values of another variable. When exploring such associations, we need to take another heuristic step to clarify the direction of the influence.\nIn a linguistic context, we denote the variable whose usage patterns we’d like to explain as the dependent or response variable. A list of possible dependent variables is provided in the section on Linguistic variables).\nIts outcomes are said to depend on one or more independent variables. These are also often referred to as explanatory variables as they are supposed to explain variation in the response variable. These can be AGE, SEX or the VARIETY of English at hand.\n\n\n14.3.3 Exercises\n\nExercise 14.1 Classify the remaining variables in the cl.order dataset.\n\n\nExercise 14.2 Consider the following statement:\n\nThis paper examines the influence of clause length on the ordering of main and subordinate clauses.\n\nWhat is the dependent variable? What is/are the independent variable(s)?\n\n\n\n\n\nAgresti, Alan, and Maria Kateri. 2022. Foundations of Statistics for Data Scientists: With r and Python. Boca Raton: CRC Press.\n\n\nGries, Stefan Thomas. 2013. Statistics for Linguistics with r: A Practical Introduction. 2nd rev. ed. Berlin; Boston: De Gruyter Mouton.\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nPaquot, Magali, and Tove Larsson. 2020. “Descriptive Statistics and Visualization with r.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Thomas Gries, 375–99. Cham: Springer.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical variables</span>"
    ]
  },
  {
    "objectID": "Chi_square_test.html#suggested-reading",
    "href": "Chi_square_test.html#suggested-reading",
    "title": "19  Chi-squared test",
    "section": "19.1 Suggested reading",
    "text": "19.1 Suggested reading\nFor linguists:\n\nGries (2021): Chapter 4.1.2.1\n\nGeneral:\n\nBaguley (2012): Chapter 4\nAgresti and Kateri (2022): Chapter 5",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Chi-squared test</span>"
    ]
  },
  {
    "objectID": "Chi_square_test.html#standardized-residuals",
    "href": "Chi_square_test.html#standardized-residuals",
    "title": "19  Chi-squared test",
    "section": "19.4 Standardized residuals",
    "text": "19.4 Standardized residuals",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Chi-squared test</span>"
    ]
  },
  {
    "objectID": "Chi_square_test.html#effect-size",
    "href": "Chi_square_test.html#effect-size",
    "title": "19  Chi-squared test",
    "section": "19.5 Effect size",
    "text": "19.5 Effect size\nThe \\(p\\)-value only indicates the presence of correlation, but not its strength – regardless of how low it may be. It does not convey how much two variables determine each other. To this end, it is recommended to report an effect size measure alongside the \\(p\\)-value. One such measure is Cramér’s \\(V\\), which takes values in the interval \\([0, 1]\\):\n\\[\nV = \\sqrt{\\frac{\\chi^2}{n \\times (min(nrow, ncol) - 1)}}.\n\\tag{19.5}\\]\nThe package confintr implements this in its cramersv() function:\n\ncramersv(freqs_test)\n\n[1] 0.5085863\n\n\nThe association between two categorical variables is stronger, the closer \\(V\\) approximates 1. Conversely, if \\(V = 0\\), then the variables are completely independent. There are various guidelines in the literature that provide thresholds for “small”, “moderate” and “large” effects, yet these are rarely justified on theoretical grounds and could be viewed as arbitrary.",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Chi-squared test</span>"
    ]
  },
  {
    "objectID": "Chi_square_test.html#moving-beyond-association-tests",
    "href": "Chi_square_test.html#moving-beyond-association-tests",
    "title": "19  Chi-squared test",
    "section": "19.6 Moving beyond association tests",
    "text": "19.6 Moving beyond association tests\n\n\n\n\nAgresti, Alan, and Maria Kateri. 2022. Foundations of Statistics for Data Scientists: With r and Python. Boca Raton: CRC Press.\n\n\nBaguley, Thomas. 2012. Serious Stats: A Guide to Advanced Statistics for the Behavioral Sciences. Houndmills, Basingstoke: Palgrave Macmillan.\n\n\nGries, Stefan Thomas. 2021. Statistics for Linguistics with r: A Practical Introduction. 3rd rev. ed. Berlin; Boston: De Gruyter Mouton.\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Chi-squared test</span>"
    ]
  },
  {
    "objectID": "Chi_square_test.html#likelihood-based-inference",
    "href": "Chi_square_test.html#likelihood-based-inference",
    "title": "19  Chi-squared test",
    "section": "19.6 Likelihood-based inference",
    "text": "19.6 Likelihood-based inference\nSince causal subordinate clauses typically follow the main clause in the sample, it may seem tempting to draw conclusions about the probability of a certain outcome of clause ORDER. Unfortunately, the \\(\\chi^2\\)-test is not designed for this kind of statistical inference. An approach that would actually allow the estimation of probabilities is Logistic regression.\n\n\n\n\n\n\nWhat would that look like?\n\n\n\n\n\n\n# Convert to factors and define reference levels\ncl.order$ORDER &lt;- factor(cl.order$ORDER, levels = c(\"sc-mc\", \"mc-sc\"))\ncl.order$SUBORDTYPE &lt;- factor(cl.order$SUBORDTYPE, levels = c(\"temp\", \"caus\")) \n\n# Fit logistic regression model\norder.glm1 &lt;- glm(ORDER ~ SUBORDTYPE, data = cl.order, family = \"binomial\")\n\n# Store model parameters\nintercept &lt;- order.glm1$coefficients[1]\nslope &lt;- order.glm1$coefficients[2]\n\n# Convert to probabilities\nunname(plogis(intercept + slope * 0)) # if temporal\n\n[1] 0.4460784\n\nunname(plogis(intercept + slope * 1)) # if causal\n\n[1] 0.9246231\n\n\nIf the subordinate clause is causal, there is a 92.5% chance that speakers will use the mc-sc ordering, but only a 44.6% chance of using sc-mc.\n\n\n\n\n\n\n\nAgresti, Alan, and Maria Kateri. 2022. Foundations of Statistics for Data Scientists: With r and Python. Boca Raton: CRC Press.\n\n\nBaguley, Thomas. 2012. Serious Stats: A Guide to Advanced Statistics for the Behavioral Sciences. Houndmills, Basingstoke: Palgrave Macmillan.\n\n\nGries, Stefan Thomas. 2021. Statistics for Linguistics with r: A Practical Introduction. 3rd rev. ed. Berlin; Boston: De Gruyter Mouton.\n\n\nHeumann, Christian, Michael Schomaker, and Shalabh. 2022. Introduction to Statistics and Data Analysis: With Exercises, Solutions and Applications in r. 2nd ed. Cham: Springer. https://doi.org/10.1007/978-3-031-11833-3.\n\n\nKrauth, J., and G. A. Lienert. 1973. Die Konfigurationsfrequenzanalyse (KFA) Und Ihre Anwendung in Psychologie Und Medizin. Reprint 1995. Weinheim: Beltz Psychologie Verlagsunion.",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Chi-squared test</span>"
    ]
  },
  {
    "objectID": "Chi_square_test.html#pearson-residuals",
    "href": "Chi_square_test.html#pearson-residuals",
    "title": "19  Chi-squared test",
    "section": "19.4 Pearson residuals",
    "text": "19.4 Pearson residuals\nIf we’re interested in what cells show the greatest difference between observed and expected frequencies, an option would be to inspect the Pearson residuals (cf. Equation 19.4).\n\\[ \\text{residuals} = \\frac{\\text{observed} - \\text{expected}}{\\sqrt{\\text{expected}}}\n\\tag{19.4}\\]\nThese can be accessed via the test results stored freqs_test.\n\nfreqs_test$residuals\n\n       \n             caus      temp\n  mc-sc  4.136760 -4.085750\n  sc-mc -6.063476  5.988708\n\n\nThe function assocplot() can automatically compute the pearson residuals for any given contingency table and create a plot that highlights their contributions. If the bar is above the dashed line, it is black and indicates that a category is observed more frequently than expected (e.g., causal subordinate clauses in the mc-sc order). Conversely, bars are coloured grey if a category is considerably less frequent than expected, such as caus in sc-mc.\n\nassocplot(t(observed_freqs), col = c(\"black\", \"lightgrey\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting the residuals: Configural Frequency Analysis\n\n\n\n\n\nThe chi-squared test only provides a \\(p\\)-value for the entire contingency table. But what if we wanted to test the residuals for their significance as well? Configural Frequency Analysis (Krauth and Lienert 1973) allows us to do exactly that: It performs a significance test for all combinations of variable values in a cross-table. Moreover, CFA is not limited to two variables only. Technically, users can test for associations between arbitrary numbers of variables, but should be aware of the increasing complexity of interpretation.\n\nlibrary(cfa) # install library beforehand\n\n# Get the observed counts and convert them to a data frame\nconfig_df &lt;- as.data.frame(observed_freqs)\n\n# Convert to matrix\nconfigs &lt;- as.matrix(config_df[, 1:2])  # First two columns contain the configurations\ncounts &lt;- config_df$Freq                # The Freq column contains the counts\n\n# Perform CFA\ncfa_output &lt;- cfa(configs, counts, bonferroni = TRUE)\n\n# Print output\nprint(cfa_output)\n\n\n*** Analysis of configuration frequencies (CFA) ***\n\n       label   n  expected         Q    chisq      p.chisq sig.chisq         z\n1 sc-mc caus  15  63.20596 0.1418682 36.76575 1.332103e-09      TRUE -6.671872\n2 sc-mc temp 113  64.79404 0.1425343 35.86463 2.115143e-09      TRUE  6.469444\n3 mc-sc caus 184 135.79404 0.1804075 17.11278 3.522441e-05      TRUE  5.027611\n4 mc-sc temp  91 139.20596 0.1827409 16.69335 4.393467e-05      TRUE -5.102385\n           p.z sig.z\n1 1.000000e+00  TRUE\n2 4.918210e-11  TRUE\n3 2.483137e-07  TRUE\n4 9.999998e-01  TRUE\n\n\nSummary statistics:\n\nTotal Chi squared         =  106.4365 \nTotal degrees of freedom  =  1 \np                         =  0 \nSum of counts             =  403 \n\nLevels:\n\nVar1 Var2 \n   2    2",
    "crumbs": [
      "Inferential Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Chi-squared test</span>"
    ]
  },
  {
    "objectID": "Gradient_boosting.html#gradient-descent",
    "href": "Gradient_boosting.html#gradient-descent",
    "title": "26  Gradient boosting",
    "section": "26.4 Gradient descent",
    "text": "26.4 Gradient descent\n\n\n\n\n\n\nWarning\n\n\n\nThis page is still under construction. More content will be added soon!\n\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome H. Friedman. 2017. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York, NY: Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. New York: Springer. https://doi.org/10.1007/978-1-0716-1418-1.",
    "crumbs": [
      "Statistical Modelling and Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Gradient boosting</span>"
    ]
  },
  {
    "objectID": "Drawing_samples.html#sampling",
    "href": "Drawing_samples.html#sampling",
    "title": "31  Drawing samples",
    "section": "31.1 Sampling",
    "text": "31.1 Sampling",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Drawing samples</span>"
    ]
  },
  {
    "objectID": "Drawing_samples.html#types-of-samples",
    "href": "Drawing_samples.html#types-of-samples",
    "title": "31  Drawing samples",
    "section": "31.2 Types of samples",
    "text": "31.2 Types of samples",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Drawing samples</span>"
    ]
  },
  {
    "objectID": "Drawing_samples.html#implementation-in-r",
    "href": "Drawing_samples.html#implementation-in-r",
    "title": "31  Drawing samples",
    "section": "31.3 Implementation in R",
    "text": "31.3 Implementation in R\n\n\n\n\n\n\nWarning\n\n\n\nThis page is still under construction. More content will be added soon!\n\n\n\n## Stratified sample -------------------------------------------------------\n\nlibrary(\"sampling\")\nlibrary(\"data.table\")\n\n## Run the kwic search for the item you're interested in and then return to this script\n\ntell_data &lt;- tibble(kwic_NI)\n\ntell_data &lt;- separate_wider_delim(tell_data, cols = docname, delim = \"-\", names = c(\"Text_category\", \"File_number\"))\n\n### Required sample size\n\nsample_size_required &lt;- 500\n\n### Proportions in the population\n\ntell_data_prop &lt;- table(tell_data$Text_category)/length(tell_data$Text_category)\n\n### Sizes of the stratified sample\n\nstrat_sample_sizes &lt;- round(sample_size_required*(tell_data_prop))\n\naddmargins(strat_sample_sizes) # view sum of hits\n\n### Draw the sample\n\ntell_data$Text_category &lt;- as.factor(tell_data$Text_category)\n\nset.seed(2203)\n\ntell_strat_sample &lt;- tibble(strata(tell_data, \"Text_category\", strat_sample_sizes, method = \"srswor\"))\n\n## Final output df\n\noutput_sample &lt;- tibble(getdata(tell_data, tell_strat_sample)) # export this as .xlsx etc.\n\n\n## Application to my data --------------------------------------------------\n\n\n### Version 1 ---------------------------------------------------------------\n\n\n## It's kinda shit to do it the way it is right now tbh\n\n## Read in the files to be sampled\n\npath_annotated &lt;- list.files(\"Hits/Annotated/\", full.names = TRUE)\n\n## Find all files which are not annotated\n\nelements &lt;- str_extract(path_annotated, \"Hits\\\\/Annotated\\\\/\\\\/\\\\w*[^A].xlsx\")\n\n\n## Drop NAs\n\nnew_verbs_paths &lt;- elements[!is.na(elements)]\n\n## Only read in those files I'm interested in\n\nunsampled_verbs_list &lt;- lapply(new_verbs_paths, function(x){\n  x &lt;- read_excel(x)\n})\n\n\n## Convert to large df\n\nunsampled_verbs_df &lt;- tibble(data.table::rbindlist(new_verbs_list, fill = TRUE)) #\n\nunsampled_verbs_list[[1]]\n\n## Prepare the register variable for each element in the list\n\n### Define a list environment\n\nverbs_list &lt;- list()\n\n### Loop\n\nfor (i in length(unsampled_verbs_list)) {\n  \n  # Create a proper Text_category column\n  unsampled_verbs_register &lt;- separate_wider_delim(\n                       unsampled_verbs_list[[i]],\n                       cols = docname,\n                       delim = \"-\",\n                       names = c(\"Text_category\", \"File_number\"))\n  \n  # Store results\n  new_list[[i]] &lt;- unsampled_verbs_register # doesn't fucking work\n\n}\n\n### Combine the output dfs\n\nunsampled_verbs_reg &lt;- dplyr::bind_rows(new_list)\n\n\n\n### Version 2 ---------------------------------------------------------------\n\n### PREPARATION\n\n# Read in all verbs I need to sample\n\npath_annotated &lt;- list.files(\"Hits/Annotated/\", full.names = TRUE)\n\n## Find all files which are not annotated\n\nelements &lt;- str_extract(path_annotated, \"Hits\\\\/Annotated\\\\/\\\\/\\\\w*[^A].xlsx\")\n\n## Drop NAs\n\nunsampled_verbs &lt;- elements[!is.na(elements)]\n\n## Read them in\n\nunsampled_verbs_list &lt;- lapply(unsampled_verbs, function(x){\n  x &lt;- read_excel(x)\n})\n\n## Convert to large df which contains all verbs I need to down-sample\n\nunsampled_verbs_df &lt;- tibble(data.table::rbindlist(unsampled_verbs_list, fill = TRUE))\n\n## Remove NAs\n\nunsampled_verbs_df &lt;- unsampled_verbs_df[!is.na(unsampled_verbs_df$lemma),]\n\n\n## See frequencies of each verb\n\nsort(table(unsampled_verbs_df$lemma), decreasing = TRUE)\n\nstr(unsampled_verbs_df)\n\n# DEFINE FUNCTION Sample_Verbs()\n## START\n\nSample_Verbs &lt;- function (data, verb, size) {\n  # Subset the df according to the verb of interest\n  subset_df &lt;- data[data$lemma == verb,]\n  \n  # Create a proper Text_category column\n  verb_data &lt;- separate_wider_delim(subset_df, cols = docname, delim = \"-\", names = c(\"Text_category\", \"File_number\"))\n  \n  # Required sample size\n  sample_size_required &lt;- size\n  \n  # Proportions in the population\n  verb_data_prop &lt;- table(verb_data$Text_category)/length(verb_data$Text_category)\n  \n  # Sizes of the stratified sample\n  strat_sample_sizes &lt;- round(sample_size_required*(verb_data_prop))\n  \n  # Draw the sample\n  verb_strat_sample &lt;- tibble(strata(verb_data,\n                                     \"Text_category\",\n                                     strat_sample_sizes,\n                                     method = \"srswor\"))\n  \n  # For reproducibility\n  set.seed(1234)\n  \n  # Generate output df\n  output_sample &lt;- tibble(getdata(verb_data, verb_strat_sample)) # export this as .xlsx etc.\n  \n  # Print df\n  print(output_sample)\n}\n\n\n## RUN FUNCTION\n\n# Define verbs to sample\n\nverbs_to_sample &lt;- unique(unsampled_verbs_df$lemma)\n  \n# Store output here\n\nfull_sample_list &lt;- list()\n\n# Run loop\n\nfor (i in verbs_to_sample) {\n  # Run the function for each element in the vector with the verbs to be sampled\n   sample_df &lt;- Sample_Verbs(unsampled_verbs_df, i, 1000)\n  \n  # Store the result of each iteration\n  full_sample_list[[i]] &lt;- sample_df\n  \n}\n\n# Combine the results\nfull_sampled_df &lt;- bind_rows(full_sample_list)\n\n\n# Remove superfluous columns and clean up\nfull_sampled_df |&gt;\n  mutate(docname = paste(Text_category, File_number, sep = \"-\")) |&gt; \n  relocate(docname) |&gt; \n  select(docname, from:lemma) -&gt; full_sampled_df_cleaned\n\n## END\n\n# Join with main df\n\n## Read in desired summary file\nNI_data_S &lt;- read_xlsx(\"Hits/Summaries/Summary_24_04_2024.xlsx\")\n\n## Join the sampled verbs df with the summary df\n\nNI_data_S2 &lt;- full_join(NI_data_S, full_sampled_df_cleaned) %&gt;% \n  arrange(keyword)\n\n## Write the df to a new file\n\nwrite_xlsx(NI_data_S2, \"Hits/Summaries/Summary_24_04_2024_joined_and_sampled.xlsx\")",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Drawing samples</span>"
    ]
  },
  {
    "objectID": "Drawing_samples.html",
    "href": "Drawing_samples.html",
    "title": "31  Drawing samples",
    "section": "",
    "text": "Warning\n\n\n\nThis page is still under construction. More content will be added soon!\n\n\n\n31.0.1 Preparation\nLoad libraries:\n\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(sampling)\nlibrary(data.table)\n\nPerform query:\n\n# Load corpus\nICE_GB &lt;- readRDS(\"ICE_GB.RDS\")\n\n# Perform query\nkwic_think &lt;- kwic(ICE_GB, \"think\")\n\n# Count number of observations\nnrow(kwic_think)\n\n[1] 2648\n\n# Show first few\nhead(kwic_think)\n\nKeyword-in-context with 6 matches.                                                       \n   [ICE_GB/S1A-001.txt, 55]          1: B &gt; I | think |\n  [ICE_GB/S1A-001.txt, 218]          1: B &gt; I | think |\n  [ICE_GB/S1A-001.txt, 534]          1: B &gt; I | think |\n  [ICE_GB/S1A-001.txt, 588] difference &lt;, &gt; I | think |\n  [ICE_GB/S1A-001.txt, 675]        &lt;, &gt; and I | think |\n [ICE_GB/S1A-001.txt, 1049]     B &gt; Uhm and I | think |\n                           \n the main things that I    \n the m &lt;, &gt;                \n that the &lt;,,              \n the main difference that I\n one of the things that    \n f for for myself &lt;        \n\n\n\n\n31.0.2 Stratified sample\n\nsource(\"https://raw.githubusercontent.com/VBuskin/Stats_with_R/refs/heads/main/Custom_functions.R\")\n\nstratified_sample_ICE(kwic_think, 500)\n\n# A tibble: 501 × 8\n   Text_category File_number  from    to pre               keyword post  pattern\n   &lt;chr&gt;         &lt;chr&gt;       &lt;int&gt; &lt;int&gt; &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt; &lt;fct&gt;  \n 1 ICE_GB/S1A    001.txt       588   588 difference &lt; , &gt;… think   the … think  \n 2 ICE_GB/S1A    001.txt      1866  1866 : B &gt; And I       think   that… think  \n 3 ICE_GB/S1A    001.txt      1974  1974 initial difficul… think   that… think  \n 4 ICE_GB/S1A    002.txt       386   386 lazy really for … think   &lt; , … think  \n 5 ICE_GB/S1A    002.txt       649   649 : C &gt; And I       think   when… think  \n 6 ICE_GB/S1A    002.txt      1578  1578 think there ' s I think   ther… think  \n 7 ICE_GB/S1A    002.txt      3621  3621 : B &gt; Therefore I think   that… think  \n 8 ICE_GB/S1A    002.txt      3710  3710 : B &gt; But I       think   &lt; , … think  \n 9 ICE_GB/S1A    003.txt       427   427 , &gt; why do you    think   phys… think  \n10 ICE_GB/S1A    003.txt      2613  2613 &gt; Well I Well I   think   it '… think  \n# ℹ 491 more rows",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Drawing samples</span>"
    ]
  },
  {
    "objectID": "Importing_exporting.html#exercises",
    "href": "Importing_exporting.html#exercises",
    "title": "10  Import/export data",
    "section": "10.5 Exercises",
    "text": "10.5 Exercises\n\n\n\n\n\n\nSolutions\n\n\n\nYou can find the solutions to the exercises here.\n\n\n\nExercise 10.1 Download the file SCOPE_reduced.RDS from this repository and read it into a variable named SCOPE. It contains data from the the South Carolina Psycholinguistic Metabase (Gao, Shinkareva, and Desai 2022), specifically:\n\nNumber of meanings (Nsenses_WordNet)\nEmotional valence ratings, which describe the pleasantness of a lexical stimulus on a scale from 1 to 9 (Valence_Warr)\nData for nearly 200,000 words\n\n\n\nExercise 10.2 Using this database, retrieve\n\nthe number of meanings for the verbs start, enjoy, begin, help. Store them in a data frame with the name senses_df.\nemotional valence ratings for the words fun, love, vacation, war, politics, failure, table. Store them in a data frame, and name it valence_df.\n\nWhat do you notice about the valence ratings? Do they align with your intuitions about these words’ emotional content?\n\n\n\n\n\n\nTip\n\n\n\nThis task is very similar to Exercise 8.3!\n\n\n\n\nExercise 10.3 Export senses_df and valence_df both as .csv and .xlsx files, and read them back into R.\n\n\nExercise 10.4 Separators determine how tabular data is stored internally. Investigate what happens when you read in frequency_data.csv with different separator settings:\n\n# Comma separator (default)\nimported_csv1 &lt;- read.csv(\"frequency_data.csv\", sep = \",\")\nprint(imported_csv1)\n\n# Tab separator\nimported_csv2 &lt;- read.csv(\"frequency_data.csv\", sep = \"\\t\")\nprint(imported_csv2)\n\n# Semi-colon separator\nimported_csv3 &lt;- read.csv(\"frequency_data.csv\", sep = \";\")\nprint(imported_csv3)\n\n\n\n\n\n\nGao, Chuanji, Svetlana V. Shinkareva, and Rutvik H. Desai. 2022. “SCOPE: The South Carolina Psycholinguistic Metabase.” Behavior Research Methods 55 (6): 2853–84. https://doi.org/10.3758/s13428-022-01934-0.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using r. New York; London: Routledge.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Import/export data</span>"
    ]
  },
  {
    "objectID": "Libraries.html#recommended-reading",
    "href": "Libraries.html#recommended-reading",
    "title": "9  Libraries",
    "section": "9.1 Recommended reading",
    "text": "9.1 Recommended reading\n\nWinter (2020): Chapter 1.13",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Libraries</span>"
    ]
  },
  {
    "objectID": "Libraries.html#working-with-packages-in-r",
    "href": "Libraries.html#working-with-packages-in-r",
    "title": "9  Libraries",
    "section": "9.2 Working with packages in R",
    "text": "9.2 Working with packages in R\nPackages expand the basic functionality of R by providing numerous quality-of-life improvements that not only considerably simplify common data wrangling tasks but which also provide frameworks for state-of-the-art methods for statistical analysis and natural language processing (NLP), among many other things.\n\n9.2.1 Installation\n\n\n\n\n\n\nHow do I install a library?\n\n\n\n\n\nNavigate to Packages &gt; Install and verify that the pop-up window says Install from: Repository (CRAN). You can now type in the name of the package you would like to install under Packages.\nVideo tutorial on YouTube\n\n\n\nThis reader will use functions from a variety of R packages. Please install the following ones:\n\n\n\nPackage\nPurpose\nSession\n\n\n\n\nreadxl\nImporting Microsoft Excel files\nImporting/exporting\n\n\nwritexl\nExporting Microsoft Excel files\nImporting/exporting\n\n\nquanteda\nAnalysis of text data\nConcordancing\n\n\nlattice\nData visualisation\nConcordancing\n\n\ntidyverse\nFramework for data manipulation and visualisation\nCategorical data, Continuous data\n\n\ncrosstable\nCreating contingency tables\nCategorical data\n\n\nflextable\nExporting contingency tables\nCategorical data\n\n\nconfintr\nEffect size measure for categorical data\nChi-squared test\n\n\n\n\n\n9.2.2 Loading packages\nOnce the installation has been completed, you can proceed to load the libraries using the code below. You can ignore the warning messages.\n\nlibrary(readxl)\nlibrary(writexl)\nlibrary(quanteda)\nlibrary(lattice)\nlibrary(tidyverse)\nlibrary(crosstable)\nlibrary(flextable)\nlibrary(confintr)\n\n\n\n\n\n\n\nActivating libraries\n\n\n\n\n\nWhenever you start a new R session (i.e., open RStudio), your libraries and their respective functions will be inactive. To re-activate a library, either use the library() function or simply select it in the Packages tab.\n\n\n\nIt is good practice to only activate those packages that are necessary for your analysis. While it won’t be a problem for the small set of packages as shown here, loading dozens of packages increases the risk of obtaining “homonymous” functions which have the same name but perform different operations. In this case, it might be helpful to “disambiguate” them by directly indicating which package a function is from:\n\nreadxl::read_xlsx(...)\n\n\n\n9.2.3 Citing R and R packages\nWhenever we draw on ideas other than our own, we give credit to the respective source by citing it appropriately. The same applies to R, RStudio as well as all the packages we rely on throughout our analyses.\nFor R, an up-to-date citation can be generated as follows:\n\ncitation()\n\n\nTo cite R in publications use:\n\n  R Core Team (2023). R: A language and environment for statistical\n  computing. R Foundation for Statistical Computing, Vienna, Austria.\n  URL https://www.R-project.org/.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2023},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nTo cite a specific package, simply supply the package name as an argument.\n\ncitation(\"quanteda\")\n\n\nTo cite package 'quanteda' in publications use:\n\n  Benoit K, Watanabe K, Wang H, Nulty P, Obeng A, Müller S, Matsuo A\n  (2018). \"quanteda: An R package for the quantitative analysis of\n  textual data.\" _Journal of Open Source Software_, *3*(30), 774.\n  doi:10.21105/joss.00774 &lt;https://doi.org/10.21105/joss.00774&gt;,\n  &lt;https://quanteda.io&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {quanteda: An R package for the quantitative analysis of textual data},\n    journal = {Journal of Open Source Software},\n    author = {Kenneth Benoit and Kohei Watanabe and Haiyan Wang and Paul Nulty and Adam Obeng and Stefan Müller and Akitaka Matsuo},\n    doi = {10.21105/joss.00774},\n    url = {https://quanteda.io},\n    volume = {3},\n    number = {30},\n    pages = {774},\n    year = {2018},\n  }\n\n\n\n\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using r. New York; London: Routledge.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Libraries</span>"
    ]
  },
  {
    "objectID": "Drawing_samples.html#preparation",
    "href": "Drawing_samples.html#preparation",
    "title": "31  Drawing samples",
    "section": "31.1 Preparation",
    "text": "31.1 Preparation\nLoad libraries:\n\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(sampling)\nlibrary(data.table)\n\nPerform query:\n\n# Load corpus\nICE_GB &lt;- readRDS(\"ICE_GB.RDS\")\n\n# Perform query\nkwic_think &lt;- kwic(ICE_GB, \"think\")\n\n# Count number of observations\nnrow(kwic_think)\n\n[1] 2648\n\n# Show first few\nhead(kwic_think)\n\nKeyword-in-context with 6 matches.                                                       \n   [ICE_GB/S1A-001.txt, 55]          1: B &gt; I | think |\n  [ICE_GB/S1A-001.txt, 218]          1: B &gt; I | think |\n  [ICE_GB/S1A-001.txt, 534]          1: B &gt; I | think |\n  [ICE_GB/S1A-001.txt, 588] difference &lt;, &gt; I | think |\n  [ICE_GB/S1A-001.txt, 675]        &lt;, &gt; and I | think |\n [ICE_GB/S1A-001.txt, 1049]     B &gt; Uhm and I | think |\n                           \n the main things that I    \n the m &lt;, &gt;                \n that the &lt;,,              \n the main difference that I\n one of the things that    \n f for for myself &lt;",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Drawing samples</span>"
    ]
  },
  {
    "objectID": "Drawing_samples.html#stratified-sample",
    "href": "Drawing_samples.html#stratified-sample",
    "title": "31  Drawing samples",
    "section": "31.2 Stratified sample",
    "text": "31.2 Stratified sample\n\n# Source function from GitHub\nsource(\"https://raw.githubusercontent.com/VBuskin/Stats_with_R/refs/heads/main/Custom_functions.R\")\n\n# Apply function to the output of kwic() to perform weighted sampling\nstratified_sample_ICE(kwic_think, 500)\n\n# A tibble: 501 × 8\n   Text_category File_number  from    to pre               keyword post  pattern\n   &lt;chr&gt;         &lt;chr&gt;       &lt;int&gt; &lt;int&gt; &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt; &lt;fct&gt;  \n 1 ICE_GB/S1A    001.txt       588   588 difference &lt; , &gt;… think   the … think  \n 2 ICE_GB/S1A    001.txt      1866  1866 : B &gt; And I       think   that… think  \n 3 ICE_GB/S1A    001.txt      1974  1974 initial difficul… think   that… think  \n 4 ICE_GB/S1A    002.txt       386   386 lazy really for … think   &lt; , … think  \n 5 ICE_GB/S1A    002.txt       649   649 : C &gt; And I       think   when… think  \n 6 ICE_GB/S1A    002.txt      1578  1578 think there ' s I think   ther… think  \n 7 ICE_GB/S1A    002.txt      3621  3621 : B &gt; Therefore I think   that… think  \n 8 ICE_GB/S1A    002.txt      3710  3710 : B &gt; But I       think   &lt; , … think  \n 9 ICE_GB/S1A    003.txt       427   427 , &gt; why do you    think   phys… think  \n10 ICE_GB/S1A    003.txt      2613  2613 &gt; Well I Well I   think   it '… think  \n# ℹ 491 more rows",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Drawing samples</span>"
    ]
  },
  {
    "objectID": "Using_AI.html#ai-assisted-programming",
    "href": "Using_AI.html#ai-assisted-programming",
    "title": "32  Working with AI",
    "section": "32.1 AI-assisted programming",
    "text": "32.1 AI-assisted programming\nAI chatbots like ChatGPT or Claude.ai are invaluable ressources for programming-related tasks. In the context of statistical computing, they can be particularly helpful – whether it’s for data filtering, visualisation or performing complex statistical analyses. This unit will explore some common analytical challenges and demonstrate how AI can be leveraged to address them.\n\n32.1.1 Data filtering and subsetting\nRecall Exercise 8.2 from the unit on data frames. It involves subsetting the data frame data according to certain criteria.\n\ndata &lt;- data.frame(lemma = c(\"start\", \"enjoy\", \"begin\", \"help\"), \n                   frequency = c(418, 139, 337, 281))\n\nprint(data)\n\n  lemma frequency\n1 start       418\n2 enjoy       139\n3 begin       337\n4  help       281\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n\n\n\nI have this data frame:\ndata &lt;- data.frame(lemma = c(“start”, “enjoy”, “begin”, “help”),\n+ frequency = c(418, 139, 337, 281))\nprint(data)\nlemma frequency\n1 start 418\n2 enjoy 139\n3 begin 337\n4 help 281\nShow me the code I need for solving these exercises:\nPrint the following elements by subsetting the data frame data accordingly.\n\n337\nbegin\nenjoy\nenjoy 139\nthe entire frequency column\n\n\n\n\n\n\n\n\n\n\n\nResponse (ChatGPT)\n\n\n\n\n\nHere is the R code to extract the specified elements from the data frame:\n\n# Data frame\ndata &lt;- data.frame(lemma = c(\"start\", \"enjoy\", \"begin\", \"help\"), \n                   frequency = c(418, 139, 337, 281))\n\n# 337 (element from the frequency column)\ndata$frequency[3]\n\n[1] 337\n\n# \"begin\" (element from the lemma column)\ndata$lemma[3]\n\n[1] \"begin\"\n\n# \"enjoy\" (element from the lemma column)\ndata$lemma[2]\n\n[1] \"enjoy\"\n\n# \"enjoy\" and 139 (row corresponding to \"enjoy\")\ndata[2, ]\n\n  lemma frequency\n2 enjoy       139\n\n# The entire frequency column\ndata$frequency\n\n[1] 418 139 337 281\n\n\nExplanation:\n\ndata$frequency[3]: Extracts the 3rd element of the frequency column.\ndata$lemma[3]: Extracts the 3rd element of the lemma column.\ndata$lemma[2]: Extracts the 2nd element of the lemma column.\ndata[2, ]: Extracts the entire 2nd row, which contains \"enjoy\" and 139.\ndata$frequency: Extracts the entire frequency column.\n\n\n\n\n\n\n32.1.2 Handling error messages\n\n32.1.2.1 Loading files\n\n\n\n\n\n\nPrompt\n\n\n\n\n\n\nI’ve downloaded a file SCOPE_reduced.RDS, but I can’t read it into RStudio. Whenever I run this code, I keep getting an error message.\nSCOPE &lt;- readRDS(\"SCOPE_reduced.RDS\")\nError in gzfile(file, “rb”) : cannot open the connection In addition: Warning message: In gzfile(file, “rb”) : cannot open compressed file ‘SCOPE_reduced.RDS’, probable reason ‘No such file or directory’\nIs there an easy solution to this problem? I remember I need to click on something and set the working directory somehow.\n\n\n\n\n\n\n\n\n\n\nResponse (ChatGPT)\n\n\n\n\n\nYes, you can resolve this issue easily in RStudio by setting your working directory to the folder where the SCOPE_reduced.RDS file is located. Follow these steps:\n\nSolution: Set the Working Directory in RStudio\n\nNavigate to the File Location:\n\n\nIn RStudio, go to the Files pane (usually in the bottom-right corner).\nUse the navigation buttons to locate the folder containing your SCOPE_reduced.RDS file.\n\n\nSet the Working Directory:\n\n\nOnce you’re in the correct folder, click the More button (three dots or a gear icon, depending on your version).\nSelect Set As Working Directory. This action will update your working directory to the folder where the file is located.\n\n\nRun the Code: After setting the working directory, you can read the file directly:\n\n\nSCOPE &lt;- readRDS(\"SCOPE_reduced.RDS\")\n\n\n\nVerify the Working Directory\nYou can confirm that the working directory is set correctly by running:\n\ngetwd()\n\nThis will print the current working directory in the console. Make sure it matches the location of your file.\n\n\nAlternative: Use the RStudio Menu\nYou can also set the working directory via the RStudio menu:\n\nGo to Session &gt; Set Working Directory &gt; Choose Directory….\nSelect the folder where your file is located and click OK.\n\n\n\nQuick Tip: Avoid Future Issues\nIf you often work with files in the same directory, you can save time by setting the working directory automatically in your script:\n\nsetwd(\"path_to_your_directory\")\n\nReplace \"path_to_your_directory\" with the folder’s full path. After doing this, your readRDS() function should work without issues!\n\n\n\n\n\n\n\n32.1.3 Specific questions\n\n\n\n\n\n\nWarning\n\n\n\nThis page is still under construction. More content will be added soon!",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Working with AI</span>"
    ]
  },
  {
    "objectID": "Importing_exporting.html#troubleshooting",
    "href": "Importing_exporting.html#troubleshooting",
    "title": "10  Import/export data",
    "section": "10.4 Troubleshooting",
    "text": "10.4 Troubleshooting\nImporting data into R is often a challenging and error-prone task, made more difficult by the wide range of potential issues. AI tools can be immensely helpful for identifying and addressing problems specific to your use case. This handout provides some practical guidance, including tips for resolving file-loading errors.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Import/export data</span>"
    ]
  },
  {
    "objectID": "Basics.html#barbierioldermenyounger2007-older-men-and-younger-women",
    "href": "Basics.html#barbierioldermenyounger2007-older-men-and-younger-women",
    "title": "1  An example from sociolinguistics",
    "section": "1.1 Barbieri (2007): Older Men and Younger Women",
    "text": "1.1 Barbieri (2007): Older Men and Younger Women\n\n1.1.1 Defining the variable\nNew quotatives in American English: be like, go, be all:\n\n“I was like ‘well if it’s not leaking, I don’t care.’”\n“Do you, I asked I was running out and I go, ‘you’re trying to deliver a pizza here?’”\n“My sister’s all ‘excuse me would you mind if I gave you, if I want your autograph.’”\n\n\n\n1.1.2 Reviewing previous research\n\nPrevious results with regard to age: Be like is known to be preferred by younger speakers.\nPrevious results with regard to sex are inconclusive.\n\n\n\n1.1.3 Research question\n\n“The study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all” (Barbieri 2007: 26).\n\n\n\n1.1.4 Data\n\nChoice of corpus: 450,000 words taken from the Conversation component of the Longman Spoken and Written English Corpus (all files where the age and gender of speakers were available), i.e., about 50 hours of casual conversation coded for speakers’ age and sex.\nData extraction: All forms of be like, be all, go, say extracted using corpus linguistic software; data entered into spreadsheet software; word counts of texts added via Delphi program.\n\n\n\n1.1.5 Presentation of Results\n\n\n\n\n\n\n\n\n\nQuotative\nAge group\nMale use (per 100,000 words)\nFemale use (per 100,000 words)\n\n\n\n\nsay\n16–26\n115\n149\n\n\n\n27–40\n101\n117\n\n\n\n41–54\n55\n214\n\n\n\n55+\n67\n433\n\n\nbe like\n16–26\n32\n329\n\n\n\n27–40\n66\n25\n\n\n\n41–54\n-\n1\n\n\n\n55+\n-\n19\n\n\ngo\n16–26\n25\n113\n\n\n\n27–40\n61\n18\n\n\n\n41–54\n-\n15\n\n\n\n55+\n-\n7\n\n\nbe all\n16–26\n14\n33\n\n\n\n\n\n\n\n\nReproduction of Barbieri’s Figure 5 ‘Proportional quotative use by men and women aged 16–26 and 27–40’\n\n\n\n\n\n\n1.1.6 Formulation of results\n\n“In sum, the patterns of proportional use and the results of the statistical analyses show that there are striking differences in the way that men and women under the age of forty use quotative verbs. In contrast, in conforming to the traditional quotative say, men and women over forty displayed similar behavior” (Barbieri 2007: 38–39).\n\n\n\n1.1.7 Some of her conclusions\n\n“In favoring be like over other quotatives, men in their late 20s and in their 30s display an ‘affinity’ with slightly younger women, women in their early 20s, rather than with women of their same age” (Barbieri 2007: 41).",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An example from sociolinguistics</span>"
    ]
  },
  {
    "objectID": "Basics.html#principles-of-empirical-linguistics",
    "href": "Basics.html#principles-of-empirical-linguistics",
    "title": "1  An example from sociolinguistics",
    "section": "1.2 Principles of Empirical Linguistics",
    "text": "1.2 Principles of Empirical Linguistics\n\nObjectivity — Independence from researchers or devices (→ replicability!)\nReliability — Studies should be replicable.\nValidity — A study must actually address the problem formulated in the research question.",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An example from sociolinguistics</span>"
    ]
  },
  {
    "objectID": "Basics.html#exercises",
    "href": "Basics.html#exercises",
    "title": "1  An example from sociolinguistics",
    "section": "1.3 Exercises",
    "text": "1.3 Exercises\n\nExercise 1.1 As you read through the sections above, think about what might go wrong at each stage of the study!\n\n\nExercise 1.2 Read another sociolinguistic study (e.g., Unuabonah and Gut 2018). As you read, identify similar key sections or “building blocks” used in this study (e.g., introduction, research question etc.). What purpose does each section serve in the study’s overall structure?\n\n\n\n\n\nBarbieri, Federica. 2007. “Older Men and Younger Women: A Corpus-Based Study of Quotative Use in American English.” English World-Wide 28 (1): 23–45. https://doi.org/10.1075/eww.28.1.03bar.\n\n\nUnuabonah, Foluke Olayinka, and Ulrike Gut. 2018. “Commentary Pragmatic Markers in Nigerian English.” English World-Wide 39 (2): 190–213.",
    "crumbs": [
      "Fundamentals of Corpus-based Research",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An example from sociolinguistics</span>"
    ]
  }
]